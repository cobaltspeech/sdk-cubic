// Copyright (2019) Cobalt Speech and Language Inc.

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package cobaltspeech.cubic;

import "google/api/annotations.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/empty.proto";

option go_package = ".;cubicpb";
option csharp_namespace = "CobaltSpeech.Cubic";

// Service that implements the Cobalt Cubic Speech Recognition API
service Cubic {
  // Queries the Version of the Server
  rpc Version (google.protobuf.Empty) returns (VersionResponse) {
    option (google.api.http) = {
      get: "/api/version"
    };
  }


  // Retrieves a list of available speech recognition models
  rpc ListModels (ListModelsRequest) returns (ListModelsResponse) {
    option (google.api.http) = {
      get: "/api/listmodels"
    };
  }

  // Performs synchronous speech recognition: receive results after all audio
  // has been sent and processed.  It is expected that this request be typically
  // used for short audio content: less than a minute long.  For longer content,
  // the `StreamingRecognize` method should be preferred.
  rpc Recognize(RecognizeRequest) returns (RecognitionResponse) {
    option (google.api.http) = {
      post: "/api/recognize"
      body: "*"
    };
  }

  // Performs bidirectional streaming speech recognition.  Receive results while
  // sending audio.  This method is only available via GRPC and not via
  // HTTP+JSON. However, a web browser may use websockets to use this service.
  rpc StreamingRecognize (stream StreamingRecognizeRequest) returns (stream RecognitionResponse) {
    option (google.api.http) = {
      get: "/api/stream"
    };
  }

  // Compiles recognition context information, such as a specialized list of
  // words or phrases, into a compact, efficient form to send with subsequent
  // `Recognize` or `StreamingRecognize` requests to customize speech
  // recognition. For example, a list of contact names may be compiled in a
  // mobile app and sent with each recognition request so that the app user's
  // contact names are more likely to be recognized than arbitrary names. This
  // pre-compilation ensures that there is no added latency for the recognition
  // request. It is important to note that in order to compile context for a
  // model, that model has to support context in the first place, which can be
  // verified by checking its `ModelAttributes.ContextInfo` obtained via the
  // `ListModels` method. Also, the compiled data will be model specific; that
  // is, the data compiled for one model will generally not be usable with a
  // different model.
  rpc CompileContext(CompileContextRequest) returns (CompileContextResponse) {
    option (google.api.http) = {
      post: "/api/compilecontext"
      body: "*"
    };
  }
}

// The top-level message sent by the client for the `ListModels` method.
message ListModelsRequest {}

// The top-level message sent by the client for the `Recognize` method.  Both
// the `RecognitionConfig` and `RecognitionAudio` fields are required.  The
// entire audio data must be sent in one request.  If your audio data is larger,
// please use the `StreamingRecognize` call..
message RecognizeRequest {
  // Provides configuration to create the recognizer.
  RecognitionConfig config = 1;

  // The audio data to be recognized
  RecognitionAudio audio = 2;
}

// The top-level message sent by the client for the `StreamingRecognize`
// request.  Multiple `StreamingRecognizeRequest` messages are sent. The first
// message must contain a `RecognitionConfig` message only, and all subsequent
// messages must contain `RecognitionAudio` only.  All `RecognitionAudio`
// messages must contain non-empty audio.  If audio content is empty, the server
// may interpret it as end of stream and stop accepting any further messages.
message StreamingRecognizeRequest {
  oneof request {
    RecognitionConfig config = 1;
    RecognitionAudio audio = 2;
  }

}

// The top-level message sent by the client for the `CompileContext` request. It
// contains a list of phrases or words, paired with a context token included in
// the model being used. The token specifies a category such as "menu_item",
// "airport", "contact", "product_name" etc. The context token is used to
// determine the places in the recognition output where the provided list of
// phrases or words may appear. The allowed context tokens for a given model can
// be found in its `ModelAttributes.ContextInfo` obtained via the `ListModels`
// method.
message CompileContextRequest {
  // Unique identifier of the model to compile the context information for. The
  // model chosen needs to support context which can be verified by checking its
  // `ModelAttributes.ContextInfo` obtained via `ListModels`.
  string model_id = 1;

  // The token that is associated with the provided list of phrases or words
  // (e.g "menu_item", "airport" etc.). Must be one of the tokens included in
  // the model being used, which can be retrieved by calling the `ListModels`
  // method.
  string token = 2;

  // List of phrases and/or words to be compiled.
  repeated ContextPhrase phrases = 3;
}

// The message sent by the server for the `Version` method.
message VersionResponse {
  // version of the cubic library handling the recognition
  string cubic = 1;

  // version of the server handling these requests
  string server = 2;
}


// The message returned to the client by the `ListModels` method.
message ListModelsResponse {
  // List of models available for use that match the request.
  repeated Model models = 1;
}

// Collection of sequence of recognition results in a portion of audio.  When
// transcribing a single audio channel (e.g. RAW_LINEAR16 input, or a mono
// file), results will be ordered chronologically.  When transcribing multiple
// channels, the results of all channels will be interleaved.  Results of each
// individual channel will be chronological.  No such promise is made for the
// ordering of results of different channels, as results are returned for each
// channel individually as soon as they are ready.
message RecognitionResponse {
  repeated RecognitionResult results = 1;
}

// The message returned to the client by the `CompileContext` method.
message CompileContextResponse {
  // Context information in a compact form that is efficient for use in
  // subsequent recognition requests. The size of the compiled form will depend
  // on the amount of text that was sent for compilation. For 1000 words it's
  // generally less than 100 kilobytes.
  CompiledContext context = 1;
}

// Configuration for setting up a Recognizer
message RecognitionConfig {
  // Unique identifier of the model to use, as obtained from a `Model` message.
  string model_id = 1;

  // The encoding of the audio data to be sent for recognition.
  //
  // For best results, the audio source should be captured and transmitted using
  // the RAW_LINEAR16 encoding.
  enum Encoding {
    // Raw (headerless) Uncompressed 16-bit signed little endian samples (linear
    // PCM), single channel, sampled at the rate expected by the chosen `Model`.
    RAW_LINEAR16 = 0;

    // WAV (data with RIFF headers), with data sampled at a rate equal to or
    // higher than the sample rate expected by the chosen Model.
    WAV = 1;

    // MP3 data, sampled at a rate equal to or higher than the sampling rate
    // expected by the chosen Model.
    MP3 = 2;

    // FLAC data, sampled at a rate equal to or higher than the sample rate
    // expected by the chosen Model.
    FLAC = 3;

    // VOX data (Dialogic ADPCM), sampled at 8 KHz.
    VOX8000 = 4;

    // Î¼-law (8-bit) encoded RAW data, single channel, sampled at 8 KHz.
    ULAW8000 = 5;
  }

  // Encoding of audio data sent/streamed through the `RecognitionAudio`
  // messages.  For encodings like WAV/MP3 that have headers, the headers are
  // expected to be sent at the beginning of the stream, not in every
  // `RecognitionAudio` message.
  //
  // If not specified, the default encoding is RAW_LINEAR16.
  //
  // Depending on how they are configured, server instances of this service may
  // not support all the encodings enumerated above. They are always required to
  // accept RAW_LINEAR16.  If any other `Encoding` is specified, and it is not
  // available on the server being used, the recognition request will result in
  // an appropriate error message.
  Encoding audio_encoding = 2;

  // Idle Timeout of the created Recognizer.  If no audio data is received by
  // the recognizer for this duration, ongoing rpc calls will result in an
  // error, the recognizer will be destroyed and thus more audio may not be sent
  // to the same recognizer.  The server may impose a limit on the maximum idle
  // timeout that can be specified, and if the value in this message exceeds
  // that serverside value, creating of the recognizer will fail with an error.
  google.protobuf.Duration idle_timeout = 3;

  // This is an optional field.  If this is set to true, each result will
  // include a list of words and the start time offset (timestamp) and the
  // duration for each of those words.  If set to `false`, no word-level
  // timestamps will be returned.  The default is `false`.
  bool enable_word_time_offsets = 4;

  // This is an optional field.  If this is set to true, each result will
  // include a list of words and the confidence for those words.  If `false`, no
  // word-level confidence information is returned.  The default is `false`.
  bool enable_word_confidence = 5;

  // This is an optional field.  If this is set to true, the field
  // `RecognitionAlternative.raw_transcript` will be populated with the raw
  // transcripts output from the recognizer will be exposed without any
  // formatting rules applied.  If this is set to false, that field will not
  // be set in the results.  The RecognitionAlternative.transcript will
  // always be populated with text formatted according to the server's settings.
  bool enable_raw_transcript = 6;

  // This is an optional field.  If this is set to true, the results will
  // include a confusion network.  If set to `false`, no confusion network will
  // be returned.  The default is `false`.  If the model being used does not
  // support a confusion network, results may be returned without a confusion
  // network available.  If this field is set to `true`, then
  // `enable_raw_transcript` is also forced to be true.
  bool enable_confusion_network = 7;

  // This is an optional field.  If the audio has multiple channels, this field
  // should be configured with the list of channel indices that should be
  // transcribed.  Channels are 0-indexed.
  //
  // Example: `[0]` for a mono file, `[0, 1]` for a stereo file.
  //
  // If this field is not set, a mono file will be assumed by default and only
  // channel-0 will be transcribed even if the file actually has additional
  // channels.
  //
  // Channels that are present in the audio may be omitted, but it is an error
  // to include a channel index in this field that is not present in the audio.
  // Channels may be listed in any order but the same index may not be repeated
  // in this list.
  //
  // BAD: `[0, 2]` for a stereo file; BAD: `[0, 0]` for a mono file.
  repeated uint32 audio_channels = 8;

  // This is an optional field.  If there is any metadata associated with the
  // audio being sent, use this field to provide it to cubic.  The server may
  // record this metadata when processing the request.  The server does not use
  // this field for any other purpose.
  RecognitionMetadata metadata = 9;

  // This is an optional field for providing any additional context information
  // that may aid speech recognition.  This can also be used to add
  // out-of-vocabulary words to the model or boost recognition of specific
  // proper names or commands. Context information must be pre-compiled via the
  // `CompileContext()` method.
  RecognitionContext context = 10;
}

// Metadata associated with the audio to be recognized.
message RecognitionMetadata {

  // Any custom metadata that the client wants to associate with the recording.
  // This could be a simple string (e.g. a tracing ID) or structured data
  // (e.g. JSON)
  string custom_metadata = 1;
}

  // A collection of additional context information that may aid speech
  // recognition.  This can be used to add out-of-vocabulary words to  
  // the model or to boost recognition of specific proper names or commands. 
message RecognitionContext {
  // List of compiled context information, with each entry being compiled from a
  // list of words or phrases using the `CompileContext` method.
  repeated CompiledContext compiled = 1;
}

// Context information in a compact form that is efficient for use in subsequent
// recognition requests. The size of the compiled form will depend on the amount
// of text that was sent for compilation. For 1000 words it's generally less
// than 100 kilobytes.
message CompiledContext {
  // The context information compiled by the `CompileContext` method. 
  bytes data = 1;
}

// A phrase or word that is to be compiled into context information that can be
// later used to improve speech recognition during a `Recognize` or
// `StreamingRecognize` call. Along with the phrase or word itself, there is an
// optional boost parameter that can be used to boost the likelihood of the
// phrase or word in the recognition output.
message ContextPhrase {
  // The actual phrase or word.
  string text  = 1;
  // This is an optional field. The boost value is a positive number which is
  // used to increase the probability of the phrase or word appearing in the
  // output. This setting can be used to differentiate between similar sounding
  // words, with the desired word given a bigger boost value.
  //
  // By default, all phrases or words are given an equal probability of 1/N
  // (where N = total number of phrases or words). If a boost value is provided,
  // the new probability is (boost + 1) * 1/N. We normalize the boosted
  // probabilities for all the phrases or words so that they sum to one. This
  // means that the boost value only has an effect if there are relative
  // differences in the values for different phrases or words. That is, if all
  // phrases or words have the same boost value, after normalization they will
  // all still have the same probability. This also means that the boost value
  // can be any positive value, but it is best to stick between 0 to 20.
  //
  // Negative values are not supported and will be treated as 0 values.
  float boost = 2;
}

// Audio to be sent to the recognizer
message RecognitionAudio {
  bytes data = 1;
}

// Description of a Cubic Model
message Model {
  // Unique identifier of the model.  This identifier is used to choose the
  // model that should be used for recognition, and is specified in the
  // `RecognitionConfig` message.
  string id = 1;

  // Model name.  This is a concise name describing the model, and maybe
  // presented to the end-user, for example, to help choose which model to use
  // for their recognition task.
  string name = 2;

  // Model attributes
  ModelAttributes attributes = 3;
}

// Attributes of a Cubic Model
message ModelAttributes {
  // Audio sample rate supported by the model
  uint32 sample_rate = 1;

  // Attributes specifc to supporting recognition context.
  ContextInfo context_info = 2;
}

// Model information specifc to supporting recognition context.
message ContextInfo {
  // If this is set to true, the model supports taking context information into
  // account to aid speech recognition. The information may be sent with with
  // recognition requests via RecognitionContext inside RecognitionConfig.
  bool supports_context = 1;

  // A list of tokens (e.g "name", "airport" etc.) that serve has placeholders
  // in the model where a client provided list of phrases or words may be used
  // to aid speech recognition and produce the exact desired recognition output.
  repeated string allowed_context_tokens = 2;
}

// A recognition result corresponding to a portion of audio.
message RecognitionResult {
  // An n-best list of recognition hypotheses alternatives
  repeated RecognitionAlternative alternatives = 1;

  // If this is set to true, it denotes that the result is an interim partial
  // result, and could change after more audio is processed.  If unset, or set
  // to false, it denotes that this is a final result and will not change.
  //
  // Servers are not required to implement support for returning partial
  // results, and clients should generally not depend on their availability.
  bool is_partial = 2;

  // If `enable_confusion_network` was set to true in the `RecognitionConfig`,
  // and if the model supports it, a confusion network will be available in the
  // results.
  RecognitionConfusionNetwork cnet = 3;

  // Channel of the audio file that this result was transcribed from.  For a
  // mono file, or RAW_LINEAR16 input, this will be set to 0.
  uint32 audio_channel = 4;

}

// A recognition hypothesis
message RecognitionAlternative {
  // Text representing the transcription of the words that the user spoke.
  //
  // The transcript will be formatted according to the servers formatting
  // configuration. If you want the raw transcript, please see the field
  // `raw_transcript`.  If the server is configured to not use any formatting,
  // then this field will contain the raw transcript.
  //
  // As an example, if the spoken utterance was "four people", and the
  // server was configured to format numbers, this field would be set to
  // "4 people".
  string transcript = 1;

  // Text representing the transcription of the words that the user spoke,
  // without any formatting.  This field will be populated only the config
  // `RecognitionConfig.enable_raw_transcript` is set to true. Otherwise this
  // field will be an empty string. If you want the formatted transcript, please
  // see the field `transcript`.
  //
  // As an example, if the spoken utterance was `here are four words`,
  // this field would be set to "HERE ARE FOUR WORDS".
  string raw_transcript = 6;

  // Confidence estimate between 0 and 1. A higher number represents a higher
  // likelihood of the output being correct.
  double confidence = 2;

  // A list of word-specific information for each recognized word in the
  // `transcript` field. This is available only if `enable_word_confidence` or
  // `enable_word_time_offsets` was set to `true` in the `RecognitionConfig`.
  repeated WordInfo words = 3;

  // A list of word-specific information for each recognized word in the
  // `raw_transcript` field. This is available only if `enable_word_confidence`
  // or `enable_word_time_offsets` was set to `true` _and_
  // `enable_raw_transcript` is also set to `true` in the `RecognitionConfig`.
  repeated WordInfo raw_words = 7;

  // Time offset relative to the beginning of audio received by the recognizer
  // and corresponding to the start of this utterance.
  google.protobuf.Duration start_time = 4;

  // Duration of the current utterance in the spoken audio.
  google.protobuf.Duration duration = 5;

}

// Word-specific information for recognized words
message WordInfo {
  // The actual word in the text
  string word = 1;

  // Confidence estimate between 0 and 1.  A higher number represents a
  // higher likelihood that the word was correctly recognized.
  double confidence = 2;

  // Time offset relative to the beginning of audio received by the recognizer
  // and corresponding to the start of this spoken word.
  google.protobuf.Duration start_time = 3;

  // Duration of the current word in the spoken audio.
  google.protobuf.Duration duration = 4;
}

// Confusion network in recognition output
message RecognitionConfusionNetwork {
  repeated ConfusionNetworkLink links = 1;
}

// A Link inside a confusion network
message ConfusionNetworkLink {
  // Time offset relative to the beginning of audio received by the recognizer
  // and corresponding to the start of this link
  google.protobuf.Duration start_time = 1;

  // Duration of the current link in the confusion network
  google.protobuf.Duration duration = 2;

  // Arcs between this link
  repeated ConfusionNetworkArc arcs = 3;
}

// An Arc inside a Confusion Network Link
message ConfusionNetworkArc {
  // Word in the recognized transcript
  string word = 1;

  // Confidence estimate between 0 and 1.  A higher number represents a higher
  // likelihood that the word was correctly recognized.
  double confidence = 2;
}
