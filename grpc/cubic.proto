// Copyright (2019) Cobalt Speech and Language Inc.

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package cobaltspeech.cubic;

import "google/api/annotations.proto";

option go_package = ".;cubicpb";
option csharp_namespace = "CobaltSpeech.Cubic";

// Service that implements the Cobalt Cubic Speech Recognition API
service Cubic {
  // Queries the Version of the Server
  rpc Version(VersionRequest) returns (VersionResponse) {
    option (google.api.http) = {
      get : "/api/version"
    };
  }

  // Retrieves a list of available speech recognition models
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse) {
    option (google.api.http) = {
      get : "/api/listmodels"
    };
  }

  // Performs bidirectional streaming speech recognition.  Receive results while
  // sending audio.  This method is only available via GRPC and not via
  // HTTP+JSON. However, a web browser may use websockets to use this service.
  rpc StreamingRecognize(stream StreamingRecognizeRequest)
      returns (stream RecognitionResponse) {
    option (google.api.http) = {
      get : "/api/stream"
    };
  }

  // Compiles recognition context information, such as a specialized list of
  // words or phrases, into a compact, efficient form to send with subsequent
  // `StreamingRecognize` requests to customize speech recognition. For example,
  // a list of contact names may be compiled in a mobile app and sent with each
  // recognition request so that the app user's contact names are more likely to
  // be recognized than arbitrary names. This pre-compilation ensures that there
  // is no added latency for the recognition request. It is important to note
  // that in order to compile context for a model, that model has to support
  // context in the first place, which can be verified by checking its
  // `ModelAttributes.ContextInfo` obtained via the `ListModels` method. Also,
  // the compiled data will be model specific; that is, the data compiled for
  // one model will generally not be usable with a different model.
  rpc CompileContext(CompileContextRequest) returns (CompileContextResponse) {
    option (google.api.http) = {
      post : "/api/compilecontext"
      body : "*"
    };
  }
}

// The top-level message sent by the client for the `Version` method.
message VersionRequest {}

// The top-level message sent by the client for the `ListModels` method.
message ListModelsRequest {}

// The top-level message sent by the client for the `StreamingRecognize`
// request.  Multiple `StreamingRecognizeRequest` messages are sent. The first
// message must contain a `RecognitionConfig` message only, and all subsequent
// messages must contain `RecognitionAudio` only.  All `RecognitionAudio`
// messages must contain non-empty audio.  If audio content is empty, the server
// may interpret it as end of stream and stop accepting any further messages.
message StreamingRecognizeRequest {
  oneof request {
    RecognitionConfig config = 1;
    RecognitionAudio audio = 2;
  }
}

// The top-level message sent by the client for the `CompileContext` request. It
// contains a list of phrases or words, paired with a context token included in
// the model being used. The token specifies a category such as "menu_item",
// "airport", "contact", "product_name" etc. The context token is used to
// determine the places in the recognition output where the provided list of
// phrases or words may appear. The allowed context tokens for a given model can
// be found in its `ModelAttributes.ContextInfo` obtained via the `ListModels`
// method.
message CompileContextRequest {
  // Unique identifier of the model to compile the context information for. The
  // model chosen needs to support context which can be verified by checking its
  // `ModelAttributes.ContextInfo` obtained via `ListModels`.
  string model_id = 1;

  // The token that is associated with the provided list of phrases or words
  // (e.g "menu_item", "airport" etc.). Must be one of the tokens included in
  // the model being used, which can be retrieved by calling the `ListModels`
  // method.
  string token = 2;

  // List of phrases and/or words to be compiled.
  repeated ContextPhrase phrases = 3;
}

// The message sent by the server for the `Version` method.
message VersionResponse {
  // version of the cubic library handling the recognition
  string cubic = 1;

  // version of the server handling these requests
  string server = 2;
}

// The message returned to the client by the `ListModels` method.
message ListModelsResponse {
  // List of models available for use that match the request.
  repeated Model models = 1;
}

// Collection of sequence of recognition results in a portion of audio.  When
// transcribing a single audio channel (e.g. RAW_LINEAR16 input, or a mono
// file), results will be ordered chronologically.  When transcribing multiple
// channels, the results of all channels will be interleaved.  Results of each
// individual channel will be chronological.  No such promise is made for the
// ordering of results of different channels, as results are returned for each
// channel individually as soon as they are ready.
message RecognitionResponse { repeated RecognitionResult results = 1; }

// The message returned to the client by the `CompileContext` method.
message CompileContextResponse {
  // Context information in a compact form that is efficient for use in
  // subsequent recognition requests. The size of the compiled form will depend
  // on the amount of text that was sent for compilation. For 1000 words it's
  // generally less than 100 kilobytes.
  CompiledContext context = 1;
}

// Configuration for setting up a Recognizer
message RecognitionConfig {
  // Unique identifier of the model to use, as obtained from a `Model` message.
  string model_id = 1;

  // Format of audio data sent/streamed through the `RecognitionAudio` messages.
  // For formats like WAV/MP3 that have headers, the headers are expected to be
  // sent at the beginning of the stream, not in every `RecognitionAudio`
  // message.
  //
  // An error is returned if the audio format is not specified.
  //
  // Depending on how they are configured, server instances of this service may
  // not support all the formats provided in this API.
  //
  // For best results, use the RAW audio with Little-Endian SignedInteger PCM
  // encoding of audio sampled at the same rate as the model you are using.
  oneof audio_format {
    AudioFormatRAW audio_format_raw = 2;
    AudioFormatWithHeaders audio_format_with_headers = 3;
  }

  message AudioFormatRAW {
    // Sample Rate of the audio in Hertz (e.g. 16000, 8000)
    uint32 sample_rate = 1;

    // Bit Depth of each audio sample (e.g. 8, 16, etc.) This field is ignored
    // for fixed-length encoding such as ULAW.
    uint32 bit_depth = 2;

    // Encoding of the audio.
    Encoding encoding = 3;

    // When audio samples have a bit-depth > 8, the data is assumed to be
    // little-endian, unless this field is set to true.
    bool big_endian = 4;

    // Number of channels present in the audio.
    uint32 num_channels = 5;

    // Encoding that describes RAW audio samples
    enum Encoding {
      // PCM data encoded as signed integers
      SIGNED_INTEGER = 0;

      // mu-law encoding.  Bit depth is assumed to be 8.
      ULAW = 1;

      // a-law encoding.  Bit depth is assumed to be 8.
      ALAW = 2;
    }
  }

  // Self-Describing Audio Formats
  enum AudioFormatWithHeaders {
    // WAV (data with RIFF headers), with data sampled at a rate equal to or
    // higher than the sample rate expected by the chosen model.
    WAV = 0;

    // MP3 data, sampled at a rate equal to or higher than the sample rate
    // expected by the chosen model.
    MP3 = 1;

    // FLAC data, sampled at a rate equal to or higher than the sample rate
    // expected by the chosen model.
    FLAC = 2;

    // OPUS data, encoded in an ogg container and sampled at a rate equal to or
    // higher than the sample rate expected by the chosen model.
    OPUS = 3;
  }

  // Initial time offset in milliseconds of audio to be sent to the newly
  // created Recognizer. This offset will be added to any timestamps returned in
  // the `RecognitionResult` messages. This field can be useful if a recognition
  // session was interrupted and needs to be restarted from a certain offset.
  uint32 initial_time_offset_ms = 4;

  // This is an optional field.  If this is set to true, each result will
  // include a list of words and the start time offset (timestamp) and the
  // duration for each of those words.  If set to `false`, no word-level
  // timestamps will be returned.  The default is `false`.
  bool enable_word_time_offsets = 5;

  // This is an optional field.  If this is set to true, each result will
  // include a list of words and the confidence for those words.  If `false`, no
  // word-level confidence information is returned.  The default is `false`.
  bool enable_word_confidence = 6;

  // This is an optional field.  If this is set to true, the field
  // `RecognitionAlternative.raw_transcript` will be populated with the raw
  // transcripts output from the recognizer will be exposed without any
  // formatting rules applied.  If this is set to false, that field will not
  // be set in the results.  The RecognitionAlternative.transcript will
  // always be populated with text formatted according to the server's settings.
  bool enable_raw_transcript = 7;

  // This is an optional field.  If this is set to true, the results will
  // include a confusion network.  If set to `false`, no confusion network will
  // be returned.  The default is `false`.  If the model being used does not
  // support a confusion network, results may be returned without a confusion
  // network available.  If this field is set to `true`, then
  // `enable_raw_transcript` is also forced to be true.
  bool enable_confusion_network = 8;

  // This is an optional field.  If the audio has multiple channels, this field
  // should be configured with the list of channel indices that should be
  // transcribed.  Channels are 0-indexed.
  //
  // Example: `[0]` for a mono file, `[0, 1]` for a stereo file.
  //
  // If this field is not set, all channels in the provided audio will be sent
  // for transcription.
  //
  // Channels that are present in the audio may be omitted, but it is an error
  // to include a channel index in this field that is not present in the audio.
  // Channels may be listed in any order but the same index may not be repeated
  // in this list.
  //
  // BAD: `[0, 2]` for a stereo file; BAD: `[0, 0]` for a mono file.
  repeated uint32 selected_audio_channels = 9;

  // This is an optional field.  If there is any metadata associated with the
  // audio being sent, use this field to provide it to cubic.  The server may
  // record this metadata when processing the request.  The server does not use
  // this field for any other purpose.
  RecognitionMetadata metadata = 10;

  // This is an optional field for providing any additional context information
  // that may aid speech recognition.  This can also be used to add
  // out-of-vocabulary words to the model or boost recognition of specific
  // proper names or commands. Context information must be pre-compiled via the
  // `CompileContext()` method.
  RecognitionContext context = 11;
}

// Metadata associated with the audio to be recognized.
message RecognitionMetadata {

  // Any custom metadata that the client wants to associate with the recording.
  // This could be a simple string (e.g. a tracing ID) or structured data
  // (e.g. JSON)
  string custom_metadata = 1;
}

// A collection of additional context information that may aid speech
// recognition.  This can be used to add out-of-vocabulary words to
// the model or to boost recognition of specific proper names or commands.
message RecognitionContext {
  // List of compiled context information, with each entry being compiled from a
  // list of words or phrases using the `CompileContext` method.
  repeated CompiledContext compiled = 1;
}

// Context information in a compact form that is efficient for use in subsequent
// recognition requests. The size of the compiled form will depend on the amount
// of text that was sent for compilation. For 1000 words it's generally less
// than 100 kilobytes.
message CompiledContext {
  // The context information compiled by the `CompileContext` method.
  bytes data = 1;
}

// A phrase or word that is to be compiled into context information that can be
// later used to improve speech recognition during a `StreamingRecognize` call.
// Along with the phrase or word itself, there is an optional boost parameter
// that can be used to boost the likelihood of the phrase or word in the
// recognition output.
message ContextPhrase {
  // The actual phrase or word.
  string text = 1;

  // This is an optional field. The boost factor is a positive number which is
  // used to multiply the probability of the phrase or word appearing in the
  // output. This setting can be used to differentiate between similar sounding
  // words, with the desired word given a bigger boost factor.
  //
  // By default, all phrases or words provided in the `RecongitionContext` are
  // given an equal probability of occurring. The boost factors larger than
  // 1 will make the phrase or word more probable. A boost factor of 2
  // corresponds to making the phrase or word twice as more likely, while a
  // boost factor of 0.5 means half as less likely.
  float boost = 2;
}

// Audio to be sent to the recognizer
message RecognitionAudio { bytes data = 1; }

// Description of a Cubic Model
message Model {
  // Unique identifier of the model.  This identifier is used to choose the
  // model that should be used for recognition, and is specified in the
  // `RecognitionConfig` message.
  string id = 1;

  // Model name.  This is a concise name describing the model, and maybe
  // presented to the end-user, for example, to help choose which model to use
  // for their recognition task.
  string name = 2;

  // Model attributes
  ModelAttributes attributes = 3;
}

// Attributes of a Cubic Model
message ModelAttributes {
  // Audio sample rate supported by the model
  uint32 sample_rate = 1;

  // Attributes specifc to supporting recognition context.
  ContextInfo context_info = 2;
}

// Model information specifc to supporting recognition context.
message ContextInfo {
  // If this is set to true, the model supports taking context information into
  // account to aid speech recognition. The information may be sent with with
  // recognition requests via RecognitionContext inside RecognitionConfig.
  bool supports_context = 1;

  // A list of tokens (e.g "name", "airport" etc.) that serve has placeholders
  // in the model where a client provided list of phrases or words may be used
  // to aid speech recognition and produce the exact desired recognition output.
  repeated string allowed_context_tokens = 2;
}

// A recognition result corresponding to a portion of audio.
message RecognitionResult {
  // An n-best list of recognition hypotheses alternatives
  repeated RecognitionAlternative alternatives = 1;

  // If this is set to true, it denotes that the result is an interim partial
  // result, and could change after more audio is processed.  If unset, or set
  // to false, it denotes that this is a final result and will not change.
  //
  // Servers are not required to implement support for returning partial
  // results, and clients should generally not depend on their availability.
  bool is_partial = 2;

  // If `enable_confusion_network` was set to true in the `RecognitionConfig`,
  // and if the model supports it, a confusion network will be available in the
  // results.
  RecognitionConfusionNetwork cnet = 3;

  // Channel of the audio file that this result was transcribed from.  For a
  // mono file, or RAW_LINEAR16 input, this will be set to 0.
  uint32 audio_channel = 4;
}

// A recognition hypothesis
message RecognitionAlternative {
  // Text representing the transcription of the words that the user spoke.
  //
  // The transcript will be formatted according to the servers formatting
  // configuration. If you want the raw transcript, please see the field
  // `raw_transcript`.  If the server is configured to not use any formatting,
  // then this field will contain the raw transcript.
  //
  // As an example, if the spoken utterance was "four people", and the
  // server was configured to format numbers, this field would be set to
  // "4 people".
  string transcript = 1;

  // Text representing the transcription of the words that the user spoke,
  // without any formatting.  This field will be populated only the config
  // `RecognitionConfig.enable_raw_transcript` is set to true. Otherwise this
  // field will be an empty string. If you want the formatted transcript, please
  // see the field `transcript`.
  //
  // As an example, if the spoken utterance was `here are four words`,
  // this field would be set to "HERE ARE FOUR WORDS".
  string raw_transcript = 6;

  // Confidence estimate between 0 and 1. A higher number represents a higher
  // likelihood of the output being correct.
  double confidence = 2;

  // A list of word-specific information for each recognized word in the
  // `transcript` field. This is available only if `enable_word_confidence` or
  // `enable_word_time_offsets` was set to `true` in the `RecognitionConfig`.
  repeated WordInfo words = 3;

  // A list of word-specific information for each recognized word in the
  // `raw_transcript` field. This is available only if `enable_word_confidence`
  // or `enable_word_time_offsets` was set to `true` _and_
  // `enable_raw_transcript` is also set to `true` in the `RecognitionConfig`.
  repeated WordInfo raw_words = 7;

  // Time offset in milliseconds relative to the beginning of audio received by
  // the recognizer and corresponding to the start of this utterance.
  uint32 start_time_ms = 4;

  // Duration in milliseconds of the current utterance in the spoken audio.
  uint32 duration_ms = 5;
}

// Word-specific information for recognized words
message WordInfo {
  // The actual word in the text
  string word = 1;

  // Confidence estimate between 0 and 1.  A higher number represents a
  // higher likelihood that the word was correctly recognized.
  double confidence = 2;

  // Time offset in milliseconds relative to the beginning of audio received by
  // the recognizer and corresponding to the start of this spoken word.
  uint32 start_time_ms = 3;

  // Duration in milliseconds of the current word in the spoken audio.
  uint32 duration_ms = 4;
}

// Confusion network in recognition output
message RecognitionConfusionNetwork { repeated ConfusionNetworkLink links = 1; }

// A Link inside a confusion network
message ConfusionNetworkLink {
  // Time offset in milliseconds relative to the beginning of audio received by
  // the recognizer and corresponding to the start of this link
  uint32 start_time_ms = 1;

  // Duration in milliseconds of the current link in the confusion network
  uint32 duration_ms = 2;

  // Arcs between this link
  repeated ConfusionNetworkArc arcs = 3;
}

// An Arc inside a Confusion Network Link
message ConfusionNetworkArc {
  // Word in the recognized transcript
  string word = 1;

  // Confidence estimate between 0 and 1.  A higher number represents a higher
  // likelihood that the word was correctly recognized.
  double confidence = 2;
}
