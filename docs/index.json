[
{
	"uri": "/getting-started/",
	"title": "Server Setup",
	"tags": [],
	"description": "",
	"content": "This section is meant to get you started using Cubic Server via a Docker image.\nIt is not necessary to go through through these steps to call the demo server for evaluation purposes, only to run Cubic Server on premises. The demo server can be found at https://demo-cubic.cobaltspeech.com.\nNote Commercial use of the demo service is not permitted. This server is for testing and demonstration purposes only and is not guaranteed to support high availability or high volume. Data uploaded to the server may be stored for internal purposes.   Installing the Cubic Server Image The SDK communicates with a Cubic Server instance using gRPC. Cobalt distributes a docker image that contains the cubicsvr binary and model files.\n  Contact Cobalt to get a link to the image file in AWS S3. This link will expire in two weeks, so be sure to download the file to your own server.\n  Download with the AWS CLI if you have it, or with curl:\nURL=\u0026#34;the url sent by Cobalt\u0026#34; IMAGE_NAME=\u0026#34;name you want to give the file (should end with the same extension as the url, usually bz2)\u0026#34; curl $URL -L -o $IMAGE_NAME   Load the docker image\ndocker load \u0026lt; $IMAGE_NAME This will output the name of the image (e.g. cubicsvr-demo-en_us-16).\n  Start the cubic service\ndocker run -p 2727:2727 -p 8080:8080 --name cobalt -v /path/on/host/usage-log:/usage cubicsvr-demo-en_us-16 That will start listening for grpc commands on port 2727 and http requests on 8080, and will stream the debug log to stdout. (You can replace --name cobalt with whatever name you want. That just provides a way to refer back to the currently running container.)\nThe command also specifies the bind-mount to which usage data is being written so you can track the amount of audio Cubic Server has processed, and submit the usage log to Cobalt if you have a volume-based license. If you have a volume-based license, you MUST provide this bind-mount so the usage data will persist even when the container is restarted; Cubic Server will not start if that location cannot be written to.\n  Verify the service is running by calling\ncurl http://localhost:8080/api/version   If you want to explore the package to see the model files etc, call the following to open the bash terminal on the previously run image. Model files are located in the /model directory.\ndocker exec -it cobalt bash   Contents of the docker image  Base docker image : debian-stretch-slim Additional dependencies  sox    Cobalt-specific files  cubicsvr - binary for performing Automatic Speech Recognition model.config - top-level config am/nnet3_online/final.mdl - this is the acoustic model am/nnet3_online/conf/online_cmvn.conf - feature extraction parameters for the features fed into the GMM model used for i-vector statistics accumulation. am/nnet3_online/conf/splice.conf - GMM feature context when accumulating statistics for i-vector accumulation. am/nnet3_online/ivector_extractor/* - Kaldi configuration files related to ivectors graph/HCLG.fst - the decoding graph: the combination of the AMs transition graph, the lexicon, and the language model graph/words.txt - an integer to word mapping, needed because the output of the HCLG graph contains only integer symbol IDs graph/phones/word_boundary.int - this is needed only when confusion network output is requested, it tells the decoder which phones are at word boundaries "
},
{
	"uri": "/using-cubic-sdk/",
	"title": "Using Cubic-SDK",
	"tags": [],
	"description": "",
	"content": "This section discusses the various aspects of using the SDK as a client.\n Installing the SDK Setting up a Connection Synchronous Recognition Streaming Recognition Recognition Context Recognition Configurations  It also provides an integration overview for Android applications.\n"
},
{
	"uri": "/protobuf/",
	"title": "Cubic API Reference",
	"tags": [],
	"description": "",
	"content": "The Cubic API is specified as a proto file. This section of the documentation is auto-generated from the spec. It describes the data types and functions defined in the spec. The \u0026ldquo;messages\u0026rdquo; below correspond to the data structures to be used, and the \u0026ldquo;service\u0026rdquo; contains the methods that can be called.\n"
},
{
	"uri": "/using-cubic-sdk/installation/",
	"title": "Installing the SDK",
	"tags": [],
	"description": "",
	"content": "Instructions for installing the SDK are language specific.\nGo The Go SDK supports Go modules and requires Go 1.12 or later. To use the SDK, import this package into your application:\nimport \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; Python The Python SDK depends on Python \u0026gt;= 3.5. You may use pip to perform a system-wide install, or use virtualenv for a local install.\npip install --upgrade pip pip install \u0026#34;git+https://github.com/cobaltspeech/sdk-cubic#egg=cobalt-cubic\u0026amp;subdirectory=grpc/py-cubic\u0026#34; C# The C# SDK utilizes the NuGet package manager. The package is called Cubic-SDK, under the owners name of CobaltSpeech.\nNuGet allows 4 different ways to install. Further instructions can be found on the nuget webpage. Installing via the dotnet cli through the command:\ndotnet add package Cubic-SDK Android Building for Android requires more steps, so it is described on the Android Integrations page.\niOS Swift Package Manager The Swift Package Manager is a tool for automating the distribution of Swift code and is integrated into the swift compiler.\nOnce you have your Swift package set up, adding swift-cubic as a dependency is as easy as adding it to the dependencies value of your Package.swift.\ndependencies: [ .package(url: \u0026#34;https://github.com/cobaltspeech/sdk-cubic.git\u0026#34;, .upToNextMajor(from: \u0026#34;1.5.0\u0026#34;)) ] "
},
{
	"uri": "/using-cubic-sdk/connecting/",
	"title": "Setup Connection",
	"tags": [],
	"description": "",
	"content": "Once you have your Cubic server up and running, let\u0026rsquo;s see how we can use the SDK to connect to it.\nFirst, you need to know the address (host:port) where the server is running. This document will assume the values 127.0.0.1:2727, but be sure to change those to point to your server instance. For example, to connect to Cobalt\u0026rsquo;s demo server, use demo-cubic.cobaltspeech.com:27271. Port 2727 is the default GRPC port that Cubic server binds to.\n 1Commercial use of the demo service is not permitted. This server is for testing and demonstration purposes only and is not guaranteed to support high availability or high volume. Data uploaded to the server may be stored for internal purposes.\n Default Connection The following code snippet connects to the server and queries its version. It uses our recommended default setup, expecting the server to be listening on a TLS encrypted connection, as the demo server does.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() version, err := client.Version(context.Background()) if err != nil { log.Fatal(err) } fmt.Println(version) }   import cubic serverAddress = \u0026#34;127.0.0.1:2727\u0026#34; client = cubic.Client(serverAddress) resp = client.Version() print(resp)   var creds = new Grpc.Core.SslCredentials(); var channel = new Grpc.Core.Channel(serverAddr, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel); var resp = client.Version(new Google.Protobuf.WellKnownTypes.Empty()); Console.WriteLine(String.Format(\u0026#34;CubicServer: {0}, Cubic: {1}\u0026#34;, resp.Server, resp.Cubic));   import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import com.cobaltspeech.cubic.CubicGrpc; String url = \u0026#34;127.0.0.1:2727\u0026#34; ManagedChannel mCubicChannel = ManagedChannelBuilder .forTarget(url) .build(); CubicGrpc.CubicStub mCubicService = CubicGrpc.newStub(mCubicChannel);   import Cubic class CubicConnection { let serverAddress = \u0026#34;demo-cubic.cobaltspeech.com\u0026#34; let serverPort = 2727 let client = Client(host: serverAddress, port: serverPort, useTLS: true) }    Insecure Connection It is sometimes required to connect to Cubic server without TLS enabled, such as during debugging.\nPlease note that if the server has TLS enabled, attempting to connect with an insecure client will fail. To connect to such an instance of cubic server, you can use:\nclient, err := cubic.NewClient(serverAddr, cubic.WithInsecure())   client = cubic.Client(serverAddress, insecure=True)   var creds = Grpc.Core.ChannelCredentials.Insecure; var channel = new Grpc.Core.Channel(serverAddr, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel);   import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import com.cobaltspeech.cubic.CubicGrpc; String url = \u0026#34;127.0.0.1:2727\u0026#34; ManagedChannel mCubicChannel = ManagedChannelBuilder .forTarget(url) .usePlainText() .build(); CubicGrpc.CubicStub mCubicService = CubicGrpc.newStub(mCubicChannel);   let client = Client(host: serverAddress, port: serverPort, useTLS: false)    Client Authentication In our recommended default setup, TLS is enabled in the gRPC setup, and when connecting to the server, clients validate the server\u0026rsquo;s SSL certificate to make sure they are talking to the right party. This is similar to how \u0026ldquo;https\u0026rdquo; connections work in web browsers.\nIn some setups, it may be desired that the server should also validate clients connecting to it and only respond to the ones it can verify. If your Cubic server is configured to do client authentication, you will need to present the appropriate certificate and key when connecting to it.\nPlease note that in the client-authentication mode, the client will still also verify the server\u0026rsquo;s certificate, and therefore this setup uses mutually authenticated TLS. This can be done with:\nclient, err := cubic.NewClient(serverAddr, cubic.WithClientCert(certPem, keyPem))   client = cubic.Client(serverAddress, clientCertificate=certPem, clientKey=keyPem)   var creds = new Grpc.Core.SslCredentials(File.ReadAllText(\u0026#34;root.pem\u0026#34;)); var channel = new Grpc.Core.Channel(serverAddr, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel);   Please see the Java section of https://grpc.io/docs/guides/auth/ for more details.    import Cubic import NIOSSL class CubicConnection { let serverAddress = \u0026#34;demo-cubic.cobaltspeech.com\u0026#34; let serverPort = 2727 let client = Client(host: serverAddress, port: serverPort, tlsCertificateFileName: \u0026#34;root\u0026#34;, tlsCertificateFormat: .pem) }    where certPem and keyPem are the bytes of the client certificate and key provided to you.\n"
},
{
	"uri": "/using-cubic-sdk/recognize/",
	"title": "Synchronous Recognition",
	"tags": [],
	"description": "",
	"content": "Note Synchronous recognize requests are suitable only for audio clips shorter than 30 seconds. In general, it is strongly recommended that you use streaming recognition.   The following example shows how to transcribe a short audio clip using Cubic\u0026rsquo;s Synchoronous Recognize Request. It is assumed that the audio file contains raw samples, PCM16SLE like Cubic expects. We will query the server for available models and use the first model to transcribe this speech.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) func main() { // Creating client without TLS. Remove cubic.WithInsecure() if using TLS  serverAddr := \u0026#34;127.0.0.1:2727\u0026#34; client, err := cubic.NewClient(serverAddr, cubic.WithInsecure()) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model \tmodel := modelResp.Models[0] f, err := os.Open(\u0026#34;test.raw\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } recResp, err := client.Recognize(context.Background(), cfg, f) for _, r := range recResp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } }   import cubic # set insecure to False if server uses TLS serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress, insecure=True) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # open audio file audio = open(\u0026#39;test.raw\u0026#39;, \u0026#39;rb\u0026#39;) resp = client.Recognize(cfg, audio) for result in resp.results: if not result.is_partial: print(result.alternatives[0].transcript)   using System; using System.IO; namespace CubicSynchronousRecognitionExample { class Program { static void Main(string[] args) { // set creds = new Grpc.Core.SslCredentials(); if using TLS  var serverAddress = \u0026#34;127.0.0.1:2727\u0026#34;; var creds = Grpc.Core.ChannelCredentials.Insecure; // Initialize a gRPC connection  var channel = new Grpc.Core.Channel(serverAddress, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel); // List the available models  var listModelsRequest = new CobaltSpeech.Cubic.ListModelsRequest(); var models = client.ListModels(listModelsRequest); var cfg = new CobaltSpeech.Cubic.RecognitionConfig { // Use the first available model  ModelId = models.Models[0].Id, }; // Open the audio file.  FileStream file = File.OpenRead(\u0026#34;test.raw\u0026#34;); var audio = new CobaltSpeech.Cubic.RecognitionAudio { Data = Google.Protobuf.ByteString.FromStream(file) }; // Create the request  var request = new CobaltSpeech.Cubic.RecognizeRequest { Config = cfg, Audio = audio, }; // Send the request  var resp = client.Recognize(request); foreach (var result in resp.Results) { if (!result.IsPartial) { Console.WriteLine(result.Alternatives[0].Transcript); } } } } }   /* Please note: this example does not attempt to handle threading and all exceptions. It gives a simplified overview of the essential gRPC calls. */ import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.stub.StreamObserver; import com.google.protobuf.ByteString; import com.cobaltspeech.cubic.CubicGrpc; import com.cobaltspeech.cubic.CubicOuterClass.*; public static int main() { // Setup connection  CubicGrpc.CubicStub mCubicService = CubicGrpc.newStub( ManagedChannelBuilder.forTarget(url).build()); // Load the file into a ByteString for gRPC.  byte[] fileContent = Files.readAllBytes(\u0026#34;/path/to/file\u0026#34;); ByteString fileContentBS = ByteString.copyFrom(fileContents) // Setup config message (Using first model available)  RecognitionConfig cfg = RecognitionConfig.newBuilder() .setModelId(\u0026#34;ModelID\u0026#34;) .build() // Setup audio message  RecognitionAudio audio = RecognitionAudio.newBuilder().setData(fileContentBS).build() // Setup callback to handle results  StreamObserver\u0026lt;\u0026gt; responseObserver = new StreamObserver\u0026lt;RecognitionResponse\u0026gt;() { @Override public void onNext(RecognitionResponse value) { System.out.println(\u0026#34;Result: \u0026#34; + value.toString()); } @Override public void onError(Throwable t) { System.err.println(\u0026#34;Error with recognition:\u0026#34; + t.toString()); } @Override public void onCompleted() { System.out.println(\u0026#34;Server is done sending responses back\u0026#34;); } }; // Send it over to the server  mCubicService.recognize( RecognizeRequest.newBuilder() .setConfig(cfg) .setAudio(audio) .build(), responseObserver); }   import Foundation import Cubic class CubicExample { // set useTLS to true if using TLS let client = Client(host: \u0026#34;127.0.0.1\u0026#34;, port: 2727, useTLS: false) var config = Cobaltspeech_Cubic_RecognitionConfig() let fileName = \u0026#34;text.raw\u0026#34; public init() { config.audioEncoding = .rawLinear16 client.listModels(success: { (models) in if let model = models?.first { self.config.modelID = model.id self.client.recognize(audioURL: URL(fileURLWithPath: fileName), config: self.config, success: { (response) in for result in response.results { if !result.isPartial, let alternative = result.alternatives.first { print(alternative.transcript) } } }) { (error) in print(error.localizedDescription) } } }) { (error) in print(error.localizedDescription) } } }   "
},
{
	"uri": "/using-cubic-sdk/streaming/",
	"title": "Streaming Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe an audio file using Cubicâ€™s Streaming Recognize Request. The stream can come from a file on disk or be directly from a microphone in real time.\nStreaming from an audio file We support several file formats including WAV, MP3, FLAC etc. For more details, please see the protocol buffer specification file in the SDK repository (grpc/cubic.proto). The examples below use a WAV file as input to the streaming recognition. We will query the server for available models and use the first model to transcribe the speech.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) func main() { // Creating client without TLS. Remove cubic.WithInsecure() if using TLS  serverAddr := \u0026#34;127.0.0.1:2727\u0026#34; client, err := cubic.NewClient(serverAddr, cubic.WithInsecure()) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model \tmodel := modelResp.Models[0] f, err := os.Open(\u0026#34;test.wav\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } // Define a callback function to handle results \tresultHandler := func(resp *cubicpb.RecognitionResponse) { for _, r := range resp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } } err = client.StreamingRecognize(context.Background(), cfg, f, resultHandler) if err != nil { log.Fatal(err) } }   import cubic # set insecure to False if server uses TLS serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress, insecure=True) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # client.StreamingRecognize takes any binary stream object that # has a read(nBytes) method. The method should return nBytes from # the stream. # open audio file stream audio = open(\u0026#39;test.wav\u0026#39;, \u0026#39;rb\u0026#39;) # send streaming request to cubic and print out results as they come in for resp in client.StreamingRecognize(cfg, audio): for result in resp.results: if result.is_partial: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\u0026#34;) else: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\\n\u0026#34;)   using System; using System.Collections.Generic; using System.IO; using System.Linq; using System.Threading.Tasks; using Grpc.Core; namespace CubicStreamingRecognitionExample { class Program { static async Task Main(string[] args) { // set creds = new Grpc.Core.SslCredentials(); if using TLS  var serverAddress = \u0026#34;127.0.0.1:2727\u0026#34;; var creds = Grpc.Core.ChannelCredentials.Insecure; // Initialize a gRPC connection  var channel = new Grpc.Core.Channel(serverAddress, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel); // List the available models  var listModelsRequest = new CobaltSpeech.Cubic.ListModelsRequest(); var modelResp = client.ListModels(listModelsRequest); // Using first model available  var model = modelResp.Models[0]; // Setup Recognition Config  var cfg = new CobaltSpeech.Cubic.RecognitionConfig { ModelId = model.Id, AudioEncoding = CobaltSpeech.Cubic.RecognitionConfig.Types.Encoding.Wav, }; string audioPath = \u0026#34;test.wav\u0026#34;; // Setup the bi-directional gRPC stream.  var call = client.StreamingRecognize(); using(call) { // Setup recieve task  var responseReaderTask = Task.Run(async() =\u0026gt; { // Wait for the next response  while (await call.ResponseStream.MoveNext()) { var response = call.ResponseStream.Current; foreach (var result in response.Results) { Console.WriteLine(result.Alternatives[0].Transcript); } } }); // Send config first, followed by the audio  { // Send the configs  var request = new CobaltSpeech.Cubic.StreamingRecognizeRequest(); request.Config = cfg; await call.RequestStream.WriteAsync(request); // Setup object for streaming audio  request.Config = null; request.Audio = new CobaltSpeech.Cubic.RecognitionAudio { }; // Send the audio, in 8kb chunks  const int chunkSize = 8192; using(FileStream file = File.OpenRead(audioPath)) { int bytesRead; var buffer = new byte[chunkSize]; while ((bytesRead = file.Read(buffer, 0, buffer.Length)) \u0026gt; 0) { var bytes = Google.Protobuf.ByteString.CopyFrom(buffer.Take(bytesRead).ToArray()); request.Audio.Data = bytes; await call.RequestStream.WriteAsync(request); } // Close the sending stream  await call.RequestStream.CompleteAsync(); } } // Wait for all of the responses to come back through the receiving stream  await responseReaderTask; } } } }   /* Please note: this example does not attempt to handle threading and all exceptions. It gives a simplified overview of the essential gRPC calls. */ import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.stub.StreamObserver; import com.google.protobuf.ByteString; import com.cobaltspeech.cubic.CubicGrpc; import com.cobaltspeech.cubic.CubicOuterClass.*; public static void transcribeFile() { // Setup connection  CubicGrpc.CubicStub mCubicService = CubicGrpc.newStub( ManagedChannelBuilder.forTarget(url).build()); // Setup callback to handle results  StreamObserver\u0026lt;\u0026gt; responseObserver = new StreamObserver\u0026lt;RecognitionResponse\u0026gt;() { @Override public void onNext(RecognitionResponse value) { System.out.println(\u0026#34;Result: \u0026#34; + value.toString()); } @Override public void onError(Throwable t) { System.err.println(\u0026#34;Error with recognition:\u0026#34; + t.toString()); } @Override public void onCompleted() { System.out.println(\u0026#34;Server is done sending responses back\u0026#34;); } }; // Setup bidirectional stream  StreamObserver\u0026lt;StreamingRecognizeRequest\u0026gt; requestObserver; // Outgoing messages are sent on this request object  requestObserver = mCubicService.streamingRecognize(mRecognitionResponseObserver); // Send config message  StreamingRecognizeRequest configs = StreamingRecognizeRequest.newBuilder() // Note, we do not call setAudio here.  .setConfig(RecognitionConfig.newBuilder() .setModelId(\u0026#34;ModelID\u0026#34;) .build()) .build(); requestObserver.onNext(configs); // Read the file in chunks and stream to server.  try { FileInputStream is = new FileInputStream(new File(\u0026#34;/path/to/file\u0026#34;)); byte[] bytes = new byte[1024]; int len = 0; // Read the file  while ((len = is.read(chunk)) != -1) { // Convert byte[] to ByteString for gRPC  ByteString audioBS = ByteString.copyFrom(chunk); // Send audio to server  RecognitionAudio audioMsg = RecognitionAudio.newBuilder() .setData(audioBS) .build() requestObserver.onNext(StreamingRecognizeRequest.newBuilder() .setAudio(audioMsg) .build()); } } catch (Exception e) { } // Handle exception  // Close the client side stream  requestObserver.onCompleted(); // Note: Once the server is done transcribing everything, responseObserver.onCompleted() will be called. }   import Foundation import Cubic class CubicExample { // set useTLS to true if using TLS let client = Client(host: \u0026#34;127.0.0.1\u0026#34;, port: 2727, useTLS: false) var config = Cobaltspeech_Cubic_RecognitionConfig() let fileName = \u0026#34;test.wav\u0026#34; let chunkSize = 8192 public init() { let fileUrl = URL(fileURLWithPath: fileName) guard let audioData = try? Data(contentsOf: fileUrl) else { return } config.audioEncoding = .wav client.listModels(success: { (models) in if let model = models?.first { self.config.modelID = model.id self.client.streamingRecognize(audioData: audioData, chunkSize: self.chunkSize, config: self.config, success: { (response) in for result in response.results { if !result.isPartial, let alternative = result.alternatives.first { print(alternative.transcript) } } }) { (error) in print(error.localizedDescription) } } }) { (error) in print(error.localizedDescription) } } }    Streaming from microphone Streaming audio from microphone input typically needs us to interact with system libraries. There are several options available, and although the examples here use one, you may choose to use an alternative as long as the recording audio format is chosen correctly.\n/* This example utilizes the portaudio bindings for Go (see https://github.com/gordonklaus/portaudio) to stream audio from a microphone. To use this package, install PortAudio (see http://www.portaudio.com/) development headers and libraries using an appropriate package manager for your system (e.g. `apt-get install portaudio19-dev` on Ubuntu, `brew install portaudio` for OSX, etc.) or build from [source](http://portaudio.com/docs/v19-doxydocs/tutorial_start.html). */ package main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;encoding/binary\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; \u0026#34;github.com/gordonklaus/portaudio\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; // Microphone implements the io.ReadCloser interface and provides // a data stream for microphone input. type Microphone struct { buffer []int16 stream *portaudio.Stream } // NewMicrophone instantiates a Microphone object with the desired // sampling rate and buffer size. When streaming to cubic, the sample // rate should be set to the sample rate of the model used. func NewMicrophone(sampleRate, bufferSize uint32) (*Microphone, error) { // bufferSize is measured in number of bytes. Since we are capturing \t// 16 bit audio, each sample is 2 bytes. The microphone has a int16 \t// buffer, so we use the number of samples as its size. \tnumSamples := bufferSize/2 mic := Microphone{buffer: make([]int16, numSamples)} portaudio.Initialize() stream, err := portaudio.OpenDefaultStream(1, 0, float64(sampleRate), int(numSamples), mic.buffer) if err != nil { return nil, err } mic.stream = stream err = mic.stream.Start() if err != nil { return nil, err } return \u0026amp;mic, nil } // Read copies N bytes into the passed buffer from the microphone audio buffer // where N is the buffer size passed to `cubic.NewClient`. Also, to be compatible // with the cubic client, it returns two things : an int representing the number of // bytes copied and an error. func (mic *Microphone) Read(buffer []byte) (int, error) { err := mic.stream.Read() if err != nil { return 0, err } byteBuffer := new(bytes.Buffer) err = binary.Write(byteBuffer, binary.LittleEndian, mic.buffer) if err != nil { return 0, err } copy(buffer, byteBuffer.Bytes()) return len(buffer), nil } // Close shuts the microphone stream down and cleans up func (mic *Microphone) Close() { mic.stream.Stop() mic.stream.Close() portaudio.Terminate() } func main() { bufferSize := uint32(8192) // Creating client without TLS. Remove cubic.WithInsecure() if using TLS \tclient, err := cubic.NewClient(serverAddr, cubic.WithInsecure(), cubic.WithStreamingBufferSize(bufferSize)) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model \tmodel := modelResp.Models[0] cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } // Define a callback function to handle results \tresultHandler := func(resp *cubicpb.RecognitionResponse) { for _, r := range resp.Results { if r.IsPartial { fmt.Print(\u0026#34;\\r\u0026#34;, r.Alternatives[0].Transcript) // print on same line \t} else { fmt.Println(\u0026#34;\\r\u0026#34;, r.Alternatives[0].Transcript) // print and move to new line \t} } } // Create microphone stream \tmic, err := NewMicrophone(model.Attributes.SampleRate, bufferSize) if err != nil { log.Fatal(err) } defer mic.Close() // Since Microphone implements the io.ReadCloser interface, we can \t// pass it directly to the StreamingRecognize function. \terr = client.StreamingRecognize(context.Background(), cfg, mic, resultHandler) if err != nil { log.Fatal(err) } }   # # This example requires the pyaudio (http://people.csail.mit.edu/hubert/pyaudio/) # module to stream audio from a microphone. Instructions for installing pyaudio for # different systems are available at the link. On most platforms, this is simply `pip install pyaudio`. import cubic import pyaudio # set insecure to False if server uses TLS serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress, insecure=True) # get list of available models modelResp = client.ListModels() # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # client.StreamingRecognize takes any binary stream object that has a read(nBytes) # method. The method should return nBytes from the stream. So pyaudio is a suitable # library to use here for streaming audio from the microphone. Other libraries or # modules may also be used as long as they have the read method or have been wrapped # to do so. # open microphone stream p = pyaudio.PyAudio() audio = p.open(format=pyaudio.paInt16, # 16 bit samples channels=1, # mono audio rate=model.attributes.sample_rate, # sample rate in hertz input=True) # audio input stream # send streaming request to cubic and print out results as they come in try: for resp in client.StreamingRecognize(cfg, audio): for result in resp.results: if result.is_partial: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\u0026#34;) else: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\\n\u0026#34;) except KeyboardInterrupt: # stop streaming when ctrl+C pressed pass except Exception as err: print(\u0026#34;Error while trying to stream audio : {}\u0026#34;.format(err)) audio.stop_stream() audio.close()   // We do not currently have example C# code for streaming from a microphone. // Simply pass the bytes from the microphone the same as is done from the file in // the `Streaming from an audio file` example above. You can do this by implementing // your own class derived from `System.IO.Stream` class which the `StreamingRecognize` // method accepts. // // For more on the `System.IO.Stream` class see: // https://docs.microsoft.com/en-us/dotnet/api/system.io.stream?view=netframework-4.8    // For a complete C++ example, see the examples-cpp github repository: // https://github.com/cobaltspeech/examples-cpp    /* This example uses the `android.media.AudioRecord` class and assumes the min API level is higher than Marshmallow. Please note: this example does not attempt to handle threading and all exceptions. It gives a simplified overview of the essential gRPC calls. For a complete android example, see the examples-android github repository: https://github.com/cobaltspeech/examples-android */ import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.stub.StreamObserver; import com.google.protobuf.ByteString; import com.cobaltspeech.cubic.CubicGrpc; import com.cobaltspeech.cubic.CubicOuterClass.*; public static void streamMicrophoneAudio() { // Setup connection  CubicGrpc.CubicStub mCubicService = CubicGrpc.newStub( ManagedChannelBuilder.forTarget(url).build()); // Setup callback to handle results  StreamObserver\u0026lt;\u0026gt; responseObserver = new StreamObserver\u0026lt;RecognitionResponse\u0026gt;() { @Override public void onNext(RecognitionResponse value) { System.out.println(\u0026#34;Result: \u0026#34; + value.toString()); } @Override public void onError(Throwable t) { System.err.println(\u0026#34;Error with recognition:\u0026#34; + t.toString()); } @Override public void onCompleted() { System.out.println(\u0026#34;Server is done sending responses back\u0026#34;); } }; // Setup bidirectional stream  StreamObserver\u0026lt;StreamingRecognizeRequest\u0026gt; requestObserver; // Outgoing messages are sent on this request object  requestObserver = mCubicService.streamingRecognize(mRecognitionResponseObserver); // Send config message  RecognitionConfig cfg = RecognitionConfig.newBuilder() .setModelId(\u0026#34;ModelID\u0026#34;) .build(); StreamingRecognizeRequest configs = StreamingRecognizeRequest.newBuilder() // Note, we do not call setAudio here.  .setConfig(cfg) .build(); requestObserver.onNext(configs); // Setup the Android Micorphone Recorder  int SAMPLE_RATE = 8000; // Same as the model is expecting  int BUFFER_SIZE = 1024; AudioRecord recorder = new AudioRecord( MediaRecorder.AudioSource.MIC, SAMPLE_RATE, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT, BUFFER_SIZE); byte[] audioBuffer = new byte[BUFFER_SIZE]; recorder.startRecording(); // Read the file in chunks and stream to server.  while (running) { recorder.read(audioBuffer, 0, BUFFER_SIZE, AudioRecord.READ_BLOCKING); // Convert byte[] to ByteString for gRPC  ByteString audioBS = ByteString.copyFrom(audioBuffer); // Send audio to server  RecognitionAudio audioMsg = RecognitionAudio.newBuilder() .setData(audioBS) .build(); requestObserver.onNext(StreamingRecognizeRequest.newBuilder() .setAudio(audioMsg) .build()); } // Stop the microphone recoding.  recorder.stop(); // Close the client side stream  requestObserver.onCompleted(); // Note: Once the server is done transcribing everything, it will call responseObserver.onCompleted(). }   // For a complete iOS example, see the `AudioRecorder class` in the examples-ios github repository: // https://github.com/cobaltspeech/examples-ios/blob/master/CubicExample/AudioRecorder.swift   "
},
{
	"uri": "/using-cubic-sdk/recognition-context/",
	"title": "Recognition Context",
	"tags": [],
	"description": "",
	"content": "Cubic allows users to send context information with a recognition request which may aid the speech recognition. For example, if you have a list of names that you want to make sure the Cubic model transcribes correctly, with the correct spelling, then you may provide the list in the form of a RecognitionContext object along with the RecognitionConfig before streaming data.\nCubic models allow different sets of \u0026ldquo;context tokens\u0026rdquo; each of which can be paired with a list of words or phrases. For example, a Cubic model may have a context token for airport names, and you can provide a list of airport names you want to be recognized correctly for this context token. Likewise, models may also be configured with tokens for \u0026ldquo;contact list names\u0026rdquo;, \u0026ldquo;menu items\u0026rdquo;, \u0026ldquo;medical jargon\u0026rdquo; etc.\nTo ensure that there is no added latency in processing the list of words or phrases during a recognition request, we have a API method called CompileContext() that allows the user to compile the list into a compact, efficient format for passing to the Recognize() or StreamingRecognize() methods.\nCompiling Recognition Context We have several examples in different langagues below showing you how to compile context data and send it during a recognition request.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) func main() { // creating client without TLS. Remove WithInsecure() if using TLS  serverAddr := \u0026#34;127.0.0.1:2727\u0026#34; client, err := cubic.NewClient(serverAddr, cubic.WithInsecure()) if err != nil { log.Fatal(err) } defer client.Close() // Get list of available models  modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } for _, m := range modelResp.Models { fmt.Printf(\u0026#34;\\nID = %v, Name = %v, Supports Context = %v\\n\u0026#34;, m.Id, m.Name, m.Attributes.ContextInfo.SupportsContext) // printing allowed context tokens  if m.Attributes.ContextInfo.SupportsContext { fmt.Printf(\u0026#34;Allowed Context Tokens = %v\\n\\n\u0026#34;, strings.Join(m.Attributes.ContextInfo.AllowedContextTokens, \u0026#34;, \u0026#34;)) } } // Assuming the first model supports context  model := modelResp.Models[0] // Let\u0026#39;s say this model has an allowed context token called \u0026#34;airport_names\u0026#34; and  // we have a list of airport names that we want to make sure the recognizer gets  // right. We compile the list of names using the CompileContext(), save the compiled  // data and send it back with subsequent recognize requests to customize and improve the results.  // a small example list  phrases := []string{\u0026#34;NARITA\u0026#34;, \u0026#34;KUALA LUMPUR INTERNATIONAL\u0026#34;, \u0026#34;ISTANBUL ATATURK\u0026#34;, \u0026#34;LAGUARDIA\u0026#34;} contextToken := model.Attributes.ContextInfo.AllowedContextTokens[0] // \u0026#34;airport_names\u0026#34;  // sending request to server  compiledResp, err := client.CompileContext( context.Background(), model.Id, contextToken, phrases, nil) if err != nil { log.Fatal(err) } // saving the compiled result for later use; note this compiled data is only  // compatible with the model whose ID was provided in the CompileContext call  compiledContexts := make([]*cubicpb.CompiledContext, 0) compiledContexts = append(compiledContexts, compiledResp.Context) // Now we can send a recognize request along with the compiled context. The  // context data is provided through the recognition config as a list of compiled  // contexts (i.e. we can provide more than one compiled context if the model  // supports more than one context token).  cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, AudioEncoding: cubicpb.RecognitionConfig_WAV, Context: \u0026amp;cubicpb.RecognitionContext{Compiled: compiledContexts}, } // The rest is the same as a usual streaming recognize request  // open audio file stream  f, err := os.Open(\u0026#34;test.wav\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() // define a callback function to handle results  resultHandler := func(resp *cubicpb.RecognitionResponse) { for _, r := range resp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } } err = client.StreamingRecognize(context.Background(), cfg, f, resultHandler) if err != nil { log.Fatal(err) } }   import cubic # set insecure to False if server uses TLS serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress, insecure=True) # Get list of available models modelResp = client.ListModels() for m in modelResp.models: print(\u0026#34;\\nID = {}, Name = {}, Supports Context = {}\u0026#34;.format( m.id, m.name, m.attributes.context_info.supports_context)) if m.attributes.context_info.supports_context: # printing allowed context tokens print(\u0026#34;Allowed context tokens = {}\\n\u0026#34;.format( str(m.attributes.context_info.allowed_context_tokens))) # Assuming the first model supports context model = modelResp.models[0] # Let\u0026#39;s say this model has an allowed context token called \u0026#34;airport_names\u0026#34; and # we have a list of airport names that we want to make sure the recognizer gets # right. We compile the list of names using the CompileContext(), save the compiled # data and send it back with subsequent recognize requests to customize and improve the results. # a small example list phrases = [\u0026#34;NARITA\u0026#34;, \u0026#34;KUALA LUMPUR INTERNATIONAL\u0026#34;, \u0026#34;ISTANBUL ATATURK\u0026#34;, \u0026#34;LAGUARDIA\u0026#34;] contextToken = model.attributes.context_info.allowed_context_tokens[0] # \u0026#34;airport_names\u0026#34; # sending request to server compiledResp = client.CompileContext(model.id, contextToken, phrases) # saving the compiled result for later use; note this compiled data is only # compatible with the model whose ID was provided in the CompileContext call compiledContexts = [] compiledContexts.append(compiledResp.context) # Now we can send a recognize request along with the compiled context. The # context data is provided through the recognition config as a list of compiled # contexts (i.e. we can provide more than one compiled context if the model # supports more than one context token). cfg = cubic.RecognitionConfig( model_id=model.id, audio_encoding=\u0026#34;WAV\u0026#34;, context=cubic.RecognitionContext(compiled=compiledContexts), ) # The rest is the same as a usual streaming recognize request # open audio file stream audio = open(\u0026#39;test.wav\u0026#39;, \u0026#39;rb\u0026#39;) # send streaming request to cubic and print out results as they come in for resp in client.StreamingRecognize(cfg, audio): for result in resp.results: if result.is_partial: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\u0026#34;) else: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\\n\u0026#34;)   using System; using System.Collections.Generic; using System.IO; using System.Linq; using System.Threading.Tasks; using Grpc.Core; namespace CubicRecognitionContextExample { class Program { static async Task Main(string[] args) { // set creds = new Grpc.Core.SslCredentials(); if using TLS  var serverAddress = \u0026#34;127.0.0.1:2727\u0026#34;; var creds = Grpc.Core.ChannelCredentials.Insecure; // Initialize a gRPC connection  var channel = new Grpc.Core.Channel(serverAddress, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel); // Get list of available models  var listModelsRequest = new CobaltSpeech.Cubic.ListModelsRequest(); var modelResp = client.ListModels(listModelsRequest); foreach (var m in modelResp.Models) { Console.WriteLine(\u0026#34;\\nID = {0}, Name = {1}, Supports Context = {2}\u0026#34;, m.Id, m.Name, m.Attributes.ContextInfo.SupportsContext); // printing allowed context tokens  if (m.Attributes.ContextInfo.SupportsContext) { Console.WriteLine(\u0026#34;Allowed Context Tokens = {0}\\n\u0026#34;, m.Attributes.ContextInfo.AllowedContextTokens); } } // Assuming the first model supports context  var model = modelResp.Models[0]; // Let\u0026#39;s say this model has an allowed context token called \u0026#34;airport_names\u0026#34; and  // we have a list of airport names that we want to make sure the recognizer gets  // right. We compile the list of names using the CompileContext(), save the compiled  // data and send it back with subsequent recognize requests to customize and improve the results.  // a small example list  string[] phrases = { \u0026#34;NARITA\u0026#34;, \u0026#34;KUALA LUMPUR INTERNATIONAL\u0026#34;, \u0026#34;ISTANBUL ATATURK\u0026#34;, \u0026#34;LAGUARDIA\u0026#34; }; string contextToken = model.Attributes.ContextInfo.AllowedContextTokens[0]; // \u0026#34;airport_names\u0026#34;  // create compile context request  var compileRequest = new CobaltSpeech.Cubic.CompileContextRequest { ModelId = model.Id, Token = contextToken, }; // put phrases into an compileRequest.Phrases  foreach (var phrase in phrases) { compileRequest.Phrases.Add( new CobaltSpeech.Cubic.ContextPhrase { Text = phrase, }); } // send request to server  var compiledResp = client.CompileContext(compileRequest); // saving the compiled result for later use; note this compiled data is only  // compatible with the model whose ID was provided in the CompileContext call  var compiledContexts = new List\u0026lt;CobaltSpeech.Cubic.CompiledContext\u0026gt;(); compiledContexts.Add(compiledResp.Context); // Now we can send a recognize request along with the compiled context. The  // context data is provided through the recognition config as a list of compiled  // contexts (i.e. we can provide more than one compiled context if the model  // supports more than one context token).  var cfg = new CobaltSpeech.Cubic.RecognitionConfig { ModelId = model.Id, AudioEncoding = CobaltSpeech.Cubic.RecognitionConfig.Types.Encoding.Wav, Context = new CobaltSpeech.Cubic.RecognitionContext(), }; foreach (var ctx in compiledContexts) { cfg.Context.Compiled.Add(ctx); } // The rest is the same as a usual streaming recognize request  string audioPath = \u0026#34;test.wav\u0026#34;; // Setup the bi-directional gRPC stream.  var call = client.StreamingRecognize(); using(call) { // Setup recieve task  var responseReaderTask = Task.Run(async() =\u0026gt; { // Wait for the next response  while (await call.ResponseStream.MoveNext()) { var response = call.ResponseStream.Current; foreach (var result in response.Results) { Console.WriteLine(result.Alternatives[0].Transcript); } } }); // Send config first, followed by the audio  { // Send the configs  var request = new CobaltSpeech.Cubic.StreamingRecognizeRequest(); request.Config = cfg; await call.RequestStream.WriteAsync(request); // Setup object for streaming audio  request.Config = null; request.Audio = new CobaltSpeech.Cubic.RecognitionAudio { }; // Send the audio, in 8kb chunks  const int chunkSize = 8192; using(FileStream file = File.OpenRead(audioPath)) { int bytesRead; var buffer = new byte[chunkSize]; while ((bytesRead = file.Read(buffer, 0, buffer.Length)) \u0026gt; 0) { var bytes = Google.Protobuf.ByteString.CopyFrom(buffer.Take(bytesRead).ToArray()); request.Audio.Data = bytes; await call.RequestStream.WriteAsync(request); } // Close the sending stream  await call.RequestStream.CompleteAsync(); } } // Wait for all of the responses to come back through the receiving stream  await responseReaderTask; } } } }   import Foundation import Cubic class CubicExample { // set useTLS to true if using TLS let client = Client(host: \u0026#34;127.0.0.1\u0026#34;, port: 2727, useTLS: false) var config = Cobaltspeech_Cubic_RecognitionConfig() let fileName = \u0026#34;test.wav\u0026#34; let chunkSize = 8192 public init() { let dispatchGroup = DispatchGroup() dispatchGroup.enter() var model: Cobaltspeech_Cubic_Model! client.listModels(success: { (models) in guard let models = models else { return } for model in models { print(\u0026#34;\\nID = \\(model.id), Name = \\(model.name), Supports Context = \\(model.attributes.contextInfo.supportsContext)\u0026#34;) // printing allowed context tokens if model.attributes.contextInfo.supportsContext { print(\u0026#34;Allowed Context Tokens = \\(model.attributes.contextInfo.allowedContextTokens)\\n\u0026#34;) } } // Assuming the first model supports context if let firstModel = models.first { model = firstModel } else { return } dispatchGroup.leave() }) { (error) in print(error.localizedDescription) dispatchGroup.leave() } dispatchGroup.wait() // Let\u0026#39;s say this model has an allowed context token called \u0026#34;airport_names\u0026#34; and // we have a list of airport names that we want to make sure the recognizer gets // right. We compile the list of names using the CompileContext(), save the compiled // data and send it back with subsequent recognize requests to customize and improve the results. // a small example list let phrases = [\u0026#34;NARITA\u0026#34;, \u0026#34;KUALA LUMPUR INTERNATIONAL\u0026#34;, \u0026#34;ISTANBUL ATATURK\u0026#34;, \u0026#34;LAGUARDIA\u0026#34;] let contextToken = model.attributes.contextInfo.allowedContextTokens[0] // \u0026#34;airport_names\u0026#34; // create compile context request var compileRequest = Cobaltspeech_Cubic_CompileContextRequest() compileRequest.modelID = model.id compileRequest.token = contextToken // put phrases into an compileRequest.Phrases for phrase in phrases { var contextPhrase = Cobaltspeech_Cubic_ContextPhrase() contextPhrase.text = phrase compileRequest.phrases.append(contextPhrase) } // send request to server var compiledContexts: [Cobaltspeech_Cubic_CompiledContext] = [] dispatchGroup.enter() self.client.compileContext(compileRequest).response.whenComplete { (result) in switch result { case .success(let response): // saving the compiled result for later use; note this compiled data is only // compatible with the model whose ID was provided in the CompileContext call compiledContexts.append(response.context) dispatchGroup.leave() case .failure(let error): print(error.localizedDescription) dispatchGroup.leave() } } dispatchGroup.wait() // Now we can send a recognize request along with the compiled context. The // context data is provided through the recognition config as a list of compiled // contexts (i.e. we can provide more than one compiled context if the model // supports more than one context token). self.config.modelID = model.id self.config.audioEncoding = .wav self.config.context = Cobaltspeech_Cubic_RecognitionContext() self.config.context.compiled.append(contentsOf: compiledContexts) // The rest is the same as a usual streaming recognize request let fileUrl = URL(fileURLWithPath: self.fileName) guard let audioData = try? Data(contentsOf: fileUrl) else { return } dispatchGroup.enter() self.client.streamingRecognize(audioData: audioData, chunkSize: self.chunkSize, config: self.config, success: { (response) in for result in response.results { if !result.isPartial, let alternative = result.alternatives.first { print(alternative.transcript) } } dispatchGroup.leave() }) { (error) in print(error.localizedDescription) dispatchGroup.leave() } } }   "
},
{
	"uri": "/using-cubic-sdk/client-configs/",
	"title": "Recognition Configurations",
	"tags": [],
	"description": "",
	"content": "An in-depth explanation of the various fields of the complete SDK can be found here. The sub-section RecognitionConfig is particularly important here.\nThis page here discusses the more common combinations sent to the server.\nFields Here is a quick overview of the fields.\n   Field Required Default Description     model_id Yes  Unique ID of the model to use.   audio_encoding Yes  Encoding format of the audio, such as RAW_LINEAR_16, WAV, MP3, etc.   idle_timeout No 0s (Unlimited) Maximum time allowed between each gRPC message. The server may place further restrictions depending on its configuration.   enable_word_time_offsets No false Toggles the calculation of word-level timestamps. The specified model must also support word-level timestamps for this field to be populated.   enable_word_confidence No false Toggles the calculation of word-level confidence scores. The specified model must also support word-level confidence for this field to be populated.   enable_raw_transcript No false If true, the raw transcript will be included in the results.   enable_confusion_network No false Toggles the inclusion of a confusion network, consisting of multiple alternative transcriptions. The specified model must also support confusion networks for this field to be populated.   audio_channels No [0] (mono) Specifies which channels of a multi-channel audio file to be transcribed, each as their own individual audio stream.   metadata No \u0026quot;\u0026rdquo; Can be used to send any custom metadata associated with the audio being sent.The server may record this metadata when processing the request. The server does not use this field for any other purpose.   context No nil Can be used to provide any context information that can aid speech recognition, such as probable phrases or words that may appear in the recognition output or even out of vocabulary words for the model being used. Currently all context information must first be pre-compiled via the CompileContext().    Use cases The most basic use case is getting a formatted transcript. This would simply need a config such as:\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34; } and your resulting transcript would be found at results.alternatives[*].transcript.\n Sometimes, only the raw (unformatted) transcript is desired. In this case, there are two options.\n Disable server-side formatting and use the above config. Retrieve the raw transcript from the results.alternatives[*].transcript field. Specify the enable_raw_transcript = true flag, and access the field results.alternatives[*].raw_transcript.   Note: Prior to cubicsvr v2.9.0 and SDK-Cubic v1.3.0, the field results.alternatives[*].transcript was populated with either the raw or the formatted transcription depending on the enable_raw_transcript config. After these changes, raw transcripts have been pushed to a new field results.alternatives[*].raw_transcript, and only populated when enable_raw_transcript is set to true.\n  Another use case would be getting both the formatted and raw transcript. This can be done using this config.\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: \u0026#34;true\u0026#34; } The formatted transcript would be found at results.alternatives[*].transcript, and the raw transcript would be found at results.alternatives[*].raw_transcript.\n If you need to know the timestamp for each word, to align subtitles with a video, for example, then you can use this config to enable those word-level timestamps. Please note the inclusion of enable_raw_transcript; the word-level information corresponds to the raw transcript, since the formatter may combine multiple words into one symbol in the formatted transcript (e.g. \u0026ldquo;twenty one\u0026rdquo; to \u0026ldquo;21\u0026rdquo;)\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_word_time_offsets\u0026#34;: true } Timestamps will be found at results.alternatives[*].words[*].start_time and results.alternatives[*].words[*].duration.\n Word-level confidences, i.e. for displaying a lighter color for less confident words, can be included much like the word-level timestamps. Please note the inclusion of enable_raw_transcript.\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_word_time_offsets\u0026#34;: true } Word-level confidence scores will be found at results.alternatives[*].words[*].confidence.\n For applications that need more than the one-best transcription, the most comprehensive and detailed Cubic results are found in the confusion network. Please refer to the in depth confusion network documentation to see what is included.\nTo enable the confusion network, the config will look similar to this:\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_confusion_network\u0026#34;: true } The confusion network can be accessed at result.cnet.\n"
},
{
	"uri": "/using-cubic-sdk/android/",
	"title": "Android Integrations",
	"tags": [],
	"description": "",
	"content": "Adding the protobuf-gradle-plugin to your Android project In your root build.gradle file, add a new protobuf-gradle-plugin dependency\nbuildscript { // ...  dependencies { // ...  classpath \u0026#34;com.google.protobuf:protobuf-gradle-plugin:0.8.10\u0026#34; // ...  } } This will allow the app\u0026rsquo;s gradle build script to generate the protobuf code.\nGenerating code from protobuf files Next, you will have to add the code to actually generate the files. To generate the gRPC code, modify your app/build.gradle file with the following:\napply plugin: \u0026#39;com.android.application\u0026#39; // Should already exist apply plugin: \u0026#39;com.google.protobuf\u0026#39; // Add this line  android { /*...*/} // This section adds a step to generate the gRPC code from `app/src/main/proto` proto files. // This section adds a step to generate the gRPC code from `app/src/main/proto` proto files // and make it available to your java/kotlin code. protobuf { protoc { artifact = \u0026#39;com.google.protobuf:protoc:3.10.0\u0026#39; } plugins { javalite { artifact = \u0026#34;com.google.protobuf:protoc-gen-javalite:3.0.0\u0026#34; } grpc { artifact = \u0026#39;io.grpc:protoc-gen-grpc-java:1.24.0\u0026#39; } } generateProtoTasks { all().each { task -\u0026gt; task.plugins { javalite {} grpc { // Options added to --grpc_out  option \u0026#39;lite\u0026#39; // the gRPC documentation suggests using lite in android applications  } } } } } // Runtime dependencies dependencies { // Existing dependencies ...  // gRPC Libraries  implementation \u0026#39;io.grpc:grpc-okhttp:1.24.0\u0026#39; implementation \u0026#39;io.grpc:grpc-protobuf-lite:1.24.0\u0026#39; // the gRPC documentation suggests using lite in android applications  implementation \u0026#39;io.grpc:grpc-stub:1.24.0\u0026#39; implementation \u0026#39;io.grpc:grpc-auth:1.24.0\u0026#39; implementation \u0026#39;javax.annotation:javax.annotation-api:1.2\u0026#39; } By default, generateProtoTasks assumes all protofiles are availabe at app/src/main/proto. To include proto files somewhere else, add lines such as this:\ndependencies { // ...  protobuf files(\u0026#34;lib/protos.tar.gz\u0026#34;) protobuf files(\u0026#34;/path/to/other/folder/to/include/\u0026#34;) } Downloading the latest cubic.proto file You can find our proto file at https://github.com/cobaltspeech/sdk-cubic/blob/master/grpc/cubic.proto. Copy this file to app/src/main/proto/cubic.proto.\ncubic.proto also relies on a few other proto files:\n   Name URL     cubic.proto https://github.com/cobaltspeech/sdk-cubic/blob/master/grpc/cubic.proto   google/api/annotations.proto https://github.com/googleapis/googleapis/blob/6ae2d42/google/api/annotations.proto   google/api/http.proto https://github.com/googleapis/googleapis/blob/6ae2d42/google/api/http.proto   google/protobuf/descriptor.proto https://github.com/protocolbuffers/protobuf/blob/044c766/src/google/protobuf/descriptor.proto   google/protobuf/duration.proto https://github.com/protocolbuffers/protobuf/blob/044c766/src/google/protobuf/duration.proto    Once you have all of these files downloaded, your file structure would look like this:\napp/src/main/proto/ â”œâ”€â”€ cubic.proto â””â”€â”€ google â”œâ”€â”€ api â”‚ â”œâ”€â”€ annotations.proto â”‚ â””â”€â”€ http.proto â””â”€â”€ protobuf â”œâ”€â”€ descriptor.proto â””â”€â”€ duration.proto At this point, you should be able to do a Build\u0026gt;Clean Build and Build\u0026gt;Rebuild Project.\nContact us We know it takes work to get environments set up. If you have any problems, don\u0026rsquo;t hesitate to contact us.\n"
},
{
	"uri": "/protobuf/autogen-doc-cubic-proto/",
	"title": "Cubic Protobuf API Docs",
	"tags": [],
	"description": "",
	"content": "cubic.proto Service: Cubic Service that implements the Cobalt Cubic Speech Recognition API\n   Method Name Request Type Response Type Description     Version .google.protobuf.Empty VersionResponse Queries the Version of the Server   ListModels ListModelsRequest ListModelsResponse Retrieves a list of available speech recognition models   Recognize RecognizeRequest RecognitionResponse Performs synchronous speech recognition: receive results after all audio has been sent and processed. It is expected that this request be typically used for short audio content: less than a minute long. For longer content, the StreamingRecognize method should be preferred.   StreamingRecognize StreamingRecognizeRequest RecognitionResponse Performs bidirectional streaming speech recognition. Receive results while sending audio. This method is only available via GRPC and not via HTTP+JSON. However, a web browser may use websockets to use this service.   CompileContext CompileContextRequest CompileContextResponse Compiles recognition context information, such as a specialized list of words or phrases, into a compact, efficient form to send with subsequent Recognize or StreamingRecognize requests to customize speech recognition. For example, a list of contact names may be compiled in a mobile app and sent with each recognition request so that the app user\u0026rsquo;s contact names are more likely to be recognized than arbitrary names. This pre-compilation ensures that there is no added latency for the recognition request. It is important to note that in order to compile context for a model, that model has to support context in the first place, which can be verified by checking its ModelAttributes.ContextInfo obtained via the ListModels method. Also, the compiled data will be model specific; that is, the data compiled for one model will generally not be usable with a different model.    Message: CompileContextRequest The top-level message sent by the client for the CompileContext request. It contains a list of phrases or words, paired with a context token included in the model being used. The token specifies a category such as \u0026ldquo;menu_item\u0026rdquo;, \u0026ldquo;airport\u0026rdquo;, \u0026ldquo;contact\u0026rdquo;, \u0026ldquo;product_name\u0026rdquo; etc. The context token is used to determine the places in the recognition output where the provided list of phrases or words may appear. The allowed context tokens for a given model can be found in its ModelAttributes.ContextInfo obtained via the ListModels method.\n   Field Type Label Description     model_id string  Unique identifier of the model to compile the context information for. The model chosen needs to support context which can be verified by checking its ModelAttributes.ContextInfo obtained via ListModels.\n   token string  The token that is associated with the provided list of phrases or words (e.g \u0026ldquo;menu_item\u0026rdquo;, \u0026ldquo;airport\u0026rdquo; etc.). Must be one of the tokens included in the model being used, which can be retrieved by calling the ListModels method.\n   phrases ContextPhrase repeated List of phrases and/or words to be compiled.\n    Message: CompileContextResponse The message returned to the client by the CompileContext method.\n   Field Type Label Description     context CompiledContext  Context information in a compact form that is efficient for use in subsequent recognition requests. The size of the compiled form will depend on the amount of text that was sent for compilation. For 1000 words it\u0026rsquo;s generally less than 100 kilobytes.\n    Message: CompiledContext Context information in a compact form that is efficient for use in subsequent recognition requests. The size of the compiled form will depend on the amount of text that was sent for compilation. For 1000 words it\u0026rsquo;s generally less than 100 kilobytes.\n   Field Type Label Description     data bytes  The context information compiled by the CompileContext method.\n    Message: ConfusionNetworkArc An Arc inside a Confusion Network Link\n   Field Type Label Description     word string  Word in the recognized transcript\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n    Message: ConfusionNetworkLink A Link inside a confusion network\n   Field Type Label Description     start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this link\n   duration google.protobuf.Duration  Duration of the current link in the confusion network\n   arcs ConfusionNetworkArc repeated Arcs between this link\n    Message: ContextInfo Model information specifc to supporting recognition context.\n   Field Type Label Description     supports_context bool  If this is set to true, the model supports taking context information into account to aid speech recognition. The information may be sent with with recognition requests via RecognitionContext inside RecognitionConfig.\n   allowed_context_tokens string repeated A list of tokens (e.g \u0026ldquo;name\u0026rdquo;, \u0026ldquo;airport\u0026rdquo; etc.) that serve has placeholders in the model where a client provided list of phrases or words may be used to aid speech recognition and produce the exact desired recognition output.\n    Message: ContextPhrase A phrase or word that is to be compiled into context information that can be later used to improve speech recognition during a Recognize or StreamingRecognize call. Along with the phrase or word itself, there is an optional boost parameter that can be used to boost the likelihood of the phrase or word in the recognition output.\n   Field Type Label Description     text string  The actual phrase or word.\n   boost float  This is an optional field. The boost value is a positive number which is used to increase the probability of the phrase or word appearing in the output. This setting can be used to differentiate between similar sounding words, with the desired word given a bigger boost value.\nBy default, all phrases or words are given an equal probability of 1/N (where N = total number of phrases or words). If a boost value is provided, the new probability is (boost + 1) * 1/N. We normalize the boosted probabilities for all the phrases or words so that they sum to one. This means that the boost value only has an effect if there are relative differences in the values for different phrases or words. That is, if all phrases or words have the same boost value, after normalization they will all still have the same probability. This also means that the boost value can be any positive value, but it is best to stick between 0 to 20.\nNegative values are not supported and will be treated as 0 values.\n    Message: ListModelsRequest The top-level message sent by the client for the ListModels method.\nThis message is empty and has no fields.\nMessage: ListModelsResponse The message returned to the client by the ListModels method.\n   Field Type Label Description     models Model repeated List of models available for use that match the request.\n    Message: Model Description of a Cubic Model\n   Field Type Label Description     id string  Unique identifier of the model. This identifier is used to choose the model that should be used for recognition, and is specified in the RecognitionConfig message.\n   name string  Model name. This is a concise name describing the model, and maybe presented to the end-user, for example, to help choose which model to use for their recognition task.\n   attributes ModelAttributes  Model attributes\n    Message: ModelAttributes Attributes of a Cubic Model\n   Field Type Label Description     sample_rate uint32  Audio sample rate supported by the model\n   context_info ContextInfo  Attributes specifc to supporting recognition context.\n    Message: RecognitionAlternative A recognition hypothesis\n   Field Type Label Description     transcript string  Text representing the transcription of the words that the user spoke.\nThe transcript will be formatted according to the servers formatting configuration. If you want the raw transcript, please see the field raw_transcript. If the server is configured to not use any formatting, then this field will contain the raw transcript.\nAs an example, if the spoken utterance was \u0026ldquo;four people\u0026rdquo;, and the server was configured to format numbers, this field would be set to \u0026ldquo;4 people\u0026rdquo;.\n   raw_transcript string  Text representing the transcription of the words that the user spoke, without any formatting. This field will be populated only the config RecognitionConfig.enable_raw_transcript is set to true. Otherwise this field will be an empty string. If you want the formatted transcript, please see the field transcript.\nAs an example, if the spoken utterance was here are four words, this field would be set to \u0026ldquo;HERE ARE FOUR WORDS\u0026rdquo;.\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood of the output being correct.\n   words WordInfo repeated A list of word-specific information for each recognized word. This is available only if enable_word_confidence or enable_word_time_offsets was set to true in the RecognitionConfig.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this utterance.\n   duration google.protobuf.Duration  Duration of the current utterance in the spoken audio.\n    Message: RecognitionAudio Audio to be sent to the recognizer\n   Field Type Label Description     data bytes  \n    Message: RecognitionConfig Configuration for setting up a Recognizer\n   Field Type Label Description     model_id string  Unique identifier of the model to use, as obtained from a Model message.\n   audio_encoding RecognitionConfig.Encoding  Encoding of audio data sent/streamed through the RecognitionAudio messages. For encodings like WAV/MP3 that have headers, the headers are expected to be sent at the beginning of the stream, not in every RecognitionAudio message.\nIf not specified, the default encoding is RAW_LINEAR16.\nDepending on how they are configured, server instances of this service may not support all the encodings enumerated above. They are always required to accept RAW_LINEAR16. If any other Encoding is specified, and it is not available on the server being used, the recognition request will result in an appropriate error message.\n   idle_timeout google.protobuf.Duration  Idle Timeout of the created Recognizer. If no audio data is received by the recognizer for this duration, ongoing rpc calls will result in an error, the recognizer will be destroyed and thus more audio may not be sent to the same recognizer. The server may impose a limit on the maximum idle timeout that can be specified, and if the value in this message exceeds that serverside value, creating of the recognizer will fail with an error.\n   enable_word_time_offsets bool  This is an optional field. If this is set to true, each result will include a list of words and the start time offset (timestamp) and the duration for each of those words. If set to false, no word-level timestamps will be returned. The default is false.\n   enable_word_confidence bool  This is an optional field. If this is set to true, each result will include a list of words and the confidence for those words. If false, no word-level confidence information is returned. The default is false.\n   enable_raw_transcript bool  This is an optional field. If this is set to true, the field RecognitionAlternative.raw_transcript will be populated with the raw transcripts output from the recognizer will be exposed without any formatting rules applied. If this is set to false, that field will not be set in the results. The RecognitionAlternative.transcript will always be populated with text formatted according to the server\u0026rsquo;s settings.\n   enable_confusion_network bool  This is an optional field. If this is set to true, the results will include a confusion network. If set to false, no confusion network will be returned. The default is false. If the model being used does not support a confusion network, results may be returned without a confusion network available. If this field is set to true, then enable_raw_transcript is also forced to be true.\n   audio_channels uint32 repeated This is an optional field. If the audio has multiple channels, this field should be configured with the list of channel indices that should be transcribed. Channels are 0-indexed.\nExample: [0] for a mono file, [0, 1] for a stereo file.\nIf this field is not set, a mono file will be assumed by default and only channel-0 will be transcribed even if the file actually has additional channels.\nChannels that are present in the audio may be omitted, but it is an error to include a channel index in this field that is not present in the audio. Channels may be listed in any order but the same index may not be repeated in this list.\nBAD: [0, 2] for a stereo file; BAD: [0, 0] for a mono file.\n   metadata RecognitionMetadata  This is an optional field. If there is any metadata associated with the audio being sent, use this field to provide it to cubic. The server may record this metadata when processing the request. The server does not use this field for any other purpose.\n   context RecognitionContext  This is an optional field for providing any additional context information that may aid speech recognition. This can also be used to add out-of-vocabulary words to the model or boost recognition of specific proper names or commands. Context information must be pre-compiled via the CompileContext() method.\n    Message: RecognitionConfusionNetwork Confusion network in recognition output\n   Field Type Label Description     links ConfusionNetworkLink repeated \n    Message: RecognitionContext A collection of additional context information that may aid speech recognition. This can be used to add out-of-vocabulary words to\nthe model or to boost recognition of specific proper names or commands.\n   Field Type Label Description     compiled CompiledContext repeated List of compiled context information, with each entry being compiled from a list of words or phrases using the CompileContext method.\n    Message: RecognitionMetadata Metadata associated with the audio to be recognized.\n   Field Type Label Description     custom_metadata string  Any custom metadata that the client wants to associate with the recording. This could be a simple string (e.g. a tracing ID) or structured data (e.g. JSON)\n    Message: RecognitionResponse Collection of sequence of recognition results in a portion of audio. When transcribing a single audio channel (e.g. RAW_LINEAR16 input, or a mono file), results will be ordered chronologically. When transcribing multiple channels, the results of all channels will be interleaved. Results of each individual channel will be chronological. No such promise is made for the ordering of results of different channels, as results are returned for each channel individually as soon as they are ready.\n   Field Type Label Description     results RecognitionResult repeated \n    Message: RecognitionResult A recognition result corresponding to a portion of audio.\n   Field Type Label Description     alternatives RecognitionAlternative repeated An n-best list of recognition hypotheses alternatives\n   is_partial bool  If this is set to true, it denotes that the result is an interim partial result, and could change after more audio is processed. If unset, or set to false, it denotes that this is a final result and will not change.\nServers are not required to implement support for returning partial results, and clients should generally not depend on their availability.\n   cnet RecognitionConfusionNetwork  If enable_confusion_network was set to true in the RecognitionConfig, and if the model supports it, a confusion network will be available in the results.\n   audio_channel uint32  Channel of the audio file that this result was transcribed from. For a mono file, or RAW_LINEAR16 input, this will be set to 0.\n    Message: RecognizeRequest The top-level message sent by the client for the Recognize method. Both the RecognitionConfig and RecognitionAudio fields are required. The entire audio data must be sent in one request. If your audio data is larger, please use the StreamingRecognize call..\n   Field Type Label Description     config RecognitionConfig  Provides configuration to create the recognizer.\n   audio RecognitionAudio  The audio data to be recognized\n    Message: StreamingRecognizeRequest The top-level message sent by the client for the StreamingRecognize request. Multiple StreamingRecognizeRequest messages are sent. The first message must contain a RecognitionConfig message only, and all subsequent messages must contain RecognitionAudio only. All RecognitionAudio messages must contain non-empty audio. If audio content is empty, the server may interpret it as end of stream and stop accepting any further messages.\n   Field Type Label Description     config RecognitionConfig  \n   audio RecognitionAudio  \n    Message: VersionResponse The message sent by the server for the Version method.\n   Field Type Label Description     cubic string  version of the cubic library handling the recognition\n   server string  version of the server handling these requests\n    Message: WordInfo Word-specific information for recognized words\n   Field Type Label Description     word string  The actual word in the text\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this spoken word.\n   duration google.protobuf.Duration  Duration of the current word in the spoken audio.\n    Enum: RecognitionConfig.Encoding The encoding of the audio data to be sent for recognition.\nFor best results, the audio source should be captured and transmitted using the RAW_LINEAR16 encoding.\n   Name Number Description     RAW_LINEAR16 0 Raw (headerless) Uncompressed 16-bit signed little endian samples (linear PCM), single channel, sampled at the rate expected by the chosen Model.   WAV 1 WAV (data with RIFF headers), with data sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   MP3 2 MP3 data, sampled at a rate equal to or higher than the sampling rate expected by the chosen Model.   FLAC 3 FLAC data, sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   VOX8000 4 VOX data (Dialogic ADPCM), sampled at 8 KHz.   ULAW8000 5 Î¼-law (8-bit) encoded RAW data, single channel, sampled at 8 KHz.    Scalar Value Types    .proto Type Notes Go Type Python Type     double  float64 float   float  float32 float   int32 Uses variable-length encoding. Inefficient for encoding negative numbers â€“ if your field is likely to have negative values, use sint32 instead. int32 int   int64 Uses variable-length encoding. Inefficient for encoding negative numbers â€“ if your field is likely to have negative values, use sint64 instead. int64 int/long   uint32 Uses variable-length encoding. uint32 int/long   uint64 Uses variable-length encoding. uint64 int/long   sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int   sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 int/long   fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int   fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 int/long   sfixed32 Always four bytes. int32 int   sfixed64 Always eight bytes. int64 int/long   bool  bool boolean   string A string must always contain UTF-8 encoded or 7-bit ASCII text. string str/unicode   bytes May contain any arbitrary sequence of bytes. []byte str    "
},
{
	"uri": "/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Cubic SDK \u0026ndash; Cobalt\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "Cubic SDK Documentation",
	"tags": [],
	"description": "",
	"content": "Cubic API Overview Cubic is Cobaltâ€™s automatic speech recognition (ASR) engine. It can be deployed on-prem and accessed over the network or on your local machine via an API. We currently support C++, C#, Go, Java and Python, and can add support for more languages as required.\nOnce running, Cubicâ€™s API provides a method to which you can stream audio. This audio can either be from a microphone or a file. We recommend uncompressed WAV as the encoding, but support other formats such as MP3.\nCubicâ€™s API provides a number of options for returning the speech recognition results. The results are passed back using Googleâ€™s protobuf library, allowing them to be handled natively by your application. Cubic can estimate its confidence in the transcription result at the word or utterance level, along with timestamps of the words. Confidence scores are in the range 0-1. Cubicâ€™s output options are described below.\nAutomatic Transcription Results The simplest result that Cubic returns is its best guess at the transcription of your audio. Cubic recognizes the audio you are streaming, listens for the end of each utterance, and returns the speech recognition result.\nCubic maintains its transcriptions in an N-best list, i.e. is the top N transcriptions from the recogniser. The best ASR result is the first entry in this list.\n Click here to see an example json representation of Cubicâ€™s N-best list with utterance-level confidence scores { \u0026#34;alternatives\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.514 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.201 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A \u0026lt;UNK\u0026gt; DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.105 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS ISN\u0026#39;T NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.093 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A YOUR DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.087 } ], }  A single stream may consist of multiple utterances separated by silence. Cubic handles each utterance separately.\nFor longer utterances, it is often useful to see the partial speech recognition results while the audio is being streamed. For example, this allows you to see what the ASR system is predicting in real-time while someone is speaking. Cubic supports both partial and final ASR results.\nConfusion Network A Confusion Network is a form of speech recognition output thatâ€™s been turned into a compact graph representation of many possible transcriptions, as here:\nNote that \u0026lt;eps\u0026gt; in this representation is silence.\n Click here to see an example json representation of this Confusion Network object, with time stamps and word-level confidence scores { \u0026#34;cnet\u0026#34;: { \u0026#34;links\u0026#34;: [ { \u0026#34;duration\u0026#34;: \u0026#34;1.350s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;0s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.690s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;TOMORROW\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;1.350s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.080s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.040s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.168s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;IS\u0026#34;, \u0026#34;confidence\u0026#34;: 0.892 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.120s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.010s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.288s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.093s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;confidence\u0026#34;: 0.620 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.233 }, { \u0026#34;word\u0026#34;: \u0026#34;ISN\u0026#39;T\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 }, { \u0026#34;word\u0026#34;: \u0026#34;THE\u0026#34;, \u0026#34;confidence\u0026#34;: 0.039 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.298s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.005s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.391s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.273s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;NEW\u0026#34;, \u0026#34;confidence\u0026#34;: 0.661 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.129 }, { \u0026#34;word\u0026#34;: \u0026#34;YOUR\u0026#34;, \u0026#34;confidence\u0026#34;: 0.107 }, { \u0026#34;word\u0026#34;: \u0026#34;YOU\u0026#34;, \u0026#34;confidence\u0026#34;: 0.102 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.396s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.420s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.954 }, { \u0026#34;word\u0026#34;: \u0026#34;TODAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.044 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.002 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.270s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;3.090s\u0026#34; } ] } }  Formatted output Speech recognition systems typically output the words that were spoken, with no formatting. For example, utterances with numbers in might return â€œtwenty seven bridgesâ€, and â€œthe year two thousand and threeâ€. Cubic has the option to enable basic formatting of speech recognition results:\n Capitalising the first letter of the utterance Numbers: â€œcobaltâ€™s atomic number is twenty sevenâ€ -\u0026gt; â€œCobaltâ€™s atomic number is 27â€ Truecasing: â€œthe iphone was launched in two thousand and sevenâ€ -\u0026gt; â€œThe iPhone was launched in 2007â€ Ordinals: â€œsummer solstice is twenty first juneâ€ -\u0026gt; â€œSummer solstice is 21st Juneâ€  Note that word level timestamps and confidences arenâ€™t supported when formatting is enabled.\nObtaining Cubic Cobalt will provide you with a package of Cubic that contains the engine, appropriate speech recognition models and a server application. This server exports Cubic\u0026rsquo;s functionality over the gRPC protocol. The https://github.com/cobaltspeech/sdk-cubic repository contains the SDK that you can use in your application to communicate with the Cubic server. This SDK is currently available for the Go and Python languages; and we would be happy to talk to you if you need support for other languages. Most of the core SDK is generated automatically using the gRPC tools, and Cobalt provides a top level package for more convenient API calls.\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]