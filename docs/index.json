[
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/",
	"title": "Server Setup",
	"tags": [],
	"description": "",
	"content": "This section is meant to get you started using Cubic Server via a Docker image.\nIt is not necessary to go through through these steps to call the demo server for evaluation purposes, only to run Cubic Server on premises. The demo server can be found at https://demo-cubic.cobaltspeech.com.\nInstalling the Cubic Server Image The SDK communicates with a Cubic Server instance using gRPC. Cobalt distributes a docker image that contains the cubicsvr binary and model files.\n  Contact Cobalt to get a link to the image file in AWS S3. This link will expire in two weeks, so be sure to download the file to your own server.\n  Download with the AWS CLI if you have it, or with curl:\nURL=\u0026#34;the url sent by Cobalt\u0026#34; IMAGE_NAME=\u0026#34;name you want to give the file (should end with the same extension as the url, usually bz2)\u0026#34; curl $URL -L -o $IMAGE_NAME   Load the docker image\ndocker load \u0026lt; $IMAGE_NAME This will output the name of the image (e.g. cubicsvr-demo-en_us-16).\n  Start the cubic service\ndocker run -p 2727:2727 -p 8080:8080 --name cobalt cubicsvr-demo-en_us-16 That will start listening for grpc commands on port 2727 and http requests on 8080, and will stream the debug log to stdout. (You can replace --name cobalt with whatever name you want. That just provides a way to refer back to the currently running container.)\n  Verify the service is running by calling\ncurl http://localhost:8080/api/version   If you want to explore the package to see the model files etc, call the following to open the bash terminal on the previously run image. Model files are located in the /model directory.\ndocker exec -it cobalt bash   Contents of the docker image  Base docker image : debian-stretch-slim Additional dependencies  sox    Cobalt-specific files  cubicsvr - binary for performing Automatic Speech Recognition model.config - top-level config am/nnet3_online/final.mdl - this is the acoustic model am/nnet3_online/conf/online_cmvn.conf - feature extraction parameters for the features fed into the GMM model used for i-vector statistics accumulation. am/nnet3_online/conf/splice.conf - GMM feature context when accumulating statistics for i-vector accumulation. am/nnet3_online/ivector_extractor/* - Kaldi configuration files related to ivectors graph/HCLG.fst - the decoding graph: the combination of the AMs transition graph, the lexicon, and the language model graph/words.txt - an integer to word mapping, needed because the output of the HCLG graph contains only integer symbol IDs graph/phones/word_boundary.int - this is needed only when confusion network output is requested, it tells the decoder which phones are at word boundaries "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/",
	"title": "Using Cubic-SDK",
	"tags": [],
	"description": "",
	"content": "This section discusses the various aspects of using the SDK as a client.\n Installing the SDK Setting up a Connection Synchronous Recognition Streaming Recognition Recognition Configurations  It also provides an integration overview for Android applications.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/",
	"title": "Cubic API Reference",
	"tags": [],
	"description": "",
	"content": "The Cubic API is specified as a proto file. This section of the documentation is auto-generated from the spec. It describes the data types and functions defined in the spec. The \u0026ldquo;messages\u0026rdquo; below correspond to the data structures to be used, and the \u0026ldquo;service\u0026rdquo; contains the methods that can be called.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/installation/",
	"title": "Installing the SDK",
	"tags": [],
	"description": "",
	"content": "Instructions for installing the SDK are language specific.\nGo The Go SDK supports Go modules and requires Go 1.12 or later. To use the SDK, import this package into your application:\nimport \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; Python The Python SDK depends on Python \u0026gt;= 3.5. You may use pip to perform a system-wide install, or use virtualenv for a local install.\npip install --upgrade pip pip install \u0026#34;git+https://github.com/cobaltspeech/sdk-cubic#egg=cobalt-cubic\u0026amp;subdirectory=grpc/py-cubic\u0026#34; C# The C# SDK utilizes the NuGet package manager. The package is called Cubic-SDK, under the owners name of CobaltSpeech.\nNuGet allows 4 different ways to install. Further instructions can be found on the nuget webpage. Installing via the dotnet cli through the command:\ndotnet add package Cubic-SDK "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/connecting/",
	"title": "Setup Connection",
	"tags": [],
	"description": "",
	"content": "Once you have your Cubic server up and running, let\u0026rsquo;s see how we can use the SDK to connect to it.\nFirst, you need to know the address (host:port) where the server is running. This document will assume the values 127.0.0.1:2727, but be sure to change those to point to your server instance. For example, to connect to Cobalt\u0026rsquo;s demo server, use demo-cubic.cobaltspeech.com:27271. Port 2727 is the default GRPC port that Cubic server binds to.\n 1Commercial use of the demo service is not permitted. This server is for testing and demonstration purposes only and is not guaranteed to support high availability or high volume. Data uploaded to the server may be stored for internal purposes.\n Default Connection The following code snippet connects to the server and queries its version. It uses our recommended default setup, expecting the server to be listening on a TLS encrypted connection, as the demo server does.\nimport( \u0026quot;context\u0026quot;\u0026quot;fmt\u0026quot;\u0026quot;log\u0026quot;\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026amp;#34;\u0026lt;/span\u0026gt;  )\nconstserverAddr = \u0026quot;127.0.0.1:2727\u0026quot;funcmain() { client, err := cubic.NewClient(serverAddr) iferr != nil{ log.Fatal(err) }\n\u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;defer\u0026lt;/span\u0026gt; client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Close\u0026lt;/span\u0026gt;() version, err := client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Version\u0026lt;/span\u0026gt;(context.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Background\u0026lt;/span\u0026gt;()) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) } fmt.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Println\u0026lt;/span\u0026gt;(version)  } serverAddress = \u0026quot;127.0.0.1:2727\u0026quot;client = cubic.Client(serverAddress)\nresp = client.Version() print(resp) varresp = client.Version(newGoogle.Protobuf.WellKnownTypes.Empty()); Console.WriteLine(String.Format(\u0026quot;CubicServer: {0}, Cubic: {1}\u0026quot;, resp.Server, resp.Cubic)); importcom.cobaltspeech.cubic.CubicGrpc;\nString url = \u0026quot;127.0.0.1:2727\u0026quot;ManagedChannel mCubicChannel = ManagedChannelBuilder .forTarget(url) .build(); CubicGrpc.CubicStubmCubicService = CubicGrpc.newStub(mCubicChannel); Insecure Connection It is sometimes required to connect to Cubic server without TLS enabled, such as during debugging.\nPlease note that if the server has TLS enabled, attempting to connect with an insecure client will fail. To connect to such an instance of cubic server, you can use:\nimportcom.cobaltspeech.cubic.CubicGrpc;\nString url = \u0026quot;127.0.0.1:2727\u0026quot;ManagedChannel mCubicChannel = ManagedChannelBuilder .forTarget(url) .usePlainText() .build(); CubicGrpc.CubicStubmCubicService = CubicGrpc.newStub(mCubicChannel); Client Authentication In our recommended default setup, TLS is enabled in the gRPC setup, and when connecting to the server, clients validate the server\u0026rsquo;s SSL certificate to make sure they are talking to the right party. This is similar to how \u0026ldquo;https\u0026rdquo; connections work in web browsers.\nIn some setups, it may be desired that the server should also validate clients connecting to it and only respond to the ones it can verify. If your Cubic server is configured to do client authentication, you will need to present the appropriate certificate and key when connecting to it.\nPlease note that in the client-authentication mode, the client will still also verify the server\u0026rsquo;s certificate, and therefore this setup uses mutually authenticated TLS. This can be done with:\nwhere certPem and keyPem are the bytes of the client certificate and key provided to you.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/recognize/",
	"title": "Synchronous Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe a short audio clip using Cubic\u0026rsquo;s Synchoronous Recognize Request. It is assumed that the audio file contains raw samples, PCM16SLE like Cubic expects. We will query the server for available models and use the first model to transcribe this speech.\nimport( \u0026quot;context\u0026quot;\u0026quot;fmt\u0026quot;\u0026quot;log\u0026quot;\u0026quot;os\u0026quot;\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026amp;#34;\u0026lt;/span\u0026gt;  )\nconstserverAddr = \u0026quot;127.0.0.1:2727\u0026quot;funcmain() { client, err := cubic.NewClient(serverAddr) iferr != nil{ log.Fatal(err) } deferclient.Close()\nmodelResp, err := client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;ListModels\u0026lt;/span\u0026gt;(context.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Background\u0026lt;/span\u0026gt;()) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) } \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Use the first available model  model := modelResp.Models[0]\nf, err := os.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Open\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;test.raw\u0026amp;#34;\u0026lt;/span\u0026gt;) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) } \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;defer\u0026lt;/span\u0026gt; f.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Close\u0026lt;/span\u0026gt;() cfg := \u0026amp;amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } recResp, err := client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Recognize\u0026lt;/span\u0026gt;(context.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Background\u0026lt;/span\u0026gt;(), cfg, f) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; _, r := \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;range\u0026lt;/span\u0026gt; recResp.Results { \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; !r.IsPartial { fmt.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Println\u0026lt;/span\u0026gt;(r.Alternatives[\u0026lt;span style=\u0026quot;color:#3677a9\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;].Transcript) } }  } serverAddress = '127.0.0.1:2727'client = cubic.Client(serverAddress)\n# get list of available modelsmodelResp = client.ListModels() formodel inmodelResp.models: print(\u0026quot;ID = {}, Name = {}\u0026quot;.format(model.id, model.name))\n# use the first available modelmodel = modelResp.models[0]\ncfg = cubic.RecognitionConfig( model_id = model.id )\n# open audio file audio = open('test.raw', 'rb')\nresp = client.Recognize(cfg, audio)\nforresult inresp.results: ifnotresult.is_partial: print(result.alternatives[0].transcript)\n// List the available models varlistModelsRequest = newCobaltSpeech.Cubic.ListModelsRequest(); varmodels = client.ListModels(listModelsRequest);\nvarcfg = newCobaltSpeech.Cubic.RecognitionConfig { // Use the first available model ModelId = models.Models[0].Id, };\n// Open the audio file. FileStream file = File.OpenRead(\u0026quot;test.raw\u0026quot;) varaudio = newCobaltSpeech.Cubic.RecognitionAudio{ Data = Google.Protobuf.ByteString.FromStream(file) };\n// Create the request varrequest = newCobaltSpeech.Cubic.RecognizeRequest { Config = cfg, Audio = audio, };\n// Send the request varresp = client.Recognize(request); foreach(varresult inresp.Results) { if(!result.IsPartial) { Console.WriteLine(result.Alternatives[0].Transcript) } } importcom.google.protobuf.ByteString;\nimportcom.cobaltspeech.cubic.CubicGrpc; importcom.cobaltspeech.cubic.CubicOuterClass.*;\npublicstaticintmain() { // Setup connection CubicGrpc.CubicStubmCubicService = CubicGrpc.newStub( ManagedChannelBuilder.forTarget(url).build());\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Load the file into a ByteString for gRPC.  byte[] fileContent = Files.readAllBytes(\u0026quot;/path/to/file\u0026quot;); ByteString fileContentBS = ByteString.copyFrom(fileContents)\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup config message (Using first model available)  RecognitionConfig cfg = RecognitionConfig.newBuilder() .setModelId(\u0026quot;ModelID\u0026quot;) .build()\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup audio message  RecognitionAudio audio = RecognitionAudio.newBuilder().setData(fileContentBS).build()\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup callback to handle results  StreamObserver\u0026lt;\u0026gt; responseObserver = newStreamObserver\u0026lt;RecognitionResponse\u0026gt;() { @OverridepublicvoidonNext(RecognitionResponse value) { System.out.println(\u0026quot;Result: \u0026quot;+ value.toString()); }\n \u0026lt;span style=\u0026quot;color:#ffa500\u0026quot;\u0026gt;@Override\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;onError\u0026lt;/span\u0026gt;(Throwable t) { System.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;err\u0026lt;/span\u0026gt;.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;Error with recognition:\u0026amp;#34;\u0026lt;/span\u0026gt; + t.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;()); } \u0026lt;span style=\u0026quot;color:#ffa500\u0026quot;\u0026gt;@Override\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;onCompleted\u0026lt;/span\u0026gt;() { System.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;Server is done sending responses back\u0026amp;#34;\u0026lt;/span\u0026gt;); } }; \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Send it over to the server  mCubicService.recognize( RecognizeRequest.newBuilder() .setConfig(cfg) .setAudio(audio) .build(), responseObserver); } "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/streaming/",
	"title": "Streaming Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe an audio file using Cubic’s Streaming Recognize Request. The stream can come from a file on disk or be directly from a microphone in real time.\nStreaming from an audio file We support several file formats including WAV, MP3, FLAC etc. For more details, please see the protocol buffer specification file in the SDK repository (grpc/cubic.proto). The examples below use a WAV file as input to the streaming recognition. We will query the server for available models and use the first model to transcribe the speech.\nimport( \u0026quot;context\u0026quot;\u0026quot;fmt\u0026quot;\u0026quot;log\u0026quot;\u0026quot;os\u0026quot;\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026amp;#34;\u0026lt;/span\u0026gt;  )\nconstserverAddr = \u0026quot;127.0.0.1:2727\u0026quot;funcmain() { client, err := cubic.NewClient(serverAddr) iferr != nil{ log.Fatal(err) } deferclient.Close()\nmodelResp, err := client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;ListModels\u0026lt;/span\u0026gt;(context.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Background\u0026lt;/span\u0026gt;()) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) } \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Use the first available model  model := modelResp.Models[0]\nf, err := os.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Open\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;test.wav\u0026amp;#34;\u0026lt;/span\u0026gt;) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) } \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;defer\u0026lt;/span\u0026gt; f.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Close\u0026lt;/span\u0026gt;() cfg := \u0026amp;amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Define a callback function to handle results  resultHandler := func(resp *cubicpb.RecognitionResponse) { for_, r := rangeresp.Results { if!r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } }\nerr = client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;StreamingRecognize\u0026lt;/span\u0026gt;(context.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Background\u0026lt;/span\u0026gt;(), cfg, f, resultHandler) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) }  } serverAddress = '127.0.0.1:2727'client = cubic.Client(serverAddress)\n# get list of available modelsmodelResp = client.ListModels() formodel inmodelResp.models: print(\u0026quot;ID = {}, Name = {}\u0026quot;.format(model.id, model.name))\n# use the first available modelmodel = modelResp.models[0]\ncfg = cubic.RecognitionConfig( model_id = model.id )\n# client.StreamingRecognize takes any binary# stream object that has a read(nBytes) method.# The method should return nBytes from the stream.# open audio file streamaudio = open('test.wav', 'rb')\n# send streaming request to cubic and # print out results as they come inforresp inclient.StreamingRecognize(cfg, audio): forresult inresp.results: ifresult.is_partial: print(\u0026quot;\\r{0}\u0026quot;.format(result.alternatives[0].transcript), end=\u0026quot;\u0026quot;) else: print(\u0026quot;\\r{0}\u0026quot;.format(result.alternatives[0].transcript), end=\u0026quot;\\n\u0026quot;)\n// List the available models varlistModelsRequest = newCobaltSpeech.Cubic.ListModelsRequest(); varmodels = client.ListModels(listModelsRequest);\n// Setup the bi-directional gRPC stream. varcall = client.StreamingRecognize(); using(call) { // Setup recieve task varresponseReaderTask = Task.Run(async() =\u0026gt; { // Wait for the next response while(awaitcall.ResponseStream.MoveNext()) { varresponse = call.ResponseStream.Current; foreach(varresult inresponse.Results) { Console.WriteLine(result.Alternatives[0].Transcript); } } });\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Send config first, followed by the audio  { // Send the configs varrequest = newCobaltSpeech.Cubic.StreamingRecognizeRequest(); request.Config = cfg; awaitcall.RequestStream.WriteAsync(request);\n \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup object for streaming audio  request.Config = null; request.Audio = newCobaltSpeech.Cubic.RecognitionAudio { };\n \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Send the audio, in 8kb chunks  constintchunkSize = 8192; using(FileStream file = File.OpenRead(\u0026quot;test.raw\u0026quot;)) { intbytesRead; varbuffer = newbyte[chunkSize]; while((bytesRead = file.Read(buffer, 0, buffer.Length)) \u0026gt; 0) { varbytes = Google.Protobuf.ByteString.CopyFrom(buffer); request.Audio.Data = bytes; awaitcall.RequestStream.WriteAsync(request); }\n \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Close the sending stream  awaitcall.RequestStream.CompleteAsync(); } }\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Wait for all of the responses to come back through the receiving stream  awaitresponseReaderTask; } importcom.google.protobuf.ByteString;\nimportcom.cobaltspeech.cubic.CubicGrpc; importcom.cobaltspeech.cubic.CubicOuterClass.*;\npublicstaticvoidtranscribeFile() { // Setup connection CubicGrpc.CubicStubmCubicService = CubicGrpc.newStub( ManagedChannelBuilder.forTarget(url).build());\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup callback to handle results  StreamObserver\u0026lt;\u0026gt; responseObserver = newStreamObserver\u0026lt;RecognitionResponse\u0026gt;() { @OverridepublicvoidonNext(RecognitionResponse value) { System.out.println(\u0026quot;Result: \u0026quot;+ value.toString()); }\n \u0026lt;span style=\u0026quot;color:#ffa500\u0026quot;\u0026gt;@Override\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;onError\u0026lt;/span\u0026gt;(Throwable t) { System.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;err\u0026lt;/span\u0026gt;.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;Error with recognition:\u0026amp;#34;\u0026lt;/span\u0026gt; + t.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;()); } \u0026lt;span style=\u0026quot;color:#ffa500\u0026quot;\u0026gt;@Override\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;onCompleted\u0026lt;/span\u0026gt;() { System.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;Server is done sending responses back\u0026amp;#34;\u0026lt;/span\u0026gt;); } }; \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup bidirectional stream  StreamObserver\u0026lt;StreamingRecognizeRequest\u0026gt; requestObserver; // Outgoing messages are sent on this request object requestObserver = mCubicService.streamingRecognize(mRecognitionResponseObserver);\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Send config message  StreamingRecognizeRequest configs = StreamingRecognizeRequest.newBuilder() // Note, we do not call setAudio here. .setConfig(RecognitionConfig.newBuilder() .setModelId(\u0026quot;ModelID\u0026quot;) .build()) .build(); requestObserver.onNext(configs);\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Read the file in chunks and stream to server.  try{ FileInputStream is = newFileInputStream(newFile(\u0026quot;/path/to/file\u0026quot;)); byte[] bytes = newbyte[1024]; intlen = 0;\n \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Read the file  while((len = is.read(chunk)) != -1) { // Convert byte[] to ByteString for gRPC ByteString audioBS = ByteString.copyFrom(chunk);\n \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Send audio to server  RecognitionAudio audioMsg = RecognitionAudio.newBuilder() .setData(audioBS) .build() requestObserver.onNext(StreamingRecognizeRequest.newBuilder() .setAudio(audioMsg) .build()); } } catch(Exception e) { } // Handle exception // Close the client side stream requestObserver.onCompleted();\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Note: Once the server is done transcribing everything, responseObserver.onCompleted() will be called.  } Streaming from microphone Streaming audio from microphone input typically needs us to interact with system libraries. There are several options available, and although the examples here use one, you may choose to use an alternative as long as the recording audio format is chosen correctly.\nimport( \u0026quot;bytes\u0026quot;\u0026quot;context\u0026quot;\u0026quot;encoding/binary\u0026quot;\u0026quot;fmt\u0026quot;\u0026quot;log\u0026quot;\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;github.com/gordonklaus/portaudio\u0026amp;#34;\u0026lt;/span\u0026gt;  )\nconstserverAddr = \u0026quot;127.0.0.1:2727\u0026quot;// Microphone implements the io.ReadCloser interface and provides // a data stream for microphone input. typeMicrophone struct{ buffer []int16stream *portaudio.Stream }\n// NewMicrophone instantiates a Microphone object with the desired // sampling rate and buffer size. When streaming to cubic, the sample // rate should be set to the sample rate of the model used. funcNewMicrophone(sampleRate, bufferSize uint32) (*Microphone, error) {\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// bufferSize is measured in number of bytes. Since we are capturing  // 16 bit audio, each sample is 2 bytes. The microphone has a int16 // buffer, so we use the number of samples as its size. numSamples := bufferSize/2mic := Microphone{buffer: make([]int16, numSamples)}\nportaudio.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Initialize\u0026lt;/span\u0026gt;() stream, err := portaudio.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;OpenDefaultStream\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#3677a9\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026quot;color:#3677a9\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026quot;color:#24909d\u0026quot;\u0026gt;float64\u0026lt;/span\u0026gt;(sampleRate), \u0026lt;span style=\u0026quot;color:#24909d\u0026quot;\u0026gt;int\u0026lt;/span\u0026gt;(numSamples), mic.buffer) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt;, err } mic.stream = stream err = mic.stream.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Start\u0026lt;/span\u0026gt;() \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt;, err } \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; \u0026amp;amp;mic, \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt;  }\n// Read copies N bytes into the passed buffer from the microphone audio buffer // where N is the buffer size passed to cubic.NewClient. Also, to be compatible // with the cubic client, it returns two things : an int representing the number of // bytes copied and an error. func(mic *Microphone) Read(buffer []byte) (int, error) { err := mic.stream.Read() iferr != nil{ return0, err } byteBuffer := new(bytes.Buffer) err = binary.Write(byteBuffer, binary.LittleEndian, mic.buffer) iferr != nil{ return0, err } copy(buffer, byteBuffer.Bytes()) returnlen(buffer), nil}\n// Close shuts the microphone stream down and cleans up func(mic *Microphone) Close() { mic.stream.Stop() mic.stream.Close() portaudio.Terminate() }\nfuncmain() {\nbufferSize := \u0026lt;span style=\u0026quot;color:#24909d\u0026quot;\u0026gt;uint32\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#3677a9\u0026quot;\u0026gt;8192\u0026lt;/span\u0026gt;) client, err := cubic.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;NewClient\u0026lt;/span\u0026gt;(serverAddr, cubic.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;WithInsecure\u0026lt;/span\u0026gt;(), cubic.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;WithStreamingBufferSize\u0026lt;/span\u0026gt;(bufferSize)) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) } \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;defer\u0026lt;/span\u0026gt; client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Close\u0026lt;/span\u0026gt;() modelResp, err := client.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;ListModels\u0026lt;/span\u0026gt;(context.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Background\u0026lt;/span\u0026gt;()) \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; err != \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;nil\u0026lt;/span\u0026gt; { log.\u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;Fatal\u0026lt;/span\u0026gt;(err) } \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Use the first available model  model := modelResp.Models[0]\ncfg := \u0026amp;amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Define a callback function to handle results  resultHandler := func(resp *cubicpb.RecognitionResponse) { for_, r := rangeresp.Results { ifr.IsPartial { fmt.Print(\u0026quot;\\r\u0026quot;, r.Alternatives[0].Transcript) // print on same line } else{ fmt.Println(\u0026quot;\\r\u0026quot;, r.Alternatives[0].Transcript) // print and move to new line } } }\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Create microphone stream  mic, err := NewMicrophone(model.Attributes.SampleRate, bufferSize) iferr != nil{ log.Fatal(err) } defermic.Close()\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Since Microphone implements the io.ReadCloser interface, we can  // pass it directly to the StreamingRecognize function. err = client.StreamingRecognize(context.Background(), cfg, mic, resultHandler) iferr != nil{ log.Fatal(err) } } serverAddress = '127.0.0.1:2727'client = cubic.Client(serverAddress)\n# get list of available modelsmodelResp = client.ListModels()\n# use the first available modelmodel = modelResp.models[0]\ncfg = cubic.RecognitionConfig( model_id = model.id )\n# client.StreamingRecognize takes any binary stream object that has a read(nBytes)# method. The method should return nBytes from the stream. So pyaudio is a suitable# library to use here for streaming audio from the microphone. Other libraries or# modules may also be used as long as they have the read method or have been wrapped# to do so.# open microphone streamp = pyaudio.PyAudio() audio = p.open(format=pyaudio.paInt16, # 16 bit sampleschannels=1, # mono audiorate=model.attributes.sample_rate, # sample rate in hertzinput=True) # audio input stream# send streaming request to cubic and # print out results as they come intry: forresp inclient.StreamingRecognize(cfg, audio): forresult inresp.results: ifresult.is_partial: print(\u0026quot;\\r{0}\u0026quot;.format(result.alternatives[0].transcript), end=\u0026quot;\u0026quot;) else: print(\u0026quot;\\r{0}\u0026quot;.format(result.alternatives[0].transcript), end=\u0026quot;\\n\u0026quot;) exceptKeyboardInterrupt: # stop streaming when ctrl+C pressedpassexceptExceptionaserr: print(\u0026quot;Error while trying to stream audio : {}\u0026quot;.format(err))\naudio.stop_stream() audio.close() importcom.google.protobuf.ByteString;\nimportcom.cobaltspeech.cubic.CubicGrpc; importcom.cobaltspeech.cubic.CubicOuterClass.*;\npublicstaticvoidstreamMicrophoneAudio() { // Setup connection CubicGrpc.CubicStubmCubicService = CubicGrpc.newStub( ManagedChannelBuilder.forTarget(url).build());\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup callback to handle results  StreamObserver\u0026lt;\u0026gt; responseObserver = newStreamObserver\u0026lt;RecognitionResponse\u0026gt;() { @OverridepublicvoidonNext(RecognitionResponse value) { System.out.println(\u0026quot;Result: \u0026quot;+ value.toString()); }\n \u0026lt;span style=\u0026quot;color:#ffa500\u0026quot;\u0026gt;@Override\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;onError\u0026lt;/span\u0026gt;(Throwable t) { System.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;err\u0026lt;/span\u0026gt;.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;Error with recognition:\u0026amp;#34;\u0026lt;/span\u0026gt; + t.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;()); } \u0026lt;span style=\u0026quot;color:#ffa500\u0026quot;\u0026gt;@Override\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6ab825;font-weight:bold\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#447fcf\u0026quot;\u0026gt;onCompleted\u0026lt;/span\u0026gt;() { System.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;.\u0026lt;span style=\u0026quot;color:#bbb\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;(\u0026lt;span style=\u0026quot;color:#ed9d13\u0026quot;\u0026gt;\u0026amp;#34;Server is done sending responses back\u0026amp;#34;\u0026lt;/span\u0026gt;); } }; \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup bidirectional stream  StreamObserver\u0026lt;StreamingRecognizeRequest\u0026gt; requestObserver; // Outgoing messages are sent on this request object requestObserver = mCubicService.streamingRecognize(mRecognitionResponseObserver);\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Send config message  RecognitionConfig cfg = RecognitionConfig.newBuilder() .setModelId(\u0026quot;ModelID\u0026quot;) .build(); StreamingRecognizeRequest configs = StreamingRecognizeRequest.newBuilder() // Note, we do not call setAudio here. .setConfig(cfg) .build(); requestObserver.onNext(configs);\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Setup the Android Micorphone Recorder  intSAMPLE_RATE = 8000; // Same as the model is expecting intBUFFER_SIZE = 1024; AudioRecord recorder = newAudioRecord( MediaRecorder.AudioSource.MIC, SAMPLE_RATE, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT, BUFFER_SIZE); byte[] audioBuffer = newbyte[BUFFER_SIZE]; recorder.startRecording();\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Read the file in chunks and stream to server.  while(running) { recorder.read(audioBuffer, 0, BUFFER_SIZE, AudioRecord.READ_BLOCKING);\n \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Convert byte[] to ByteString for gRPC  ByteString audioBS = ByteString.copyFrom(audioBuffer);\n \u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Send audio to server  RecognitionAudio audioMsg = RecognitionAudio.newBuilder() .setData(audioBS) .build(); requestObserver.onNext(StreamingRecognizeRequest.newBuilder() .setAudio(audioMsg) .build()); }\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Stop the microphone recoding.  recorder.stop();\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Close the client side stream  requestObserver.onCompleted();\n\u0026lt;span style=\u0026quot;color:#999;font-style:italic\u0026quot;\u0026gt;// Note: Once the server is done transcribing everything, it will call responseObserver.onCompleted().  } "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/client-configs/",
	"title": "Recognition Configurations",
	"tags": [],
	"description": "",
	"content": "An in-depth explanation of the various fields of the complete SDK can be found here. The sub-section RecognitionConfig is particularly important here.\nThis page here discusses the more common combinations sent to the server.\nFields Here is a quick overview of the fields.\n   Field Required Default Description     model_id Yes  Unique ID of the model to use.   audio_encoding Yes  Encoding format of the audio, such as RAW_LINEAR_16, WAV, MP3, etc.   idle_timeout No 0s (Unlimited) Maximum time allowed between each gRPC message. The server may place further restrictions depending on its configuration.   enable_word_time_offsets No false Toggles the calculation of word-level timestamps. The specified model must also support word-level timestamps for this field to be populated.   enable_word_confidence No false Toggles the calculation of word-level confidence scores. The specified model must also support word-level confidence for this field to be populated.   enable_raw_transcript No false If true, the raw transcript will be included in the results.   enable_confusion_network No false Toggles the inclusion of a confusion network, consisting of multiple alternative transcriptions. The specified model must also support confusion networks for this field to be populated.   audio_channels No [0] (mono) Specifies which channels of a multi-channel audio file to be transcribed, each as their own individual audio stream.    Use cases The most basic use case is getting a formatted transcript. This would simply need a config such as:\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34; } and your resulting transcript would be found at results.alternatives[*].transcript.\n Sometimes, only the raw (unformatted) transcript is desired. In this case, there are two options.\n Disable server-side formatting and use the above config. Retrieve the raw transcript from the results.alternatives[*].transcript field. Specify the enable_raw_transcript = true flag, and access the field results.alternatives[*].raw_transcript.   Note: Prior to cubicsvr v2.9.0 and SDK-Cubic v1.3.0, the field results.alternatives[*].transcript was populated with either the raw or the formatted transcription depending on the enable_raw_transcript config. After these changes, raw transcripts have been pushed to a new field results.alternatives[*].raw_transcript, and only populated when enable_raw_transcript is set to true.\n  Another use case would be getting both the formatted and raw transcript. This can be done using this config.\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: \u0026#34;true\u0026#34; } The formatted transcript would be found at results.alternatives[*].transcript, and the raw transcript would be found at results.alternatives[*].raw_transcript.\n If you need to know the timestamp for each word, to align subtitles with a video, for example, then you can use this config to enable those word-level timestamps. Please note the inclusion of enable_raw_transcript; the word-level information corresponds to the raw transcript, since the formatter may combine multiple words into one symbol in the formatted transcript (e.g. \u0026ldquo;twenty one\u0026rdquo; to \u0026ldquo;21\u0026rdquo;)\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_word_time_offsets\u0026#34;: true } Timestamps will be found at results.alternatives[*].words[*].start_time and results.alternatives[*].words[*].duration.\n Word-level confidences, i.e. for displaying a lighter color for less confident words, can be included much like the word-level timestamps. Please note the inclusion of enable_raw_transcript.\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_word_time_offsets\u0026#34;: true } Word-level confidence scores will be found at results.alternatives[*].words[*].confidence.\n For applications that need more than the one-best transcription, the most comprehensive and detailed Cubic results are found in the confusion network. Please refer to the in depth confusion network documentation to see what is included.\nTo enable the confusion network, the config will look similar to this:\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_confusion_network\u0026#34;: true } The confusion network can be accessed at result.cnet.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/android/",
	"title": "Android Integrations",
	"tags": [],
	"description": "",
	"content": "Adding the protobuf-gradle-plugin to your Android project In your root build.gradle file, add a new protobuf-gradle-plugin dependency\nbuildscript { // ...  dependencies { // ...  classpath \u0026#34;com.google.protobuf:protobuf-gradle-plugin:0.8.10\u0026#34; // ...  } } This will allow the app\u0026rsquo;s gradle build script to generate the protobuf code.\nGenerating code from protobuf files Next, you will have to add the code to actually generate the files. To generate the gRPC code, modify your app/build.gradle file with the following:\napply plugin: \u0026#39;com.android.application\u0026#39; // Should already exist apply plugin: \u0026#39;com.google.protobuf\u0026#39; // Add this line  android { /*...*/} // This section adds a step to generate the gRPC code from `app/src/main/proto` proto files. // This section adds a step to generate the gRPC code from `app/src/main/proto` proto files // and make it available to your java/kotlin code. protobuf { protoc { artifact = \u0026#39;com.google.protobuf:protoc:3.10.0\u0026#39; } plugins { javalite { artifact = \u0026#34;com.google.protobuf:protoc-gen-javalite:3.0.0\u0026#34; } grpc { artifact = \u0026#39;io.grpc:protoc-gen-grpc-java:1.24.0\u0026#39; } } generateProtoTasks { all().each { task -\u0026gt; task.plugins { javalite {} grpc { // Options added to --grpc_out  option \u0026#39;lite\u0026#39; // the gRPC documentation suggests using lite in android applications  } } } } } // Runtime dependencies dependencies { // Existing dependencies ...  // gRPC Libraries  implementation \u0026#39;io.grpc:grpc-okhttp:1.24.0\u0026#39; implementation \u0026#39;io.grpc:grpc-protobuf-lite:1.24.0\u0026#39; // the gRPC documentation suggests using lite in android applications  implementation \u0026#39;io.grpc:grpc-stub:1.24.0\u0026#39; implementation \u0026#39;io.grpc:grpc-auth:1.24.0\u0026#39; implementation \u0026#39;javax.annotation:javax.annotation-api:1.2\u0026#39; } By default, generateProtoTasks assumes all protofiles are availabe at app/src/main/proto. To include proto files somewhere else, add lines such as this:\ndependencies { // ...  protobuf files(\u0026#34;lib/protos.tar.gz\u0026#34;) protobuf files(\u0026#34;/path/to/other/folder/to/include/\u0026#34;) } Downloading the latest cubic.proto file You can find our proto file at https://github.com/cobaltspeech/sdk-cubic/blob/master/grpc/cubic.proto. Copy this file to app/src/main/proto/cubic.proto.\ncubic.proto also relies on a few other proto files:\n   Name URL     cubic.proto https://github.com/cobaltspeech/sdk-cubic/blob/master/grpc/cubic.proto   google/api/annotations.proto https://github.com/googleapis/googleapis/blob/6ae2d42/google/api/annotations.proto   google/api/http.proto https://github.com/googleapis/googleapis/blob/6ae2d42/google/api/http.proto   google/protobuf/descriptor.proto https://github.com/protocolbuffers/protobuf/blob/044c766/src/google/protobuf/descriptor.proto   google/protobuf/duration.proto https://github.com/protocolbuffers/protobuf/blob/044c766/src/google/protobuf/duration.proto    Once you have all of these files downloaded, your file structure would look like this:\napp/src/main/proto/ ├── cubic.proto └── google ├── api │ ├── annotations.proto │ └── http.proto └── protobuf ├── descriptor.proto └── duration.proto At this point, you should be able to do a Build\u0026gt;Clean Build and Build\u0026gt;Rebuild Project.\nContact us We know it takes work to get environments set up. If you have any problems, don\u0026rsquo;t hesitate to contact us.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/autogen-doc-cubic-proto/",
	"title": "Cubic Protobuf API Docs",
	"tags": [],
	"description": "",
	"content": "cubic.proto Service: Cubic Service that implements the Cobalt Cubic Speech Recognition API\n   Method Name Request Type Response Type Description     Version .google.protobuf.Empty VersionResponse Queries the Version of the Server   ListModels ListModelsRequest ListModelsResponse Retrieves a list of available speech recognition models   Recognize RecognizeRequest RecognitionResponse Performs synchronous speech recognition: receive results after all audio has been sent and processed. It is expected that this request be typically used for short audio content: less than a minute long. For longer content, the StreamingRecognize method should be preferred.   StreamingRecognize StreamingRecognizeRequest RecognitionResponse Performs bidirectional streaming speech recognition. Receive results while sending audio. This method is only available via GRPC and not via HTTP+JSON. However, a web browser may use websockets to use this service.    Message: ConfusionNetworkArc An Arc inside a Confusion Network Link\n   Field Type Label Description     word string  Word in the recognized transcript   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.    Message: ConfusionNetworkLink A Link inside a confusion network\n   Field Type Label Description     start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this link   duration google.protobuf.Duration  Duration of the current link in the confusion network   arcs ConfusionNetworkArc repeated Arcs between this link    Message: ListModelsRequest The top-level message sent by the client for the ListModels method.\nThis message is empty and has no fields.\nMessage: ListModelsResponse The message returned to the client by the ListModels method.\n   Field Type Label Description     models Model repeated List of models available for use that match the request.    Message: Model Description of a Cubic Model\n   Field Type Label Description     id string  Unique identifier of the model. This identifier is used to choose the model that should be used for recognition, and is specified in the RecognitionConfig message.   name string  Model name. This is a concise name describing the model, and maybe presented to the end-user, for example, to help choose which model to use for their recognition task.   attributes ModelAttributes  Model attributes    Message: ModelAttributes Attributes of a Cubic Model\n   Field Type Label Description     sample_rate uint32  Audio sample rate supported by the model    Message: RecognitionAlternative A recognition hypothesis\n   Field Type Label Description     transcript string  Text representing the transcription of the words that the user spoke.The transcript will be formatted according to the servers formatting configuration. If you want the raw transcript, please see the field raw_transcript. If the server is configured to not use any formatting, then this field will contain the raw transcript.As an example, if the spoken utterance was \u0026ldquo;four people\u0026rdquo;, and the server was configured to format numbers, this field would be set to \u0026ldquo;4 people\u0026rdquo;.   raw_transcript string  Text representing the transcription of the words that the user spoke, without any formatting. This field will be populated only the config RecognitionConfig.enable_raw_transcript is set to true. Otherwise this field will be an empty string. If you want the formatted transcript, please see the field transcript.As an example, if the spoken utterance was here are four words, this field would be set to \u0026ldquo;HERE ARE FOUR WORDS\u0026rdquo;.   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood of the output being correct.   words WordInfo repeated A list of word-specific information for each recognized word. This is available only if enable_word_confidence or enable_word_time_offsets was set to true in the RecognitionConfig.   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this utterance.   duration google.protobuf.Duration  Duration of the current utterance in the spoken audio.    Message: RecognitionAudio Audio to be sent to the recognizer\n   Field Type Label Description     data bytes      Message: RecognitionConfig Configuration for setting up a Recognizer\n   Field Type Label Description     model_id string  Unique identifier of the model to use, as obtained from a Model message.   audio_encoding RecognitionConfig.Encoding  Encoding of audio data sent/streamed through the RecognitionAudio messages. For encodings like WAV/MP3 that have headers, the headers are expected to be sent at the beginning of the stream, not in every RecognitionAudio message.If not specified, the default encoding is RAW_LINEAR16.Depending on how they are configured, server instances of this service may not support all the encodings enumerated above. They are always required to accept RAW_LINEAR16. If any other Encoding is specified, and it is not available on the server being used, the recognition request will result in an appropriate error message.   idle_timeout google.protobuf.Duration  Idle Timeout of the created Recognizer. If no audio data is received by the recognizer for this duration, ongoing rpc calls will result in an error, the recognizer will be destroyed and thus more audio may not be sent to the same recognizer. The server may impose a limit on the maximum idle timeout that can be specified, and if the value in this message exceeds that serverside value, creating of the recognizer will fail with an error.   enable_word_time_offsets bool  This is an optional field. If this is set to true, each result will include a list of words and the start time offset (timestamp) and the duration for each of those words. If set to false, no word-level timestamps will be returned. The default is false.   enable_word_confidence bool  This is an optional field. If this is set to true, each result will include a list of words and the confidence for those words. If false, no word-level confidence information is returned. The default is false.   enable_raw_transcript bool  This is an optional field. If this is set to true, the field RecognitionAlternative.raw_transcript will be populated with the raw transcripts output from the recognizer will be exposed without any formatting rules applied. If this is set to false, that field will not be set in the results. The RecognitionAlternative.transcript will always be populated with text formatted according to the server\u0026rsquo;s settings.   enable_confusion_network bool  This is an optional field. If this is set to true, the results will include a confusion network. If set to false, no confusion network will be returned. The default is false. If the model being used does not support a confusion network, results may be returned without a confusion network available. If this field is set to true, then enable_raw_transcript is also forced to be true.   audio_channels uint32 repeated This is an optional field. If the audio has multiple channels, this field should be configured with the list of channel indices that should be transcribed. Channels are 0-indexed.Example: [0] for a mono file, [0, 1] for a stereo file.If this field is not set, a mono file will be assumed by default and only channel-0 will be transcribed even if the file actually has additional channels.Channels that are present in the audio may be omitted, but it is an error to include a channel index in this field that is not present in the audio. Channels may be listed in any order but the same index may not be repeated in this list.BAD: [0, 2] for a stereo file; BAD: [0, 0] for a mono file.    Message: RecognitionConfusionNetwork Confusion network in recognition output\n   Field Type Label Description     links ConfusionNetworkLink repeated     Message: RecognitionResponse Collection of sequence of recognition results in a portion of audio. When transcribing a single audio channel (e.g. RAW_LINEAR16 input, or a mono file), results will be ordered chronologically. When transcribing multiple channels, the results of all channels will be interleaved. Results of each individual channel will be chronological. No such promise is made for the ordering of results of different channels, as results are returned for each channel individually as soon as they are ready.\n   Field Type Label Description     results RecognitionResult repeated     Message: RecognitionResult A recognition result corresponding to a portion of audio.\n   Field Type Label Description     alternatives RecognitionAlternative repeated An n-best list of recognition hypotheses alternatives   is_partial bool  If this is set to true, it denotes that the result is an interim partial result, and could change after more audio is processed. If unset, or set to false, it denotes that this is a final result and will not change.Servers are not required to implement support for returning partial results, and clients should generally not depend on their availability.   cnet RecognitionConfusionNetwork  If enable_confusion_network was set to true in the RecognitionConfig, and if the model supports it, a confusion network will be available in the results.   audio_channel uint32  Channel of the audio file that this result was transcribed from. For a mono file, or RAW_LINEAR16 input, this will be set to 0.    Message: RecognizeRequest The top-level message sent by the client for the Recognize method. Both the RecognitionConfig and RecognitionAudio fields are required. The entire audio data must be sent in one request. If your audio data is larger, please use the StreamingRecognize call..\n   Field Type Label Description     config RecognitionConfig  Provides configuration to create the recognizer.   audio RecognitionAudio  The audio data to be recognized    Message: StreamingRecognizeRequest The top-level message sent by the client for the StreamingRecognize request. Multiple StreamingRecognizeRequest messages are sent. The first message must contain a RecognitionConfig message only, and all subsequent messages must contain RecognitionAudio only. All RecognitionAudio messages must contain non-empty audio. If audio content is empty, the server may interpret it as end of stream and stop accepting any further messages.\n   Field Type Label Description     config RecognitionConfig     audio RecognitionAudio      Message: VersionResponse The message sent by the server for the Version method.\n   Field Type Label Description     cubic string  version of the cubic library handling the recognition   server string  version of the server handling these requests    Message: WordInfo Word-specific information for recognized words\n   Field Type Label Description     word string  The actual word in the text   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this spoken word.   duration google.protobuf.Duration  Duration of the current word in the spoken audio.    Enum: RecognitionConfig.Encoding The encoding of the audio data to be sent for recognition.\nFor best results, the audio source should be captured and transmitted using the RAW_LINEAR16 encoding.\n   Name Number Description     RAW_LINEAR16 0 Raw (headerless) Uncompressed 16-bit signed little endian samples (linear PCM), single channel, sampled at the rate expected by the chosen Model.   WAV 1 WAV (data with RIFF headers), with data sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   MP3 2 MP3 data, sampled at a rate equal to or higher than the sampling rate expected by the chosen Model.   FLAC 3 FLAC data, sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   VOX8000 4 VOX data (Dialogic ADPCM), sampled at 8 KHz.   ULAW8000 5 μ-law (8-bit) encoded RAW data, single channel, sampled at 8 KHz.    Scalar Value Types    .proto Type Notes Go Type Python Type     double  float64 float   float  float32 float   int32 Uses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint32 instead. int32 int   int64 Uses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint64 instead. int64 int/long   uint32 Uses variable-length encoding. uint32 int/long   uint64 Uses variable-length encoding. uint64 int/long   sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int   sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 int/long   fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int   fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 int/long   sfixed32 Always four bytes. int32 int   sfixed64 Always eight bytes. int64 int/long   bool  bool boolean   string A string must always contain UTF-8 encoded or 7-bit ASCII text. string str/unicode   bytes May contain any arbitrary sequence of bytes. []byte str    "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Cubic SDK \u0026ndash; Cobalt\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/",
	"title": "Cubic SDK Documentation",
	"tags": [],
	"description": "",
	"content": "Cubic API Overview Cubic is Cobalt’s automatic speech recognition (ASR) engine. It can be deployed on-prem and accessed over the network or on your local machine via an API. We currently support Go and Python, and are adding support for more languages.\nOnce running, Cubic’s API provides a method to which you can stream audio. This audio can either be from a microphone or a file. We recommend uncompressed WAV as the encoding, but support other formats such as MP3.\nCubic’s API provides a number of options for returning the speech recognition results. The results are passed back using Google’s protobuf library, allowing them to be handled natively by your application. Cubic can estimate its confidence in the transcription result at the word or utterance level, along with timestamps of the words. Confidence scores are in the range 0-1. Cubic’s output options are described below.\nAutomatic Transcription Results The simplest result that Cubic returns is its best guess at the transcription of your audio. Cubic recognizes the audio you are streaming, listens for the end of each utterance, and returns the speech recognition result.\nCubic maintains its transcriptions in an N-best list, i.e. is the top N transcriptions from the recogniser. The best ASR result is the first entry in this list.\n{ \u0026#34;alternatives\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.514 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.201 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A \u0026lt;UNK\u0026gt; DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.105 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS ISN\u0026#39;T NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.093 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A YOUR DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.087 } ], } A single stream may consist of multiple utterances separated by silence. Cubic handles each utterance separately.\nFor longer utterances, it is often useful to see the partial speech recognition results while the audio is being streamed. For example, this allows you to see what the ASR system is predicting in real-time while someone is speaking. Cubic supports both partial and final ASR results.\nConfusion Network A Confusion Network is a form of speech recognition output that’s been turned into a compact graph representation of many possible transcriptions, as here:\nNote that \u0026lt;eps\u0026gt; in this representation is silence.\n{ \u0026#34;cnet\u0026#34;: { \u0026#34;links\u0026#34;: [ { \u0026#34;duration\u0026#34;: \u0026#34;1.350s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;0s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.690s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;TOMORROW\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;1.350s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.080s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.040s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.168s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;IS\u0026#34;, \u0026#34;confidence\u0026#34;: 0.892 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.120s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.010s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.288s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.093s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;confidence\u0026#34;: 0.620 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.233 }, { \u0026#34;word\u0026#34;: \u0026#34;ISN\u0026#39;T\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 }, { \u0026#34;word\u0026#34;: \u0026#34;THE\u0026#34;, \u0026#34;confidence\u0026#34;: 0.039 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.298s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.005s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.391s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.273s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;NEW\u0026#34;, \u0026#34;confidence\u0026#34;: 0.661 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.129 }, { \u0026#34;word\u0026#34;: \u0026#34;YOUR\u0026#34;, \u0026#34;confidence\u0026#34;: 0.107 }, { \u0026#34;word\u0026#34;: \u0026#34;YOU\u0026#34;, \u0026#34;confidence\u0026#34;: 0.102 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.396s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.420s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.954 }, { \u0026#34;word\u0026#34;: \u0026#34;TODAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.044 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.002 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.270s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;3.090s\u0026#34; } ] } } Formatted output Speech recognition systems typically output the words that were spoken, with no formatting. For example, utterances with numbers in might return “twenty seven bridges”, and “the year two thousand and three”. Cubic has the option to enable basic formatting of speech recognition results:\n Capitalising the first letter of the utterance Numbers: “cobalt’s atomic number is twenty seven” -\u0026gt; “Cobalt’s atomic number is 27” Truecasing: “the iphone was launched in two thousand and seven” -\u0026gt; “The iPhone was launched in 2007” Ordinals: “summer solstice is twenty first june” -\u0026gt; “Summer solstice is 21st June”  Note that word level timestamps and confidences aren’t supported when formatting is enabled.\nObtaining Cubic Cobalt will provide you with a package of Cubic that contains the engine, appropriate speech recognition models and a server application. This server exports Cubic\u0026rsquo;s functionality over the gRPC protocol. The https://github.com/cobaltspeech/sdk-cubic repository contains the SDK that you can use in your application to communicate with the Cubic server. This SDK is currently available for the Go and Python languages; and we would be happy to talk to you if you need support for other languages. Most of the core SDK is generated automatically using the gRPC tools, and Cobalt provides a top level package for more convenient API calls.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]