[
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/",
	"title": "Server Setup",
	"tags": [],
	"description": "",
	"content": "This section is meant to get you started using Cubic Server via a Docker image.\nIt is not necessary to go through through these steps to call the demo server for evaluation purposes, only to run Cubic Server on premises. The demo server can be found at https://demo-cubic.cobaltspeech.com.\nNote Commercial use of the demo service is not permitted. This server is for testing and demonstration purposes only and is not guaranteed to support high availability or high volume. Data uploaded to the server may be stored for internal purposes.  \nInstalling the Cubic Server Image The SDK communicates with a Cubic Server instance using gRPC. Cobalt distributes a docker image that contains the cubicsvr binary and model files.\n Contact Cobalt to get a link to the image file in AWS S3. This link will expire in two weeks, so be sure to download the file to your own server.\n Download with the AWS CLI if you have it, or with curl:\nURL=\u0026#34;the url sent by Cobalt\u0026#34; IMAGE_NAME=\u0026#34;name you want to give the file (should end with the same extension as the url, usually bz2)\u0026#34; curl $URL -L -o $IMAGE_NAME Load the docker image\ndocker load \u0026lt; $IMAGE_NAME This will output the name of the image (e.g. cubicsvr-demo-en_us-16).\n Start the cubic service\ndocker run -p 2727:2727 -p 8080:8080 --name cobalt cubicsvr-demo-en_us-16 That will start listening for grpc commands on port 2727 and http requests on 8080, and will stream the debug log to stdout. (You can replace --name cobalt with whatever name you want. That just provides a way to refer back to the currently running container.)\n Verify the service is running by calling\ncurl http://localhost:8080/api/version If you want to explore the package to see the model files etc, call the following to open the bash terminal on the previously run image. Model files are located in the /model directory.\ndocker exec -it cobalt bash  Contents of the docker image  Base docker image : debian-stretch-slim Additional dependencies  sox   Cobalt-specific files  cubicsvr - binary for performing Automatic Speech Recognition model.config - top-level config am/nnet3_online/final.mdl - this is the acoustic model am/nnet3_online/conf/online_cmvn.conf - feature extraction parameters for the features fed into the GMM model used for i-vector statistics accumulation. am/nnet3_online/conf/splice.conf - GMM feature context when accumulating statistics for i-vector accumulation. am/nnet3_online/ivector_extractor/* - Kaldi configuration files related to ivectors\n graph/HCLG.fst - the decoding graph: the combination of the AMs transition graph, the lexicon, and the language model graph/words.txt - an integer to word mapping, needed because the output of the HCLG graph contains only integer symbol IDs graph/phones/word_boundary.int - this is needed only when confusion network output is requested, it tells the decoder which phones are at word boundaries "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/",
	"title": "Using Cubic-SDK",
	"tags": [],
	"description": "",
	"content": "This section discusses the various aspects of using the SDK as a client.\n Installing the SDK Setting up a Connection Synchronous Recognition Streaming Recognition Recognition Configurations  "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/",
	"title": "Cubic API Reference",
	"tags": [],
	"description": "",
	"content": "The Cubic API is specified as a proto file. This section of the documentation is auto-generated from the spec. It describes the data types and functions defined in the spec. The \u0026ldquo;messages\u0026rdquo; below correspond to the data structures to be used, and the \u0026ldquo;service\u0026rdquo; contains the methods that can be called.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/installation/",
	"title": "Installing the SDK",
	"tags": [],
	"description": "",
	"content": "Instructions for installing the SDK are language specific.\nGo The Go SDK supports Go modules and requires Go 1.12 or later. To use the SDK, import this package into your application:\nimport \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; Python The Python SDK depends on Python \u0026gt;= 3.5. You may use pip to perform a system-wide install, or use virtualenv for a local install.\npip install --upgrade pip pip install \u0026#34;git+https://github.com/cobaltspeech/sdk-cubic#egg=cobalt-cubic\u0026amp;subdirectory=grpc/py-cubic\u0026#34; C The C# SDK utilizes the NuGet package manager. The package is called Cubic-SDK, under the owners name of CobaltSpeech.\nNuGet allows 4 different ways to install. Further instructions can be found on the nuget webpage. Installing via the dotnet cli through the command:\ndotnet add package Cubic-SDK"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/connecting/",
	"title": "Setup Connection",
	"tags": [],
	"description": "",
	"content": "Once you have your Cubic server up and running, let\u0026rsquo;s see how we can use the SDK to connect to it.\nFirst, you need to know the address (host:port) where the server is running. This document will assume the values 127.0.0.1:2727, but be sure to change those to point to your server instance. For example, to connect to Cobalt\u0026rsquo;s demo server, use demo-cubic.cobaltspeech.com:27271. Port 2727 is the default GRPC port that Cubic server binds to.\n 1Commercial use of the demo service is not permitted. This server is for testing and demonstration purposes only and is not guaranteed to support high availability or high volume. Data uploaded to the server may be stored for internal purposes.\n Default Connection The following code snippet connects to the server and queries its version. It uses our recommended default setup, expecting the server to be listening on a TLS encrypted connection, as the demo server does.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() version, err := client.Version(context.Background()) if err != nil { log.Fatal(err) } fmt.Println(version) }  import cubic serverAddress = \u0026#34;127.0.0.1:2727\u0026#34; client = cubic.Client(serverAddress) resp = client.Version() print(resp)  var creds = new Grpc.Core.SslCredentials(); var channel = new Grpc.Core.Channel(serverAddr, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel); var resp = client.Version(new Google.Protobuf.WellKnownTypes.Empty()); Console.WriteLine(String.Format(\u0026#34;CubicServer: {0}, Cubic: {1}\u0026#34;, resp.Server, resp.Cubic));    \nInsecure Connection It is sometimes required to connect to Cubic server without TLS enabled, such as during debugging.\nPlease note that if the server has TLS enabled, attempting to connect with an insecure client will fail. To connect to such an instance of cubic server, you can use:\n client, err := cubic.NewClient(serverAddr, cubic.WithInsecure())  client = cubic.Client(serverAddress, insecure=True)  var creds = Grpc.Core.ChannelCredentials.Insecure; var channel = new Grpc.Core.Channel(serverAddr, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel);    \nClient Authentication In our recommended default setup, TLS is enabled in the gRPC setup, and when connecting to the server, clients validate the server\u0026rsquo;s SSL certificate to make sure they are talking to the right party. This is similar to how \u0026ldquo;https\u0026rdquo; connections work in web browsers.\nIn some setups, it may be desired that the server should also validate clients connecting to it and only respond to the ones it can verify. If your Cubic server is configured to do client authentication, you will need to present the appropriate certificate and key when connecting to it.\nPlease note that in the client-authentication mode, the client will still also verify the server\u0026rsquo;s certificate, and therefore this setup uses mutually authenticated TLS. This can be done with:\n client, err := cubic.NewClient(serverAddr, cubic.WithClientCert(certPem, keyPem))  client = cubic.Client(serverAddress, clientCertificate=certPem, clientKey=keyPem)  var creds = new Grpc.Core.SslCredentials(File.ReadAllText(\u0026#34;root.pem\u0026#34;)); var channel = new Grpc.Core.Channel(serverAddr, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel);    \nwhere certPem and keyPem are the bytes of the client certificate and key provided to you.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/recognize/",
	"title": "Synchronous Recognition",
	"tags": [],
	"description": "",
	"content": "Note Synchronous recognize requests are suitable only for audio clips shorter than 30 seconds. In general, it is strongly recommended that you use streaming recognition.  \nThe following example shows how to transcribe a short audio clip using Cubic\u0026rsquo;s Synchoronous Recognize Request. It is assumed that the audio file contains raw samples, PCM16SLE like Cubic expects. We will query the server for available models and use the first model to transcribe this speech.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model  model := modelResp.Models[0] f, err := os.Open(\u0026#34;test.raw\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } recResp, err := client.Recognize(context.Background(), cfg, f) for _, r := range recResp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } }  import cubic serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # open audio file  audio = open(\u0026#39;test.raw\u0026#39;, \u0026#39;rb\u0026#39;) resp = client.Recognize(cfg, audio) for result in resp.results: if not result.is_partial: print(result.alternatives[0].transcript)   \n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/streaming/",
	"title": "Streaming Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe an audio file using Cubic’s Streaming Recognize Request. The stream can come from a file on disk or be directly from a microphone in real time.\nStreaming from an audio file We support several file formats including WAV, MP3, FLAC etc. For more details, please see the protocol buffer specification file in the SDK repository (grpc/cubic.proto). The examples below use a WAV file as input to the streaming recognition. We will query the server for available models and use the first model to transcribe the speech.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model  model := modelResp.Models[0] f, err := os.Open(\u0026#34;test.wav\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } // Define a callback function to handle results  resultHandler := func(resp *cubicpb.RecognitionResponse) { for _, r := range resp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } } err = client.StreamingRecognize(context.Background(), cfg, f, resultHandler) if err != nil { log.Fatal(err) } }  import cubic serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # client.StreamingRecognize takes any binary # stream object that has a read(nBytes) method. # The method should return nBytes from the stream. # open audio file stream audio = open(\u0026#39;test.wav\u0026#39;, \u0026#39;rb\u0026#39;) # send streaming request to cubic and  # print out results as they come in for resp in client.StreamingRecognize(cfg, audio): for result in resp.results: if result.is_partial: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\u0026#34;) else: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\\n\u0026#34;)  // Initialize a gRPC connection var creds = Grpc.Core.ChannelCredentials.Insecure; var channel = new Grpc.Core.Channel(url, creds); var client = new CobaltSpeech.Cubic.Cubic.CubicClient(channel); // List the available models var listModelsRequest = new CobaltSpeech.Cubic.ListModelsRequest(); var models = client.ListModels(listModelsRequest); // Setup the bi-directional gRPC stream. var call = client.StreamingRecognize(); using (call) { // Setup recieve task  var responseReaderTask = Task.Run(async () =\u0026gt; { // Wait for the next response  while (await call.ResponseStream.MoveNext()) { var response = call.ResponseStream.Current; foreach (var result in response.Results) { Console.WriteLine(result.Alternatives[0].Transcript); } } }); // Send config first, followed by the audio  { // Send the configs  var request = new CobaltSpeech.Cubic.StreamingRecognizeRequest(); request.Config = cfg; await call.RequestStream.WriteAsync(request); // Setup object for streaming audio  request.Config = null; request.Audio = new CobaltSpeech.Cubic.RecognitionAudio { }; // Send the audio, in 8kb chunks  const int chunkSize = 8192; using (FileStream file = File.OpenRead(\u0026#34;test.raw\u0026#34;)) { int bytesRead; var buffer = new byte[chunkSize]; while ((bytesRead = file.Read(buffer, 0, buffer.Length)) \u0026gt; 0) { var bytes = Google.Protobuf.ByteString.CopyFrom(buffer); request.Audio.Data = bytes; await call.RequestStream.WriteAsync(request); } // Close the sending stream  await call.RequestStream.CompleteAsync(); } } // Wait for all of the responses to come back through the receiving stream  await responseReaderTask; }    \nStreaming from microphone Streaming audio from microphone input typically needs us to interact with system libraries. There are several options available, and although the examples here use one, you may choose to use an alternative as long as the recording audio format is chosen correctly.\n This example utilizes the portaudio bindings for Go to stream audio from a microphone. To use this package, install PortAudio development headers and libraries using an appropriate package manager for your system (e.g. apt-get install portaudio19-dev on Ubuntu, brew install portaudio for OSX, etc.) or build from source.\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;encoding/binary\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; \u0026#34;github.com/gordonklaus/portaudio\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; // Microphone implements the io.ReadCloser interface and provides // a data stream for microphone input. type Microphone struct { buffer []int16 stream *portaudio.Stream } // NewMicrophone instantiates a Microphone object with the desired // sampling rate and buffer size. When streaming to cubic, the sample // rate should be set to the sample rate of the model used. func NewMicrophone(sampleRate, bufferSize uint32) (*Microphone, error) { // bufferSize is measured in number of bytes. Since we are capturing  // 16 bit audio, each sample is 2 bytes. The microphone has a int16  // buffer, so we use the number of samples as its size.  numSamples := bufferSize/2 mic := Microphone{buffer: make([]int16, numSamples)} portaudio.Initialize() stream, err := portaudio.OpenDefaultStream(1, 0, float64(sampleRate), int(numSamples), mic.buffer) if err != nil { return nil, err } mic.stream = stream err = mic.stream.Start() if err != nil { return nil, err } return \u0026amp;mic, nil } // Read copies N bytes into the passed buffer from the microphone audio buffer // where N is the buffer size passed to `cubic.NewClient`. Also, to be compatible // with the cubic client, it returns two things : an int representing the number of // bytes copied and an error. func (mic *Microphone) Read(buffer []byte) (int, error) { err := mic.stream.Read() if err != nil { return 0, err } byteBuffer := new(bytes.Buffer) err = binary.Write(byteBuffer, binary.LittleEndian, mic.buffer) if err != nil { return 0, err } copy(buffer, byteBuffer.Bytes()) return len(buffer), nil } // Close shuts the microphone stream down and cleans up func (mic *Microphone) Close() { mic.stream.Stop() mic.stream.Close() portaudio.Terminate() } func main() { bufferSize := uint32(8192) client, err := cubic.NewClient(serverAddr, cubic.WithInsecure(), cubic.WithStreamingBufferSize(bufferSize)) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model  model := modelResp.Models[0] cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } // Define a callback function to handle results  resultHandler := func(resp *cubicpb.RecognitionResponse) { for _, r := range resp.Results { if r.IsPartial { fmt.Print(\u0026#34;\\r\u0026#34;, r.Alternatives[0].Transcript) // print on same line  } else { fmt.Println(\u0026#34;\\r\u0026#34;, r.Alternatives[0].Transcript) // print and move to new line  } } } // Create microphone stream  mic, err := NewMicrophone(model.Attributes.SampleRate, bufferSize) if err != nil { log.Fatal(err) } defer mic.Close() // Since Microphone implements the io.ReadCloser interface, we can  // pass it directly to the StreamingRecognize function.  err = client.StreamingRecognize(context.Background(), cfg, mic, resultHandler) if err != nil { log.Fatal(err) } }  This example requires the pyaudio module to stream audio from a microphone. Instructions for installing pyaudio for different systems are available at the link. On most platforms, this is simply pip install pyaudio.\nimport cubic import pyaudio serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress) # get list of available models modelResp = client.ListModels() # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # client.StreamingRecognize takes any binary stream object that has a read(nBytes) # method. The method should return nBytes from the stream. So pyaudio is a suitable # library to use here for streaming audio from the microphone. Other libraries or # modules may also be used as long as they have the read method or have been wrapped # to do so. # open microphone stream p = pyaudio.PyAudio() audio = p.open(format=pyaudio.paInt16, # 16 bit samples channels=1, # mono audio rate=model.attributes.sample_rate, # sample rate in hertz input=True) # audio input stream # send streaming request to cubic and  # print out results as they come in try: for resp in client.StreamingRecognize(cfg, audio): for result in resp.results: if result.is_partial: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\u0026#34;) else: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\\n\u0026#34;) except KeyboardInterrupt: # stop streaming when ctrl+C pressed pass except Exception as err: print(\u0026#34;Error while trying to stream audio : {}\u0026#34;.format(err)) audio.stop_stream() audio.close()  We do not currently have example C# code for streaming from a microphone. Simply pass the bytes from the microphone the same as is done from the file in the Streaming from an audio file example above.\n  \n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/using-cubic-sdk/client-configs/",
	"title": "Recognition Configurations",
	"tags": [],
	"description": "",
	"content": " An in-depth explanation of the various fields of the complete SDK can be found here. The sub-section RecognitionConfig is particularly important here.\nThis page here discusses the more common combinations sent to the server.\nFields Here is a quick overview of the fields.\n   Field Required Default Description     model_id Yes  Unique ID of the model to use.   audio_encoding Yes  Encoding format of the audio, such as RAW_LINEAR_16, WAV, MP3, etc.   idle_timeout No 0s (Unlimited) Maximum time allowed between each gRPC message. The server may place further restrictions depending on its configuration.   enable_word_time_offsets No false Toggles the calculation of word-level timestamps. The specified model must also support word-level timestamps for this field to be populated.   enable_word_confidence No false Toggles the calculation of word-level confidence scores. The specified model must also support word-level confidence for this field to be populated.   enable_raw_transcript No false If true, the raw transcript will be included in the results.   enable_confusion_network No false Toggles the inclusion of a confusion network, consisting of multiple alternative transcriptions. The specified model must also support confusion networks for this field to be populated.   audio_channels No [0] (mono) Specifies which channels of a multi-channel audio file to be transcribed, each as their own individual audio stream.    Use cases The most basic use case is getting a formatted transcript. This would simply need a config such as:\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34; } and your resulting transcript would be found at results.alternatives[*].transcript.\nSometimes, only the raw (unformatted) transcript is desired. In this case, there are two options.\n Disable server-side formatting and use the above config. Retrieve the raw transcript from the results.alternatives[*].transcript field. Specify the enable_raw_transcript = true flag, and access the field results.alternatives[*].raw_transcript.   Note: Prior to cubicsvr v2.9.0 and SDK-Cubic v1.3.0, the field results.alternatives[*].transcript was populated with either the raw or the formatted transcription depending on the enable_raw_transcript config. After these changes, raw transcripts have been pushed to a new field results.alternatives[*].raw_transcript, and only populated when enable_raw_transcript is set to true.\n Another use case would be getting both the formatted and raw transcript. This can be done using this config.\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: \u0026#34;true\u0026#34; } The formatted transcript would be found at results.alternatives[*].transcript, and the raw transcript would be found at results.alternatives[*].raw_transcript.\nIf you need to know the timestamp for each word, to align subtitles with a video, for example, then you can use this config to enable those word-level timestamps. Please note the inclusion of enable_raw_transcript; the word-level information corresponds to the raw transcript, since the formatter may combine multiple words into one symbol in the formatted transcript (e.g. \u0026ldquo;twenty one\u0026rdquo; to \u0026ldquo;21\u0026rdquo;)\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_word_time_offsets\u0026#34;: true } Timestamps will be found at results.alternatives[*].words[*].start_time and results.alternatives[*].words[*].duration.\nWord-level confidences, i.e. for displaying a lighter color for less confident words, can be included much like the word-level timestamps. Please note the inclusion of enable_raw_transcript.\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_word_time_offsets\u0026#34;: true } Word-level confidence scores will be found at results.alternatives[*].words[*].confidence.\nFor applications that need more than the one-best transcription, the most comprehensive and detailed Cubic results are found in the confusion network. Please refer to the in depth confusion network documentation to see what is included.\nTo enable the confusion network, the config will look similar to this:\n{ \u0026#34;model_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;audio_encoding\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;enable_raw_transcript\u0026#34;: true, \u0026#34;enable_confusion_network\u0026#34;: true } The confusion network can be accessed at result.cnet.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/autogen-doc-cubic-proto/",
	"title": "Cubic Protobuf API Docs",
	"tags": [],
	"description": "",
	"content": " cubic.proto Service: Cubic Service that implements the Cobalt Cubic Speech Recognition API\n   Method Name Request Type Response Type Description     Version .google.protobuf.Empty VersionResponse Queries the Version of the Server   ListModels ListModelsRequest ListModelsResponse Retrieves a list of available speech recognition models   Recognize RecognizeRequest RecognitionResponse Performs synchronous speech recognition: receive results after all audio has been sent and processed. It is expected that this request be typically used for short audio content: less than a minute long. For longer content, the StreamingRecognize method should be preferred.   StreamingRecognize StreamingRecognizeRequest RecognitionResponse Performs bidirectional streaming speech recognition. Receive results while sending audio. This method is only available via GRPC and not via HTTP+JSON. However, a web browser may use websockets to use this service.    Message: ConfusionNetworkArc An Arc inside a Confusion Network Link\n   Field Type Label Description     word string  Word in the recognized transcript\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n    Message: ConfusionNetworkLink A Link inside a confusion network\n   Field Type Label Description     start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this link\n   duration google.protobuf.Duration  Duration of the current link in the confusion network\n   arcs ConfusionNetworkArc repeated Arcs between this link\n    Message: ListModelsRequest The top-level message sent by the client for the ListModels method.\nThis message is empty and has no fields.\nMessage: ListModelsResponse The message returned to the client by the ListModels method.\n   Field Type Label Description     models Model repeated List of models available for use that match the request.\n    Message: Model Description of a Cubic Model\n   Field Type Label Description     id string  Unique identifier of the model. This identifier is used to choose the model that should be used for recognition, and is specified in the RecognitionConfig message.\n   name string  Model name. This is a concise name describing the model, and maybe presented to the end-user, for example, to help choose which model to use for their recognition task.\n   attributes ModelAttributes  Model attributes\n    Message: ModelAttributes Attributes of a Cubic Model\n   Field Type Label Description     sample_rate uint32  Audio sample rate supported by the model\n    Message: RecognitionAlternative A recognition hypothesis\n   Field Type Label Description     transcript string  Text representing the transcription of the words that the user spoke.\nThe transcript will be formatted according to the servers formatting configuration. If you want the raw transcript, please see the field raw_transcript. If the server is configured to not use any formatting, then this field will contain the raw transcript.\nAs an example, if the spoken utterance was \u0026ldquo;four people\u0026rdquo;, and the server was configured to format numbers, this field would be set to \u0026ldquo;4 people\u0026rdquo;.\n   raw_transcript string  Text representing the transcription of the words that the user spoke, without any formatting. This field will be populated only the config RecognitionConfig.enable_raw_transcript is set to true. Otherwise this field will be an empty string. If you want the formatted transcript, please see the field transcript.\nAs an example, if the spoken utterance was here are four words, this field would be set to \u0026ldquo;HERE ARE FOUR WORDS\u0026rdquo;.\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood of the output being correct.\n   words WordInfo repeated A list of word-specific information for each recognized word. This is available only if enable_word_confidence or enable_word_time_offsets was set to true in the RecognitionConfig.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this utterance.\n   duration google.protobuf.Duration  Duration of the current utterance in the spoken audio.\n    Message: RecognitionAudio Audio to be sent to the recognizer\n   Field Type Label Description     data bytes  \n    Message: RecognitionConfig Configuration for setting up a Recognizer\n   Field Type Label Description     model_id string  Unique identifier of the model to use, as obtained from a Model message.\n   audio_encoding RecognitionConfig.Encoding  Encoding of audio data sent/streamed through the RecognitionAudio messages. For encodings like WAV/MP3 that have headers, the headers are expected to be sent at the beginning of the stream, not in every RecognitionAudio message.\nIf not specified, the default encoding is RAW_LINEAR16.\nDepending on how they are configured, server instances of this service may not support all the encodings enumerated above. They are always required to accept RAW_LINEAR16. If any other Encoding is specified, and it is not available on the server being used, the recognition request will result in an appropriate error message.\n   idle_timeout google.protobuf.Duration  Idle Timeout of the created Recognizer. If no audio data is received by the recognizer for this duration, ongoing rpc calls will result in an error, the recognizer will be destroyed and thus more audio may not be sent to the same recognizer. The server may impose a limit on the maximum idle timeout that can be specified, and if the value in this message exceeds that serverside value, creating of the recognizer will fail with an error.\n   enable_word_time_offsets bool  This is an optional field. If this is set to true, each result will include a list of words and the start time offset (timestamp) and the duration for each of those words. If set to false, no word-level timestamps will be returned. The default is false.\n   enable_word_confidence bool  This is an optional field. If this is set to true, each result will include a list of words and the confidence for those words. If false, no word-level confidence information is returned. The default is false.\n   enable_raw_transcript bool  This is an optional field. If this is set to true, the field RecognitionAlternative.raw_transcript will be populated with the raw transcripts output from the recognizer will be exposed without any formatting rules applied. If this is set to false, that field will not be set in the results. The RecognitionAlternative.transcript will always be populated with text formatted according to the server\u0026rsquo;s settings.\n   enable_confusion_network bool  This is an optional field. If this is set to true, the results will include a confusion network. If set to false, no confusion network will be returned. The default is false. If the model being used does not support a confusion network, results may be returned without a confusion network available. If this field is set to true, then enable_raw_transcript is also forced to be true.\n   audio_channels uint32 repeated This is an optional field. If the audio has multiple channels, this field should be configured with the list of channel indices that should be transcribed. Channels are 0-indexed.\nExample: [0] for a mono file, [0, 1] for a stereo file.\nIf this field is not set, a mono file will be assumed by default and only channel-0 will be transcribed even if the file actually has additional channels.\nChannels that are present in the audio may be omitted, but it is an error to include a channel index in this field that is not present in the audio. Channels may be listed in any order but the same index may not be repeated in this list.\nBAD: [0, 2] for a stereo file; BAD: [0, 0] for a mono file.\n    Message: RecognitionConfusionNetwork Confusion network in recognition output\n   Field Type Label Description     links ConfusionNetworkLink repeated \n    Message: RecognitionResponse Collection of sequence of recognition results in a portion of audio. When transcribing a single audio channel (e.g. RAW_LINEAR16 input, or a mono file), results will be ordered chronologically. When transcribing multiple channels, the results of all channels will be interleaved. Results of each individual channel will be chronological. No such promise is made for the ordering of results of different channels, as results are returned for each channel individually as soon as they are ready.\n   Field Type Label Description     results RecognitionResult repeated \n    Message: RecognitionResult A recognition result corresponding to a portion of audio.\n   Field Type Label Description     alternatives RecognitionAlternative repeated An n-best list of recognition hypotheses alternatives\n   is_partial bool  If this is set to true, it denotes that the result is an interim partial result, and could change after more audio is processed. If unset, or set to false, it denotes that this is a final result and will not change.\nServers are not required to implement support for returning partial results, and clients should generally not depend on their availability.\n   cnet RecognitionConfusionNetwork  If enable_confusion_network was set to true in the RecognitionConfig, and if the model supports it, a confusion network will be available in the results.\n   audio_channel uint32  Channel of the audio file that this result was transcribed from. For a mono file, or RAW_LINEAR16 input, this will be set to 0.\n    Message: RecognizeRequest The top-level message sent by the client for the Recognize method. Both the RecognitionConfig and RecognitionAudio fields are required. The entire audio data must be sent in one request. If your audio data is larger, please use the StreamingRecognize call..\n   Field Type Label Description     config RecognitionConfig  Provides configuration to create the recognizer.\n   audio RecognitionAudio  The audio data to be recognized\n    Message: StreamingRecognizeRequest The top-level message sent by the client for the StreamingRecognize request. Multiple StreamingRecognizeRequest messages are sent. The first message must contain a RecognitionConfig message only, and all subsequent messages must contain RecognitionAudio only. All RecognitionAudio messages must contain non-empty audio. If audio content is empty, the server may interpret it as end of stream and stop accepting any further messages.\n   Field Type Label Description     config RecognitionConfig  \n   audio RecognitionAudio  \n    Message: VersionResponse The message sent by the server for the Version method.\n   Field Type Label Description     cubic string  version of the cubic library handling the recognition\n   server string  version of the server handling these requests\n    Message: WordInfo Word-specific information for recognized words\n   Field Type Label Description     word string  The actual word in the text\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this spoken word.\n   duration google.protobuf.Duration  Duration of the current word in the spoken audio.\n    Enum: RecognitionConfig.Encoding The encoding of the audio data to be sent for recognition.\nFor best results, the audio source should be captured and transmitted using the RAW_LINEAR16 encoding.\n   Name Number Description     RAW_LINEAR16 0 Raw (headerless) Uncompressed 16-bit signed little endian samples (linear PCM), single channel, sampled at the rate expected by the chosen Model.   WAV 1 WAV (data with RIFF headers), with data sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   MP3 2 MP3 data, sampled at a rate equal to or higher than the sampling rate expected by the chosen Model.   FLAC 3 FLAC data, sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   VOX8000 4 VOX data (Dialogic ADPCM), sampled at 8 KHz.   ULAW8000 5 μ-law (8-bit) encoded RAW data, single channel, sampled at 8 KHz.    Scalar Value Types    .proto Type Notes Go Type Python Type     double  float64 float   float  float32 float   int32 Uses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint32 instead. int32 int   int64 Uses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint64 instead. int64 int/long   uint32 Uses variable-length encoding. uint32 int/long   uint64 Uses variable-length encoding. uint64 int/long   sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int   sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 int/long   fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int   fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 int/long   sfixed32 Always four bytes. int32 int   sfixed64 Always eight bytes. int64 int/long   bool  bool boolean   string A string must always contain UTF-8 encoded or 7-bit ASCII text. string str/unicode   bytes May contain any arbitrary sequence of bytes. []byte str    "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Cubic SDK \u0026ndash; Cobalt\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/",
	"title": "Cubic SDK Documentation",
	"tags": [],
	"description": "",
	"content": " Cubic API Overview Cubic is Cobalt’s automatic speech recognition (ASR) engine. It can be deployed on-prem and accessed over the network or on your local machine via an API. We currently support Go and Python, and are adding support for more languages.\nOnce running, Cubic’s API provides a method to which you can stream audio. This audio can either be from a microphone or a file. We recommend uncompressed WAV as the encoding, but support other formats such as MP3.\nCubic’s API provides a number of options for returning the speech recognition results. The results are passed back using Google’s protobuf library, allowing them to be handled natively by your application. Cubic can estimate its confidence in the transcription result at the word or utterance level, along with timestamps of the words. Confidence scores are in the range 0-1. Cubic’s output options are described below.\nAutomatic Transcription Results The simplest result that Cubic returns is its best guess at the transcription of your audio. Cubic recognizes the audio you are streaming, listens for the end of each utterance, and returns the speech recognition result.\nCubic maintains its transcriptions in an N-best list, i.e. is the top N transcriptions from the recogniser. The best ASR result is the first entry in this list.\n Click here to see an example json representation of Cubic’s N-best list with utterance-level confidence scores\n{ \u0026#34;alternatives\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.514 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.201 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A \u0026lt;UNK\u0026gt; DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.105 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS ISN\u0026#39;T NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.093 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A YOUR DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.087 } ], } \nA single stream may consist of multiple utterances separated by silence. Cubic handles each utterance separately.\nFor longer utterances, it is often useful to see the partial speech recognition results while the audio is being streamed. For example, this allows you to see what the ASR system is predicting in real-time while someone is speaking. Cubic supports both partial and final ASR results.\nConfusion Network A Confusion Network is a form of speech recognition output that’s been turned into a compact graph representation of many possible transcriptions, as here:\nNote that \u0026lt;eps\u0026gt; in this representation is silence.\n Click here to see an example json representation of this Confusion Network object, with time stamps and word-level confidence scores\n{ \u0026#34;cnet\u0026#34;: { \u0026#34;links\u0026#34;: [ { \u0026#34;duration\u0026#34;: \u0026#34;1.350s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;0s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.690s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;TOMORROW\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;1.350s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.080s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.040s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.168s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;IS\u0026#34;, \u0026#34;confidence\u0026#34;: 0.892 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.120s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.010s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.288s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.093s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;confidence\u0026#34;: 0.620 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.233 }, { \u0026#34;word\u0026#34;: \u0026#34;ISN\u0026#39;T\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 }, { \u0026#34;word\u0026#34;: \u0026#34;THE\u0026#34;, \u0026#34;confidence\u0026#34;: 0.039 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.298s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.005s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.391s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.273s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;NEW\u0026#34;, \u0026#34;confidence\u0026#34;: 0.661 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.129 }, { \u0026#34;word\u0026#34;: \u0026#34;YOUR\u0026#34;, \u0026#34;confidence\u0026#34;: 0.107 }, { \u0026#34;word\u0026#34;: \u0026#34;YOU\u0026#34;, \u0026#34;confidence\u0026#34;: 0.102 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.396s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.420s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.954 }, { \u0026#34;word\u0026#34;: \u0026#34;TODAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.044 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.002 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.270s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;3.090s\u0026#34; } ] } } \nFormatted output Speech recognition systems typically output the words that were spoken, with no formatting. For example, utterances with numbers in might return “twenty seven bridges”, and “the year two thousand and three”. Cubic has the option to enable basic formatting of speech recognition results:\n Capitalising the first letter of the utterance Numbers: “cobalt’s atomic number is twenty seven” -\u0026gt; “Cobalt’s atomic number is 27” Truecasing: “the iphone was launched in two thousand and seven” -\u0026gt; “The iPhone was launched in 2007” Ordinals: “summer solstice is twenty first june” -\u0026gt; “Summer solstice is 21st June”  Note that word level timestamps and confidences aren’t supported when formatting is enabled.\nObtaining Cubic Cobalt will provide you with a package of Cubic that contains the engine, appropriate speech recognition models and a server application. This server exports Cubic\u0026rsquo;s functionality over the gRPC protocol. The https://github.com/cobaltspeech/sdk-cubic repository contains the SDK that you can use in your application to communicate with the Cubic server. This SDK is currently available for the Go and Python languages; and we would be happy to talk to you if you need support for other languages. Most of the core SDK is generated automatically using the gRPC tools, and Cobalt provides a top level package for more convenient API calls.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]
