[
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/",
	"title": "Getting started",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/quickstart/",
	"title": "Quick Start",
	"tags": [],
	"description": "",
	"content": "To evaluate Cobalt\u0026rsquo;s speech recognition, you can access the Cubic web demo or can download a command-line interface built using our Go SDK to specify an audio file or list of files and get transcripts back from a running instance of cubicsvr. The source code for cubic-cli is available to use as an example client.\nThe command-line tool can call a local instance of Cubic (see Installing the Cubicsvr Image) or the demo server.\n# Local instance bin/cubic-cli --insecure --server localhost:2727 transcribe ./test.wav # Demo server ./bin/cubic-cli --server demo-cubic.cobaltspeech.com:2727 transcribe ./test.wav Commercial use of the demo service is not permitted. This server is for testing and demonstration purposes only and is not guaranteed to support high availability or high volume. Data uploaded to the server may be stored for internal purposes.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/cubic_docker/",
	"title": "Installing the Cubicsvr Image",
	"tags": [],
	"description": "",
	"content": "The SDK is used to call an instance of cubicsvr using gRPC. Cobalt distributes a docker image that contains the cubicsvr binary and model files. It is not necessary to go through through these steps to call the demo server for evaluation purposes, only to run cubicsvr on premises.\n Contact Cobalt to get a link to the image file in AWS S3. This link will expire in two weeks, so be sure to download the file to your own server.\n Download with the AWS CLI if you have it, or with curl:\nURL=\u0026#34;the url sent by Cobalt\u0026#34; IMAGE_NAME=\u0026#34;name you want to give the file (should end with the same extension as the url, usually bz2)\u0026#34; curl $URL -L -o $IMAGE_NAME Load the docker image:\ndocker load \u0026lt; $IMAGE_NAME This will output the name of the image (e.g. cubicsvr-demo-en_us-16).\n Start the cubic service listening:\ndocker run -p 2727:2727 -p 8080:8080 --name cobalt cubicsvr-demo-en_us-16 That will start listening for grpc commands on port 2727 and http requests on 8080, and will stream the debug log to stdout. (You can replace --name cobalt with whatever name you want. That just provides a way to refer back to the currently running container.)\n Verify the service is running by calling\ncurl http://localhost:8080/api/version If you want to explore the package to see the model files etc, call the following to open the bash terminal on the previously run image. Model files are located in the /model directory.\ndocker exec -it cobalt bash  Contents of the docker image  Base docker image : debian-stretch-slim Additional dependencies (installed with yum install on centos or apt-get on ubuntu)  libgfortran3 sox   Cobalt-specific files  cubicsvr - binary for performing Automatic Speech Recognition model.config - top-level config am/nnet3_online/final.mdl - this is the acoustic model am/nnet3_online/conf/online_cmvn.conf - feature extraction parameters for the features fed into the GMM model used for i-vector statistics accumulation. am/nnet3_online/conf/splice.conf - GMM feature context when accumulating statistics for i-vector accumulation. am/nnet3_online/ivector_extractor/* - Kaldi configuration files related to ivectors\n graph/HCLG.fst - the decoding graph: the combination of the AMs transition graph, the lexicon, and the language model graph/words.txt - an integer to word mapping, needed because the output of the HCLG graph contains only integer symbol IDs graph/phones/word_boundary.int - this is needed only when confusion network output is requested, it tells the decoder which phones are at word boundaries "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/installation/",
	"title": "Installing the SDK",
	"tags": [],
	"description": "",
	"content": "Instructions for installing the SDK are language specific.\nGo The Go SDK supports Go modules and requires Go 1.12 or later. To use the SDK, import this package into your application:\nimport \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; Python The Python SDK depends on Python \u0026gt;= 3.5. You may use pip to perform a system-wide install, or use virtualenv for a local install.\npip install --upgrade pip pip install \u0026#34;git+https://github.com/cobaltspeech/sdk-cubic#egg=cobalt-cubic\u0026amp;subdirectory=grpc/py-cubic\u0026#34;"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/",
	"title": "Cubic API Reference",
	"tags": [],
	"description": "",
	"content": "The Cubic API is specified as a proto file. This section of the documentation is auto-generated from the spec. It describes the data types and functions defined in the spec. The \u0026ldquo;messages\u0026rdquo; below correspond to the data structures to be used, and the \u0026ldquo;service\u0026rdquo; contains the methods that can be called.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/connecting/",
	"title": "Setup Connection",
	"tags": [],
	"description": "",
	"content": "Once you have your Cubic server up and running, let\u0026rsquo;s see how we can use the SDK to connect to it.\nFirst, you need to know the address (host:port) where the server is running. This document will assume the values 127.0.0.1:2727, but be sure to change those to point to your server instance. For example, to connect to Cobalt\u0026rsquo;s demo server, use demo-cubic.cobaltspeech.com:27271. Port 2727 is the default GRPC port that Cubic server binds to.\n 1Commercial use of the demo service is not permitted. This server is for testing and demonstration purposes only and is not guaranteed to support high availability or high volume. Data uploaded to the server may be stored for internal purposes.\n Default Connection The following code snippet connects to the server and queries its version. It uses our recommended default setup, expecting the server to be listening on a TLS encrypted connection, as the demo server does.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() version, err := client.Version(context.Background()) if err != nil { log.Fatal(err) } fmt.Println(version) }  import cubic serverAddress = \u0026#34;127.0.0.1:2727\u0026#34; client = cubic.Client(serverAddress) resp = client.Version() print(resp)   \nInsecure Connection It is sometimes required to connect to Cubic server without TLS enabled, such as during debugging.\nPlease note that if the server has TLS enabled, attempting to connect with an insecure client will fail. To connect to such an instance of cubic server, you can use:\n client, err := cubic.NewClient(serverAddr, cubic.WithInsecure())  client = cubic.Client(serverAddress, insecure=True)   \nClient Authentication In our recommended default setup, TLS is enabled in the gRPC setup, and when connecting to the server, clients validate the server\u0026rsquo;s SSL certificate to make sure they are talking to the right party. This is similar to how \u0026ldquo;https\u0026rdquo; connections work in web browsers.\nIn some setups, it may be desired that the server should also validate clients connecting to it and only respond to the ones it can verify. If your Cubic server is configured to do client authentication, you will need to present the appropriate certificate and key when connecting to it.\nPlease note that in the client-authentication mode, the client will still also verify the server\u0026rsquo;s certificate, and therefore this setup uses mutually authenticated TLS. This can be done with:\n client, err := cubic.NewClient(serverAddr, cubic.WithClientCert(certPem, keyPem))  client = cubic.Client(serverAddress, clientCertificate=certPem, clientKey=keyPem)   \nwhere certPem and keyPem are the bytes of the client certificate and key provided to you.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/recognize/",
	"title": "Synchronous Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe a short audio clip using Cubic\u0026rsquo;s Synchoronous Recognize Request. It is assumed that the audio file contains raw samples, PCM16SLE like Cubic expects. We will query the server for available models and use the first model to transcribe this speech.\nSynchronous recognize requests are suitable only for audio clips shorter than 30 seconds. In general, it is strongly recommended that you use streaming recognition.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model  model := modelResp.Models[0] f, err := os.Open(\u0026#34;test.raw\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } recResp, err := client.Recognize(context.Background(), cfg, f) for _, r := range recResp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } }  import cubic serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # open audio file  audio = open(\u0026#39;test.raw\u0026#39;, \u0026#39;rb\u0026#39;) resp = client.Recognize(cfg, audio) for result in resp.results: if not result.is_partial: print(result.alternatives[0].transcript)   \n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/streaming/",
	"title": "Streaming Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe an audio file using Cubic’s Streaming Recognize Request. The example uses a WAV file as input to the streaming recognition. We will query the server for available models and use the first model to transcribe the speech.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model  model := modelResp.Models[0] f, err := os.Open(\u0026#34;test.wav\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } // define a callback function to handle results  resultHandler := func(resp *cubicpb.RecognitionResponse) { for _, r := range resp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } } err = client.StreamingRecognize(context.Background(), cfg, f, resultHandler) if err != nil { log.Fatal(err) } }  import cubic serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # client.StreamingRecognize takes any binary # stream object that has a read(nBytes) method. # The method should return nBytes from the stream. # open audio file stream audio = open(\u0026#39;test.wav\u0026#39;, \u0026#39;rb\u0026#39;) # send streaming request to cubic and  # print out results as they come in for resp in client.StreamingRecognize(cfg, audio): for result in resp.results: if result.is_partial: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\u0026#34;) else: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\\n\u0026#34;)   \n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/autogen-doc-cubic-proto/",
	"title": "Cubic Protobuf API Docs",
	"tags": [],
	"description": "",
	"content": " cubic.proto Service: Cubic Service that implements the Cobalt Cubic Speech Recognition API\n   Method Name Request Type Response Type Description     Version .google.protobuf.Empty VersionResponse Queries the Version of the Server   ListModels ListModelsRequest ListModelsResponse Retrieves a list of available speech recognition models   Recognize RecognizeRequest RecognitionResponse Performs synchronous speech recognition: receive results after all audio has been sent and processed. It is expected that this request be typically used for short audio content: less than a minute long. For longer content, the StreamingRecognize method should be preferred.   StreamingRecognize StreamingRecognizeRequest RecognitionResponse Performs bidirectional streaming speech recognition. Receive results while sending audio. This method is only available via GRPC and not via HTTP+JSON. However, a web browser may use websockets to use this service.    Message: ConfusionNetworkArc An Arc inside a Confusion Network Link\n   Field Type Label Description     word string  Word in the recognized transcript\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n    Message: ConfusionNetworkLink A Link inside a confusion network\n   Field Type Label Description     start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this link\n   duration google.protobuf.Duration  Duration of the current link in the confusion network\n   arcs ConfusionNetworkArc repeated Arcs between this link\n    Message: ListModelsRequest The top-level message sent by the client for the ListModels method.\nThis message is empty and has no fields.\nMessage: ListModelsResponse The message returned to the client by the ListModels method.\n   Field Type Label Description     models Model repeated List of models available for use that match the request.\n    Message: Model Description of a Cubic Model\n   Field Type Label Description     id string  Unique identifier of the model. This identifier is used to choose the model that should be used for recognition, and is specified in the RecognitionConfig message.\n   name string  Model name. This is a concise name describing the model, and maybe presented to the end-user, for example, to help choose which model to use for their recognition task.\n   attributes ModelAttributes  Model attributes\n    Message: ModelAttributes Attributes of a Cubic Model\n   Field Type Label Description     sample_rate uint32  Audio sample rate supported by the model\n    Message: RecognitionAlternative A recognition hypothesis\n   Field Type Label Description     transcript string  Text representing the transcription of the words that the user spoke.\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood of the output being correct.\n   words WordInfo repeated A list of word-specific information for each recognized word. This is available only if enable_word_confidence or enable_word_time_offsets was set to true in the RecognitionConfig.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this utterance.\n   duration google.protobuf.Duration  Duration of the current utterance in the spoken audio.\n    Message: RecognitionAudio Audio to be sent to the recognizer\n   Field Type Label Description     data bytes  \n    Message: RecognitionConfig Configuration for setting up a Recognizer\n   Field Type Label Description     model_id string  Unique identifier of the model to use, as obtained from a Model message.\n   audio_encoding RecognitionConfig.Encoding  Encoding of audio data sent/streamed through the RecognitionAudio messages. For encodings like WAV/MP3 that have headers, the headers are expected to be sent at the beginning of the stream, not in every RecognitionAudio message.\nIf not specified, the default encoding is RAW_LINEAR16.\nDepending on how they are configured, server instances of this service may not support all the encodings enumerated above. They are always required to accept RAW_LINEAR16. If any other Encoding is specified, and it is not available on the server being used, the recognition request will result in an appropriate error message.\n   idle_timeout google.protobuf.Duration  Idle Timeout of the created Recognizer. If no audio data is received by the recognizer for this duration, ongoing rpc calls will result in an error, the recognizer will be destroyed and thus more audio may not be sent to the same recognizer. The server may impose a limit on the maximum idle timeout that can be specified, and if the value in this message exceeds that serverside value, creating of the recognizer will fail with an error.\n   enable_word_time_offsets bool  This is an optional field. If this is set to true, each result will include a list of words and the start time offset (timestamp) and the duration for each of those words. If set to false, no word-level timestamps will be returned. The default is false.\n   enable_word_confidence bool  This is an optional field. If this is set to true, each result will include a list of words and the confidence for those words. If false, no word-level confidence information is returned. The default is false.\n   enable_raw_transcript bool  This is an optional field. If this is set to true, the transcripts will be presented as raw output from the recognizer without any formatting rules applied. They will be in all UPPER CASE, numbers and other special entities would be presented as the spoken words. If set to false, formatting rules will be applied to all results. The default is false.\nAs an example, if the spoken utterance was here are four words: with this field set to false: \u0026ldquo;Here are 4 words\u0026rdquo; with this field set to \u0026lsquo;true\u0026rsquo; : \u0026ldquo;HERE ARE FOUR WORDS\u0026rdquo;\n   enable_confusion_network bool  This is an optional field. If this is set to true, the results will include a confusion network. If set to false, no confusion network will be returned. The default is false. If the model being used does not support a confusion network, results may be returned without a confusion network available. If this field is set to true, then enable_raw_transcript is also forced to be true.\n   audio_channels uint32 repeated This is an optional field. If the audio has multiple channels, this field should be configured with the list of channel indices that should be transcribed. Channels are 0-indexed.\nExample: [0] for a mono file, [0, 1] for a stereo file.\nIf this field is not set, a mono file will be assumed by default and only channel-0 will be transcribed even if the file actually has additional channels.\nChannels that are present in the audio may be omitted, but it is an error to include a channel index in this field that is not present in the audio. Channels may be listed in any order but the same index may not be repeated in this list.\nBAD: [0, 2] for a stereo file; BAD: [0, 0] for a mono file.\n    Message: RecognitionConfusionNetwork Confusion network in recognition output\n   Field Type Label Description     links ConfusionNetworkLink repeated \n    Message: RecognitionResponse Collection of sequence of recognition results in a portion of audio. When transcribing a single audio channel (e.g. RAW_LINEAR16 input, or a mono file), results will be ordered chronologically. When transcribing multiple channels, the results of all channels will be interleaved. Results of each individual channel will be chronological. No such promise is made for the ordering of results of different channels, as results are returned for each channel individually as soon as they are ready.\n   Field Type Label Description     results RecognitionResult repeated \n    Message: RecognitionResult A recognition result corresponding to a portion of audio.\n   Field Type Label Description     alternatives RecognitionAlternative repeated An n-best list of recognition hypotheses alternatives\n   is_partial bool  If this is set to true, it denotes that the result is an interim partial result, and could change after more audio is processed. If unset, or set to false, it denotes that this is a final result and will not change.\nServers are not required to implement support for returning partial results, and clients should generally not depend on their availability.\n   cnet RecognitionConfusionNetwork  If enable_confusion_network was set to true in the RecognitionConfig, and if the model supports it, a confusion network will be available in the results.\n   audio_channel uint32  Channel of the audio file that this result was transcribed from. For a mono file, or RAW_LINEAR16 input, this will be set to 0.\n    Message: RecognizeRequest The top-level message sent by the client for the Recognize method. Both the RecognitionConfig and RecognitionAudio fields are required. The entire audio data must be sent in one request. If your audio data is larger, please use the StreamingRecognize call..\n   Field Type Label Description     config RecognitionConfig  Provides configuration to create the recognizer.\n   audio RecognitionAudio  The audio data to be recognized\n    Message: StreamingRecognizeRequest The top-level message sent by the client for the StreamingRecognize request. Multiple StreamingRecognizeRequest messages are sent. The first message must contain a RecognitionConfig message only, and all subsequent messages must contain RecognitionAudio only. All RecognitionAudio messages must contain non-empty audio. If audio content is empty, the server may interpret it as end of stream and stop accepting any further messages.\n   Field Type Label Description     config RecognitionConfig  \n   audio RecognitionAudio  \n    Message: VersionResponse The message sent by the server for the Version method.\n   Field Type Label Description     cubic string  version of the cubic library handling the recognition\n   server string  version of the server handling these requests\n    Message: WordInfo Word-specific information for recognized words\n   Field Type Label Description     word string  The actual word in the text\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this spoken word.\n   duration google.protobuf.Duration  Duration of the current word in the spoken audio.\n    Enum: RecognitionConfig.Encoding The encoding of the audio data to be sent for recognition.\nFor best results, the audio source should be captured and transmitted using the RAW_LINEAR16 encoding.\n   Name Number Description     RAW_LINEAR16 0 Raw (headerless) Uncompressed 16-bit signed little endian samples (linear PCM), single channel, sampled at the rate expected by the chosen Model.   WAV 1 WAV (data with RIFF headers), with data sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   MP3 2 MP3 data, sampled at a rate equal to or higher than the sampling rate expected by the chosen Model.   FLAC 3 FLAC data, sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   VOX8000 4 VOX data (Dialogic ADPCM), sampled at 8 KHz.   ULAW8000 5 μ-law (8-bit) encoded RAW data, single channel, sampled at 8 KHz.    Scalar Value Types    .proto Type Notes Go Type Python Type     double  float64 float   float  float32 float   int32 Uses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint32 instead. int32 int   int64 Uses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint64 instead. int64 int/long   uint32 Uses variable-length encoding. uint32 int/long   uint64 Uses variable-length encoding. uint64 int/long   sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int   sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 int/long   fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int   fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 int/long   sfixed32 Always four bytes. int32 int   sfixed64 Always eight bytes. int64 int/long   bool  bool boolean   string A string must always contain UTF-8 encoded or 7-bit ASCII text. string str/unicode   bytes May contain any arbitrary sequence of bytes. []byte str    "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Cubic SDK \u0026ndash; Cobalt\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/",
	"title": "Cubic SDK Documentation",
	"tags": [],
	"description": "",
	"content": " Cubic API Overview Cubic is Cobalt’s automatic speech recognition (ASR) engine. It can be deployed on-prem and accessed over the network or on your local machine via an API. We currently support Go and Python, and are adding support for more languages.\nOnce running, Cubic’s API provides a method to which you can stream audio. This audio can either be from a microphone or a file. We recommend uncompressed WAV as the encoding, but support other formats such as MP3.\nCubic’s API provides a number of options for returning the speech recognition results. The results are passed back using Google’s protobuf library, allowing them to be handled natively by your application. Cubic can estimate its confidence in the transcription result at the word or utterance level, along with timestamps of the words. Confidence scores are in the range 0-1. Cubic’s output options are described below.\nAutomatic Transcription Results The simplest result that Cubic returns is its best guess at the transcription of your audio. Cubic recognizes the audio you are streaming, listens for the end of each utterance, and returns the speech recognition result.\nCubic maintains its transcriptions in an N-best list, i.e. is the top N transcriptions from the recogniser. The best ASR result is the first entry in this list.\n Click here to see an example json representation of Cubic’s N-best list with utterance-level confidence scores\n{ \u0026#34;alternatives\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.514 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.201 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A \u0026lt;UNK\u0026gt; DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.105 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS ISN\u0026#39;T NEW DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.093 }, { \u0026#34;transcript\u0026#34;: \u0026#34;TOMORROW IS A YOUR DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.087 } ], } \nA single stream may consist of multiple utterances separated by silence. Cubic handles each utterance separately.\nFor longer utterances, it is often useful to see the partial speech recognition results while the audio is being streamed. For example, this allows you to see what the ASR system is predicting in real-time while someone is speaking. Cubic supports both partial and final ASR results.\nConfusion Network A Confusion Network is a form of speech recognition output that’s been turned into a compact graph representation of many possible transcriptions, as here:\nNote that \u0026lt;eps\u0026gt; in this representation is silence.\n Click here to see an example json representation of this Confusion Network object, with time stamps and word-level confidence scores\n{ \u0026#34;cnet\u0026#34;: { \u0026#34;links\u0026#34;: [ { \u0026#34;duration\u0026#34;: \u0026#34;1.350s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;0s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.690s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;TOMORROW\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;1.350s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.080s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.040s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.168s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;IS\u0026#34;, \u0026#34;confidence\u0026#34;: 0.892 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.120s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.010s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.288s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.093s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;confidence\u0026#34;: 0.620 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.233 }, { \u0026#34;word\u0026#34;: \u0026#34;ISN\u0026#39;T\u0026#34;, \u0026#34;confidence\u0026#34;: 0.108 }, { \u0026#34;word\u0026#34;: \u0026#34;THE\u0026#34;, \u0026#34;confidence\u0026#34;: 0.039 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.298s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.005s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.391s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.273s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;NEW\u0026#34;, \u0026#34;confidence\u0026#34;: 0.661 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.129 }, { \u0026#34;word\u0026#34;: \u0026#34;YOUR\u0026#34;, \u0026#34;confidence\u0026#34;: 0.107 }, { \u0026#34;word\u0026#34;: \u0026#34;YOU\u0026#34;, \u0026#34;confidence\u0026#34;: 0.102 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.396s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.420s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;DAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.954 }, { \u0026#34;word\u0026#34;: \u0026#34;TODAY\u0026#34;, \u0026#34;confidence\u0026#34;: 0.044 }, { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;UNK\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 0.002 } ], \u0026#34;startTime\u0026#34;: \u0026#34;2.670s\u0026#34; }, { \u0026#34;duration\u0026#34;: \u0026#34;0.270s\u0026#34;, \u0026#34;arcs\u0026#34;: [ { \u0026#34;word\u0026#34;: \u0026#34;\u0026lt;eps\u0026gt;\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0 } ], \u0026#34;startTime\u0026#34;: \u0026#34;3.090s\u0026#34; } ] } } \nFormatted output Speech recognition systems typically output the words that were spoken, with no formatting. For example, utterances with numbers in might return “twenty seven bridges”, and “the year two thousand and three”. Cubic has the option to enable basic formatting of speech recognition results:\n Capitalising the first letter of the utterance Numbers: “cobalt’s atomic number is twenty seven” -\u0026gt; “Cobalt’s atomic number is 27” Truecasing: “the iphone was launched in two thousand and seven” -\u0026gt; “The iPhone was launched in 2007” Ordinals: “summer solstice is twenty first june” -\u0026gt; “Summer solstice is 21st June”  Note that word level timestamps and confidences aren’t supported when formatting is enabled.\nObtaining Cubic Cobalt will provide you with a package of Cubic that contains the engine, appropriate speech recognition models and a server application. This server exports Cubic\u0026rsquo;s functionality over the gRPC protocol. The https://github.com/cobaltspeech/sdk-cubic repository contains the SDK that you can use in your application to communicate with the Cubic server. This SDK is currently available for the Go and Python languages; and we would be happy to talk to you if you need support for other languages. Most of the core SDK is generated automatically using the gRPC tools, and Cobalt provides a top level package for more convenient API calls.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]