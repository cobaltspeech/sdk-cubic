[
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/",
	"title": "Getting started",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "Instructions for installing the SDK are language specific.\nGo The Go SDK supports Go modules and requires Go 1.12 or later. To use the SDK, import this package into your application:\nimport \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; Python The Python SDK depends on Python \u0026gt;= 3.5. You may use pip to perform a system-wide install, or use virtualenv for a local install.\npip install --upgrade pip pip install \u0026#34;git+https://github.com/cobaltspeech/sdk-cubic#egg=cobalt-cubic\u0026amp;subdirectory=grpc/py-cubic\u0026#34;"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/",
	"title": "Protobuf Reference",
	"tags": [],
	"description": "",
	"content": "The Cubic API is specified as a proto file. This section of the documentation is auto-generated from the spec. It describes the data types and functions defined in the spec. The \u0026ldquo;messages\u0026rdquo; below correspond to the data structures to be used, and the \u0026ldquo;service\u0026rdquo; contains the methods that can be called.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/connecting/",
	"title": "Setup Connection",
	"tags": [],
	"description": "",
	"content": "Once you have your Cubic server up and running, let\u0026rsquo;s see how we can use the SDK to connect to it.\nFirst, you need to know the address (host:port) where the server is running. This document will assume the values 127.0.0.1:2727, but be sure to change those to point to your server instance. Port 2727 is the default GRPC port that Cubic server binds to.\nThe following code snipped connects to the server and queries its version. This (default) connection expects the server to be listening on a TLS encrypted connection.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() version, err := client.Version(context.Background()) if err != nil { log.Fatal(err) } fmt.Println(version) }  import cubic serverAddress = \u0026#34;127.0.0.1:2727\u0026#34; client = cubic.Client(serverAddress) resp = client.Version() print(resp)   \nInsecure Connection It is sometimes required to connect to Cubic server without TLS enabled, such as during debugging.\nPlease note that if the server has TLS enabled, attempting to connect with an insecure client will fail. To connect to such an instance of cubic server, you can use:\n client, err := cubic.NewClient(serverAddr, cubic.WithInsecure())  client = cubic.Client(serverAddress, insecure=True)   \nClient Authentication In our recommended default setup, TLS is enabled in the gRPC setup, and when connecting to the server, clients validate the server\u0026rsquo;s SSL certificate to make sure they are talking to the right party. This is similar to how \u0026ldquo;https\u0026rdquo; connections work in web browsers.\nIn some setups, it may be desired that the server should also validate clients connecting to it and only respond to the ones it can verify. If your Cubic server is configured to do client authentication, you will need to present the appropriate certificate and key when connecting to it.\nPlease note that in the client-authentication mode, the client will still also verify the server\u0026rsquo;s certificate, and therefore this setup uses mutually authenticated TLS. This can be done with:\n client, err := cubic.NewClient(serverAddr, cubic.WithClientCert(certPem, keyPem))  client = cubic.Client(serverAddress, clientCertificate=certPem, clientKey=keyPem)   \nwhere certPem and keyPem are the bytes of the client certificate and key provided to you.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/recognize/",
	"title": "Synchronous Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe a short audio clip using Cubic\u0026rsquo;s Synchoronous Recognize Request. It is assumed that the audio file contains raw samples, PCM16SLE like Cubic expects. We will query the server for available models and use the first model to transcribe this speech.\nSynchronous recognize requests are suitable only for audio clips shorter than 30 seconds. In general, it is strongly recommended that you use streaming recognition.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model  model := modelResp.Models[0] f, err := os.Open(\u0026#34;test.raw\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } recResp, err := client.Recognize(context.Background(), cfg, f) for _, r := range recResp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } }  import cubic serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # open audio file  audio = open(\u0026#39;test.raw\u0026#39;, \u0026#39;rb\u0026#39;) resp = client.Recognize(cfg, audio) for result in resp.results: if not result.is_partial: print(result.alternatives[0].transcript)   \n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/getting-started/streaming/",
	"title": "Streaming Recognition",
	"tags": [],
	"description": "",
	"content": "The following example shows how to transcribe an audio file using Cubicâ€™s Streaming Recognize Request. The example uses a WAV file as input to the streaming recognition. We will query the server for available models and use the first model to transcribe the speech.\n package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic\u0026#34; \u0026#34;github.com/cobaltspeech/sdk-cubic/grpc/go-cubic/cubicpb\u0026#34; ) const serverAddr = \u0026#34;127.0.0.1:2727\u0026#34; func main() { client, err := cubic.NewClient(serverAddr) if err != nil { log.Fatal(err) } defer client.Close() modelResp, err := client.ListModels(context.Background()) if err != nil { log.Fatal(err) } // Use the first available model  model := modelResp.Models[0] f, err := os.Open(\u0026#34;test.wav\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() cfg := \u0026amp;cubicpb.RecognitionConfig{ ModelId: model.Id, } // define a callback function to handle results  resultHandler := func(resp *cubicpb.RecognitionResponse) { for _, r := range resp.Results { if !r.IsPartial { fmt.Println(r.Alternatives[0].Transcript) } } } err = client.StreamingRecognize(context.Background(), cfg, f, resultHandler) if err != nil { log.Fatal(err) } }  import cubic serverAddress = \u0026#39;127.0.0.1:2727\u0026#39; client = cubic.Client(serverAddress) # get list of available models modelResp = client.ListModels() for model in modelResp.models: print(\u0026#34;ID = {}, Name = {}\u0026#34;.format(model.id, model.name)) # use the first available model model = modelResp.models[0] cfg = cubic.RecognitionConfig( model_id = model.id ) # client.StreamingRecognize takes any binary # stream object that has a read(nBytes) method. # The method should return nBytes from the stream. # open audio file stream audio = open(\u0026#39;test.wav\u0026#39;, \u0026#39;rb\u0026#39;) # send streaming request to cubic and  # print out results as they come in for resp in client.StreamingRecognize(cfg, audio): for result in resp.results: if result.is_partial: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\u0026#34;) else: print(\u0026#34;\\r{0}\u0026#34;.format(result.alternatives[0].transcript), end=\u0026#34;\\n\u0026#34;)   \n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/protobuf/autogen-doc-cubic-proto/",
	"title": "Proto Generated Docs",
	"tags": [],
	"description": "",
	"content": " cubic.proto Service: Cubic Service that implements the Cobalt Cubic Speech Recognition API\n   Method Name Request Type Response Type Description     Version .google.protobuf.Empty VersionResponse Queries the Version of the Server   ListModels ListModelsRequest ListModelsResponse Retrieves a list of available speech recognition models   Recognize RecognizeRequest RecognitionResponse Performs synchronous speech recognition: receive results after all audio has been sent and processed. It is expected that this request be typically used for short audio content: less than a minute long. For longer content, the StreamingRecognize method should be preferred.   StreamingRecognize StreamingRecognizeRequest RecognitionResponse Performs bidirectional streaming speech recognition. Receive results while sending audio. This method is only available via GRPC and not via HTTP+JSON. However, a web browser may use websockets to use this service.    Message: ConfusionNetworkArc An Arc inside a Confusion Network Link\n   Field Type Label Description     word string  Word in the recognized transcript\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n    Message: ConfusionNetworkLink A Link inside a confusion network\n   Field Type Label Description     start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this link\n   duration google.protobuf.Duration  Duration of the current link in the confusion network\n   arcs ConfusionNetworkArc repeated Arcs between this link\n    Message: ListModelsRequest The top-level message sent by the client for the ListModels method.\nThis message is empty and has no fields.\nMessage: ListModelsResponse The message returned to the client by the ListModels method.\n   Field Type Label Description     models Model repeated List of models available for use that match the request.\n    Message: Model Description of a Cubic Model\n   Field Type Label Description     id string  Unique identifier of the model. This identifier is used to choose the model that should be used for recognition, and is specified in the RecognitionConfig message.\n   name string  Model name. This is a concise name describing the model, and maybe presented to the end-user, for example, to help choose which model to use for their recognition task.\n   attributes ModelAttributes  Model attributes\n    Message: ModelAttributes Attributes of a Cubic Model\n   Field Type Label Description     sample_rate uint32  Audio sample rate supported by the model\n    Message: RecognitionAlternative A recognition hypothesis\n   Field Type Label Description     transcript string  Text representing the transcription of the words that the user spoke.\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood of the output being correct.\n   words WordInfo repeated A list of word-specific information for each recognized word. This is available only if enable_word_confidence or enable_word_time_offsets was set to true in the RecognitionConfig.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this utterance.\n   duration google.protobuf.Duration  Duration of the current utterance in the spoken audio.\n    Message: RecognitionAudio Audio to be sent to the recognizer\n   Field Type Label Description     data bytes  \n    Message: RecognitionConfig Configuration for setting up a Recognizer\n   Field Type Label Description     model_id string  Unique identifier of the model to use, as obtained from a Model message.\n   audio_encoding RecognitionConfig.Encoding  Encoding of audio data sent/streamed through the RecognitionAudio messages. For encodings like WAV/MP3 that have headers, the headers are expected to be sent at the beginning of the stream, not in every RecognitionAudio message.\nIf not specified, the default encoding is RAW_LINEAR16.\nDepending on how they are configured, server instances of this service may not support all the encodings enumerated above. They are always required to accept RAW_LINEAR16. If any other Encoding is specified, and it is not available on the server being used, the recognition request will result in an appropriate error message.\n   idle_timeout google.protobuf.Duration  Idle Timeout of the created Recognizer. If no audio data is received by the recognizer for this duration, ongoing rpc calls will result in an error, the recognizer will be destroyed and thus more audio may not be sent to the same recognizer. The server may impose a limit on the maximum idle timeout that can be specified, and if the value in this message exceeds that serverside value, creating of the recognizer will fail with an error.\n   enable_word_time_offsets bool  This is an optional field. If this is set to true, each result will include a list of words and the start time offset (timestamp) and the duration for each of those words. If set to false, no word-level timestamps will be returned. The default is false.\n   enable_word_confidence bool  This is an optional field. If this is set to true, each result will include a list of words and the confidence for those words. If false, no word-level confidence information is returned. The default is false.\n   enable_raw_transcript bool  This is an optional field. If this is set to true, the transcripts will be presented as raw output from the recognizer without any formatting rules applied. They will be in all UPPER CASE, numbers and other special entities would be presented as the spoken words. If set to false, formatting rules will be applied to all results. The default is false.\nAs an example, if the spoken utterance was here are four words: with this field set to false: \u0026ldquo;Here are 4 words\u0026rdquo; with this field set to \u0026lsquo;true\u0026rsquo; : \u0026ldquo;HERE ARE FOUR WORDS\u0026rdquo;\n   enable_confusion_network bool  This is an optional field. If this is set to true, the results will include a confusion network. If set to false, no confusion network will be returned. The default is false. If the model being used does not support a confusion network, results may be returned without a confusion network available. If this field is set to true, then enable_raw_transcript is also forced to be true.\n   audio_channels uint32 repeated This is an optional field. If the audio has multiple channels, this field should be configured with the list of channel indices that should be transcribed. Channels are 0-indexed.\nExample: [0] for a mono file, [0, 1] for a stereo file.\nIf this field is not set, a mono file will be assumed by default and only channel-0 will be transcribed even if the file actually has additional channels.\nChannels that are present in the audio may be omitted, but it is an error to include a channel index in this field that is not present in the audio. Channels may be listed in any order but the same index may not be repeated in this list.\nBAD: [0, 2] for a stereo file; BAD: [0, 0] for a mono file.\n    Message: RecognitionConfusionNetwork Confusion network in recognition output\n   Field Type Label Description     links ConfusionNetworkLink repeated \n    Message: RecognitionResponse Collection of sequence of recognition results in a portion of audio. When transcribing a single audio channel (e.g. RAW_LINEAR16 input, or a mono file), results will be ordered chronologically. When transcribing multiple channels, the results of all channels will be interleaved. Results of each individual channel will be chronological. No such promise is made for the ordering of results of different channels, as results are returned for each channel individually as soon as they are ready.\n   Field Type Label Description     results RecognitionResult repeated \n    Message: RecognitionResult A recognition result corresponding to a portion of audio.\n   Field Type Label Description     alternatives RecognitionAlternative repeated An n-best list of recognition hypotheses alternatives\n   is_partial bool  If this is set to true, it denotes that the result is an interim partial result, and could change after more audio is processed. If unset, or set to false, it denotes that this is a final result and will not change.\nServers are not required to implement support for returning partial results, and clients should generally not depend on their availability.\n   cnet RecognitionConfusionNetwork  If enable_confusion_network was set to true in the RecognitionConfig, and if the model supports it, a confusion network will be available in the results.\n   audio_channel uint32  Channel of the audio file that this result was transcribed from. For a mono file, or RAW_LINEAR16 input, this will be set to 0.\n    Message: RecognizeRequest The top-level message sent by the client for the Recognize method. Both the RecognitionConfig and RecognitionAudio fields are required. The entire audio data must be sent in one request. If your audio data is larger, please use the StreamingRecognize call..\n   Field Type Label Description     config RecognitionConfig  Provides configuration to create the recognizer.\n   audio RecognitionAudio  The audio data to be recognized\n    Message: StreamingRecognizeRequest The top-level message sent by the client for the StreamingRecognize request. Multiple StreamingRecognizeRequest messages are sent. The first message must contain a RecognitionConfig message only, and all subsequent messages must contain RecognitionAudio only. All RecognitionAudio messages must contain non-empty audio. If audio content is empty, the server may interpret it as end of stream and stop accepting any further messages.\n   Field Type Label Description     config RecognitionConfig  \n   audio RecognitionAudio  \n    Message: VersionResponse The message sent by the server for the Version method.\n   Field Type Label Description     cubic string  version of the cubic library handling the recognition\n   server string  version of the server handling these requests\n    Message: WordInfo Word-specific information for recognized words\n   Field Type Label Description     word string  The actual word in the text\n   confidence double  Confidence estimate between 0 and 1. A higher number represents a higher likelihood that the word was correctly recognized.\n   start_time google.protobuf.Duration  Time offset relative to the beginning of audio received by the recognizer and corresponding to the start of this spoken word.\n   duration google.protobuf.Duration  Duration of the current word in the spoken audio.\n    Enum: RecognitionConfig.Encoding The encoding of the audio data to be sent for recognition.\nFor best results, the audio source should be captured and transmitted using the RAW_LINEAR16 encoding.\n   Name Number Description     RAW_LINEAR16 0 Raw (headerless) Uncompressed 16-bit signed little endian samples (linear PCM), single channel, sampled at the rate expected by the chosen Model.   WAV 1 WAV (data with RIFF headers), with data sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   MP3 2 MP3 data, sampled at a rate equal to or higher than the sampling rate expected by the chosen Model.   FLAC 3 FLAC data, sampled at a rate equal to or higher than the sample rate expected by the chosen Model.   VOX8000 4 VOX data (Dialogic ADPCM), sampled at 8 KHz.   ULAW8000 5 Î¼-law (8-bit) encoded RAW data, single channel, sampled at 8 KHz.    Scalar Value Types    .proto Type Notes Go Type Python Type     double  float64 float   float  float32 float   int32 Uses variable-length encoding. Inefficient for encoding negative numbers â€“ if your field is likely to have negative values, use sint32 instead. int32 int   int64 Uses variable-length encoding. Inefficient for encoding negative numbers â€“ if your field is likely to have negative values, use sint64 instead. int64 int/long   uint32 Uses variable-length encoding. uint32 int/long   uint64 Uses variable-length encoding. uint64 int/long   sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int   sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 int/long   fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int   fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 int/long   sfixed32 Always four bytes. int32 int   sfixed64 Always eight bytes. int64 int/long   bool  bool boolean   string A string must always contain UTF-8 encoded or 7-bit ASCII text. string str/unicode   bytes May contain any arbitrary sequence of bytes. []byte str    "
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Cubic SDK \u0026ndash; Cobalt\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/",
	"title": "Cubic SDK Documentation",
	"tags": [],
	"description": "",
	"content": " Cubic SDK Cubic is Cobalt\u0026rsquo;s speech recognition engine. It can be bundled into your application as a library, or deployed on-prem and accessed over the network. This documentation refers to accessing the network-based Cubic server.\nCobalt will provide you with a package of Cubic that contains the engine, appropriate speech recognition models and a server application. This server exports Cubic\u0026rsquo;s functionality over the gRPC protocol. The https://github.com/cobaltspeech/sdk-cubic repository contains the SDK that you can use in your application to communicate with the Cubic server. This SDK is currently available for the Go language; and we would be happy to talk to you if you need support for other languages. Most of the core SDK is generated automatically using the gRPC tools, and Cobalt provides a top level package for more convenient API calls.\n"
},
{
	"uri": "https://cobaltspeech.github.io/sdk-cubic/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]